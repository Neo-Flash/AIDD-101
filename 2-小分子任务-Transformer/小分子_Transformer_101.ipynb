{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "X7AN78JLc32N",
        "7RahmMfxUWFX",
        "1UpN6sw5Seww",
        "qX01sL27Y6OV",
        "1zNgfssDaPEP",
        "VZ9UmMoqb2gX",
        "TQWstQ1JdzwK"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **å®‰è£…ä¾èµ–åº“**"
      ],
      "metadata": {
        "id": "X7AN78JLc32N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QIYUh0E2dZw"
      },
      "source": [
        "## **ä½œè€…ç®€ä»‹**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X91JIVkqUNMV"
      },
      "source": [
        "#### ä½œè€…ï¼š**âš¡å°é—ªç”µâš¡**\n",
        "\n",
        "#### Bç«™ä¸»é¡µ\n",
        "- [å°é—ªç”µçš„Bç«™ä¸»é¡µ](https://space.bilibili.com/122699831?spm_id_from=333.1007.0.0)\n",
        "\n",
        "#### äº¤æµç¾¤\n",
        "æ¬¢è¿åŠ å…¥AIDDäº¤æµç¾¤ï¼  \n",
        "åŠ æˆ‘å¾®ä¿¡ï¼ˆå¾®ä¿¡å·: `xxxFLASHxxx`ï¼‰ï¼Œé‚€è¯·ä½ è¿›ç¾¤ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RahmMfxUWFX"
      },
      "source": [
        "## **å®‰è£…ä¾èµ–åº“**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cf3YlQPJkqE"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡ä¸é¢„å¤„ç† (Data Preprocessing)**"
      ],
      "metadata": {
        "id": "1UpN6sw5Seww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ==========================================\n",
        "# 0. å…¨å±€é…ç½®ä¸é…è‰² (Visual Identity)\n",
        "# ==========================================\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# å®šä¹‰ä¸“å±é…è‰²\n",
        "COLOR_BLUE = (75 / 255, 116 / 255, 178 / 255) # ä¸»è‰²è°ƒ / è®­ç»ƒé›† / ä½ç”œ\n",
        "COLOR_RED = (219 / 255, 49 / 255, 36 / 255)   # å¼ºè°ƒè‰² / æµ‹è¯•é›† / é«˜ç”œ\n",
        "COLOR_GREY = \"#f0f0f0\"                        # èƒŒæ™¯è‰²\n",
        "\n",
        "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "except:\n",
        "    plt.style.use('ggplot')\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ ¸å¿ƒç»„ä»¶: SMILES Tokenizer\n",
        "# ==========================================\n",
        "class SMILESTokenizer:\n",
        "    \"\"\"\n",
        "    åŸºäºåŒ–å­¦æ­£åˆ™è¡¨è¾¾å¼çš„åˆ†è¯å™¨ã€‚\n",
        "    æ”¯æŒä¿å­˜/åŠ è½½è¯è¡¨ï¼Œä¿è¯æ¨ç†æ—¶çš„ä¸€è‡´æ€§ã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len=128, vocab=None):\n",
        "        self.max_len = max_len\n",
        "        # æ ¸å¿ƒæ­£åˆ™ï¼šè¯†åˆ« Br, Cl, [nH], =, #, æ•°å­— ç­‰\n",
        "        self.pattern =  r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "        self.regex = re.compile(self.pattern)\n",
        "\n",
        "        # ç‰¹æ®Šæ ‡è®°\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "        self.mask_token = \"<mask>\" # é¢„ç•™ç»™ BERT\n",
        "\n",
        "        self.special_tokens = [self.pad_token, self.unk_token, self.sos_token, self.eos_token, self.mask_token]\n",
        "\n",
        "        if vocab:\n",
        "            self.vocab = vocab\n",
        "        else:\n",
        "            self.vocab = {token: i for i, token in enumerate(self.special_tokens)}\n",
        "\n",
        "        self.inv_vocab = {i: token for token, i in self.vocab.items()}\n",
        "\n",
        "    def tokenize(self, smiles):\n",
        "        \"\"\"æ­£åˆ™åˆ‡åˆ†\"\"\"\n",
        "        return [token for token in self.regex.findall(smiles)]\n",
        "\n",
        "    def build_vocab(self, smiles_list):\n",
        "        \"\"\"æ‰«ææ•´ä¸ªæ•°æ®é›†æ„å»ºè¯è¡¨\"\"\"\n",
        "        print(\"ğŸ—ï¸  æ­£åœ¨æ„å»ºåŒ–å­¦è¯è¡¨...\")\n",
        "        counter = Counter()\n",
        "        for s in smiles_list:\n",
        "            tokens = self.tokenize(s)\n",
        "            counter.update(tokens)\n",
        "\n",
        "        # æŒ‰é¢‘ç‡æ’åºåŠ å…¥è¯è¡¨\n",
        "        for token, count in counter.most_common():\n",
        "            if token not in self.vocab:\n",
        "                idx = len(self.vocab)\n",
        "                self.vocab[token] = idx\n",
        "                self.inv_vocab[idx] = token\n",
        "        print(f\"âœ… è¯è¡¨æ„å»ºå®Œæˆ | å¤§å°: {len(self.vocab)} | Top Token: {counter.most_common(1)[0]}\")\n",
        "\n",
        "    def encode(self, smiles, add_special_tokens=True, padding=True, truncation=True):\n",
        "        \"\"\"SMILES -> IDs\"\"\"\n",
        "        tokens = self.tokenize(smiles)\n",
        "\n",
        "        # 1. æˆªæ–­ (ç•™ä½ç½®ç»™ SOS/EOS)\n",
        "        if truncation and len(tokens) > self.max_len - 2:\n",
        "            tokens = tokens[:self.max_len - 2]\n",
        "\n",
        "        # 2. æŸ¥è¡¨\n",
        "        ids = [self.vocab.get(t, self.vocab[self.unk_token]) for t in tokens]\n",
        "\n",
        "        # 3. æ·»åŠ ç‰¹æ®Š Token\n",
        "        if add_special_tokens:\n",
        "            ids = [self.vocab[self.sos_token]] + ids + [self.vocab[self.eos_token]]\n",
        "\n",
        "        # 4. å¡«å……\n",
        "        if padding:\n",
        "            pad_len = self.max_len - len(ids)\n",
        "            if pad_len > 0:\n",
        "                ids = ids + [self.vocab[self.pad_token]] * pad_len\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids, skip_special_tokens=True):\n",
        "        \"\"\"IDs -> SMILES\"\"\"\n",
        "        tokens = []\n",
        "        for i in ids:\n",
        "            token = self.inv_vocab.get(i, self.unk_token)\n",
        "            if skip_special_tokens and token in self.special_tokens:\n",
        "                continue\n",
        "            tokens.append(token)\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def save_vocab(self, path):\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.vocab, f, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load_vocab(cls, path, max_len=128):\n",
        "        with open(path, 'r') as f:\n",
        "            vocab = json.load(f)\n",
        "        return cls(max_len=max_len, vocab=vocab)\n",
        "\n",
        "# ==========================================\n",
        "# 2. æ•°æ®ç®¡é“ (Dataset & Split)\n",
        "# ==========================================\n",
        "class SweetnessDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.smiles = df['Smiles'].values\n",
        "        self.labels = df['logSw'].values\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.smiles[idx]\n",
        "        label = self.labels[idx]\n",
        "        # ç¼–ç \n",
        "        ids = self.tokenizer.encode(s)\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'labels': torch.tensor(label, dtype=torch.float),\n",
        "            'raw_smiles': s # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²æ–¹ä¾¿debug\n",
        "        }\n",
        "\n",
        "def prepare_data(csv_path, max_len=100, batch_size=32):\n",
        "    # 1. è¯»å–\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # åˆ—åæ¸…æ´—\n",
        "    df.columns = [c.lower() for c in df.columns]\n",
        "    rename_map = {'smiles': 'Smiles', 'logsw': 'logSw'}\n",
        "    df.rename(columns=rename_map, inplace=True)\n",
        "    df.dropna(subset=['Smiles', 'logSw'], inplace=True)\n",
        "\n",
        "    # 2. Tokenizer\n",
        "    tokenizer = SMILESTokenizer(max_len=max_len)\n",
        "    tokenizer.build_vocab(df['Smiles'].tolist())\n",
        "\n",
        "    # 3. åˆ’åˆ† (8:1:1)\n",
        "    dataset = SweetnessDataset(df, tokenizer)\n",
        "    total_len = len(dataset)\n",
        "    train_len = int(0.8 * total_len)\n",
        "    val_len = int(0.1 * total_len)\n",
        "    test_len = total_len - train_len - val_len\n",
        "\n",
        "    train_ds, val_ds, test_ds = random_split(\n",
        "        dataset, [train_len, val_len, test_len],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # 4. Loaders\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "    print(f\"\\nğŸ“Š æ•°æ®åˆ’åˆ†å®Œæˆ:\")\n",
        "    print(f\"   Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
        "\n",
        "    return tokenizer, train_loader, val_loader, test_loader, df\n",
        "\n",
        "# ==========================================\n",
        "# 3. èŠ±é‡Œèƒ¡å“¨çš„å¯è§†åŒ– (Visualization)\n",
        "# ==========================================\n",
        "def visualize_data_distribution(df, tokenizer, max_len):\n",
        "    \"\"\"\n",
        "    ç»˜åˆ¶ä»ªè¡¨ç›˜é£æ ¼çš„æ•°æ®åˆ†å¸ƒå›¾\n",
        "    \"\"\"\n",
        "    # è®¡ç®—ç»Ÿè®¡é‡\n",
        "    seq_lens = [len(tokenizer.tokenize(s)) for s in df['Smiles']]\n",
        "    all_tokens = [t for s in df['Smiles'] for t in tokenizer.tokenize(s)]\n",
        "    token_counts = Counter(all_tokens).most_common(10)\n",
        "\n",
        "    # åˆ›å»ºç”»å¸ƒ\n",
        "    fig = plt.figure(figsize=(15, 8), dpi=150)\n",
        "    gs = gridspec.GridSpec(2, 2, width_ratios=[1.5, 1], height_ratios=[1, 1])\n",
        "    fig.suptitle('Sweetness Dataset Analytics Dashboard', fontsize=18, fontweight='bold', y=0.95)\n",
        "\n",
        "    # --- å›¾1: åºåˆ—é•¿åº¦åˆ†å¸ƒ (å·¦ä¸Š) ---\n",
        "    ax1 = plt.subplot(gs[0, 0])\n",
        "    sns.histplot(seq_lens, bins=40, kde=True, color=COLOR_BLUE, edgecolor='white', alpha=0.7, ax=ax1)\n",
        "    ax1.axvline(np.mean(seq_lens), color=COLOR_RED, linestyle='--', linewidth=2, label=f'Mean: {np.mean(seq_lens):.1f}')\n",
        "    ax1.axvline(max_len, color='gray', linestyle=':', linewidth=2, label=f'Max Limit: {max_len}')\n",
        "    ax1.set_title(\"Sequence Length Distribution\", fontsize=12, fontweight='bold', loc='left', color='#333')\n",
        "    ax1.legend()\n",
        "    # æ·»åŠ æ–‡å­—æ ‡æ³¨\n",
        "    stats_text = f\"Min: {min(seq_lens)}\\nMax: {max(seq_lens)}\\nMedian: {int(np.median(seq_lens))}\"\n",
        "    ax1.text(0.95, 0.5, stats_text, transform=ax1.transAxes, ha='right', bbox=dict(facecolor='white', alpha=0.8, boxstyle='round'))\n",
        "\n",
        "    # --- å›¾2: Top 10 Token (å³ä¸Š) ---\n",
        "    ax2 = plt.subplot(gs[:, 1]) # å å³è¾¹ä¸¤è¡Œ\n",
        "    y = [x[0] for x in token_counts]\n",
        "    x = [x[1] for x in token_counts]\n",
        "    sns.barplot(x=x, y=y, palette=[COLOR_BLUE]*10, ax=ax2, orient='h')\n",
        "    # çªå‡ºç¬¬ä¸€å\n",
        "    ax2.patches[0].set_color(COLOR_RED)\n",
        "    ax2.set_title(\"Top 10 Frequent Tokens\", fontsize=12, fontweight='bold', loc='left')\n",
        "    ax2.set_xlabel(\"Count\")\n",
        "    for i, v in enumerate(x):\n",
        "        ax2.text(v + 3, i, str(v), color='black', va='center', fontsize=9)\n",
        "\n",
        "    # --- å›¾3: ç”œåº¦å€¼åˆ†å¸ƒ (å·¦ä¸‹) ---\n",
        "    ax3 = plt.subplot(gs[1, 0])\n",
        "    # ç»˜åˆ¶åŒè‰²èƒŒæ™¯æ¥åŒºåˆ†é«˜ä½ç”œåº¦\n",
        "    sns.kdeplot(df['logSw'], fill=True, color=COLOR_BLUE, alpha=0.3, ax=ax3, label=\"Density\")\n",
        "    sns.histplot(df['logSw'], bins=30, kde=False, stat=\"density\", color=COLOR_BLUE, alpha=0.4, edgecolor='white', ax=ax3)\n",
        "\n",
        "    # ç»˜åˆ¶é˜ˆå€¼çº¿\n",
        "    ax3.axvline(3.0, color=COLOR_RED, linestyle='--', linewidth=2)\n",
        "    ax3.text(3.1, ax3.get_ylim()[1]*0.8, \"High Sweetness (>3)\", color=COLOR_RED, fontweight='bold')\n",
        "    ax3.text(2.9, ax3.get_ylim()[1]*0.8, \"Low Sweetness (<=3)\", color=COLOR_BLUE, fontweight='bold', ha='right')\n",
        "\n",
        "    ax3.set_title(\"Label Distribution (logSw)\", fontsize=12, fontweight='bold', loc='left')\n",
        "    ax3.set_xlabel(\"Log Sweetness\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.88)\n",
        "    plt.show()\n",
        "\n",
        "def visualize_tokenization_demo(tokenizer, smiles):\n",
        "    \"\"\"\n",
        "    é»‘ç›’å¯è§†åŒ–ï¼šå±•ç¤ºSMILESå¦‚ä½•å˜æˆID\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(smiles)\n",
        "    ids = tokenizer.encode(smiles)\n",
        "\n",
        "    print(\"\\nğŸ” Tokenizer Deep Dive\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"ğŸ§ª Input:  {smiles}\")\n",
        "    print(f\"âœ‚ï¸  Tokens: {tokens}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Token':<12} | {'ID':<6} | {'Visual'}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # å±•ç¤ºå‰5ä¸ªå’Œå5ä¸ªï¼Œä¸­é—´çœç•¥\n",
        "    display_items = []\n",
        "    # é‡æ„æ˜¾ç¤ºåˆ—è¡¨ (å«ç‰¹æ®Štoken)\n",
        "    full_tokens = [tokenizer.sos_token] + tokens + [tokenizer.eos_token]\n",
        "\n",
        "    for i, t in enumerate(full_tokens):\n",
        "        if i < 5 or i >= len(full_tokens)-3:\n",
        "            # ç®€å•çš„è¿›åº¦æ¡å¯è§†åŒ–\n",
        "            bar = \"â–ˆ\" * (ids[i] % 10 + 1)\n",
        "            print(f\"{t:<12} | {ids[i]:<6} | {bar}\")\n",
        "        elif i == 5:\n",
        "            print(f\"{'...':<12} | {'...':<6} |\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ==========================================\n",
        "# 4. æ‰§è¡Œå…¥å£\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # é…ç½®\n",
        "    CSV_FILE = '/content/SweetpredDB.csv'\n",
        "    MAX_LEN = 100\n",
        "    BATCH_SIZE = 64\n",
        "\n",
        "    # 1. å‡†å¤‡æ•°æ®\n",
        "    tokenizer, train_dl, val_dl, test_dl, raw_df = prepare_data(CSV_FILE, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "    # 2. ä¿å­˜è¯è¡¨ (ä¾›åç»­æ¨¡å‹ä½¿ç”¨)\n",
        "    tokenizer.save_vocab(\"vocab_sweetness.json\")\n",
        "    print(f\"ğŸ’¾ è¯è¡¨å·²ä¿å­˜è‡³ vocab_sweetness.json (Vocab Size: {len(tokenizer.vocab)})\")\n",
        "\n",
        "    # 3. è¿è¡Œå¯è§†åŒ–\n",
        "    print(\"\\nğŸ¨ æ­£åœ¨ç»˜åˆ¶æ•°æ®åˆ†å¸ƒ...\")\n",
        "    visualize_data_distribution(raw_df, tokenizer, MAX_LEN)\n",
        "\n",
        "    # 4. åˆ†è¯æ¼”ç¤º\n",
        "    visualize_tokenization_demo(tokenizer, raw_df['Smiles'].iloc[0])\n",
        "\n",
        "    # 5. æ£€æŸ¥ Batch æ•°æ®\n",
        "    sample = next(iter(train_dl))\n",
        "    print(\"\\nğŸ“¦ Batch Data Check:\")\n",
        "    print(f\"   Input Shape: {sample['input_ids'].shape}\") # [64, 100]\n",
        "    print(f\"   Label Shape: {sample['labels'].shape}\")    # [64]"
      ],
      "metadata": {
        "id": "gSMKmLmjS8PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¬¬äºŒéƒ¨åˆ†ï¼šRNN ç”œåº¦é¢„æµ‹ (Baseline Model)**"
      ],
      "metadata": {
        "id": "qX01sL27Y6OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# å¤ç”¨ä¹‹å‰çš„é…è‰²\n",
        "COLOR_BLUE = (75 / 255, 116 / 255, 178 / 255)\n",
        "COLOR_RED = (219 / 255, 49 / 255, 36 / 255)\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ¨¡å‹å®šä¹‰: Bi-LSTM Regressor\n",
        "# ==========================================\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64, num_layers=2, pad_idx=0):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "\n",
        "        # 1. Embedding å±‚: å°†ç¦»æ•£çš„ ID è½¬æ¢ä¸ºç¨ å¯†å‘é‡\n",
        "        # padding_idx=pad_idx å¾ˆé‡è¦ï¼Œå®ƒå‘Šè¯‰ PyTorch è¿™ä¸ª ID æ˜¯å¡«å……çš„ï¼Œä¸åº”è¯¥æœ‰æ¢¯åº¦\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # 2. LSTM å±‚: å¤„ç†åºåˆ—çš„æ ¸å¿ƒ\n",
        "        # batch_first=True è®©è¾“å…¥å˜æˆ [Batch, Seq, Feature] ç¬¦åˆç›´è§‰\n",
        "        # bidirectional=True å¼€å¯åŒå‘ï¼Œè¾“å‡ºç»´åº¦ä¼šå˜æˆ hidden_dim * 2\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # 3. å›å½’å¤´ (MLP): å°†æå–çš„ç‰¹å¾æ˜ å°„åˆ°ç”œåº¦å€¼\n",
        "        # è¾“å…¥ç»´åº¦æ˜¯ hidden_dim * 2 (å› ä¸ºæ˜¯åŒå‘)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1) # è¾“å‡ºä¸€ä¸ªæ ‡é‡ï¼šç”œåº¦å€¼\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [Batch, Seq_Len]\n",
        "\n",
        "        # [Step 1] Embedding\n",
        "        # out: [Batch, Seq_Len, Embed_Dim]\n",
        "        x_emb = self.embedding(x)\n",
        "\n",
        "        # [Step 2] Bi-LSTM\n",
        "        # out: [Batch, Seq_Len, Hidden_Dim * 2]\n",
        "        # h_n, c_n æˆ‘ä»¬è¿™é‡Œä¸éœ€è¦ï¼Œåªéœ€è¦æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡º\n",
        "        lstm_out, _ = self.lstm(x_emb)\n",
        "\n",
        "        # [Step 3] Mean Pooling (å…³é”®æ­¥éª¤)\n",
        "        # æˆ‘ä»¬è¦æŠŠåºåˆ—é•¿åº¦ç»´åº¦å‹ç¼©æ‰ï¼Œå˜æˆä¸€ä¸ªåˆ†å­çš„æ•´ä½“å‘é‡\n",
        "        # ç®€å•çš„åšæ³•æ˜¯å–å¹³å‡å€¼: mean(dim=1)\n",
        "        feature = lstm_out.mean(dim=1) # shape: [Batch, Hidden_Dim * 2]\n",
        "\n",
        "        # [Step 4] MLP Regression\n",
        "        # out: [Batch, 1]\n",
        "        score = self.regressor(feature)\n",
        "\n",
        "        return score.squeeze(-1) # å»æ‰æœ€åä¸€ç»´ï¼Œå˜æˆ [Batch]\n",
        "\n",
        "# ==========================================\n",
        "# 2. è®­ç»ƒå¼•æ“ (Training Engine)\n",
        "# ==========================================\n",
        "class Trainer:\n",
        "    def __init__(self, model, device, lr=0.001):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss() # å›å½’ä»»åŠ¡æ ‡å‡†æŸå¤±å‡½æ•°\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            # æ¬è¿æ•°æ®åˆ° GPU/CPU\n",
        "            ids = batch['input_ids'].to(self.device)\n",
        "            labels = batch['labels'].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            preds = self.model(ids)\n",
        "            loss = self.criterion(preds, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        return total_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, loader):\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                ids = batch['input_ids'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                preds = self.model(ids)\n",
        "                loss = self.criterion(preds, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        r2 = r2_score(all_labels, all_preds)\n",
        "        return avg_loss, r2, np.array(all_labels), np.array(all_preds)\n",
        "\n",
        "# ==========================================\n",
        "# 3. è¿è¡Œè®­ç»ƒä¸å¯è§†åŒ–\n",
        "# ==========================================\n",
        "def run_lstm_baseline(tokenizer, train_dl, val_dl, test_dl):\n",
        "    print(\"\\nğŸš€ å¯åŠ¨ Baseline: Bi-LSTM æ¨¡å‹è®­ç»ƒ...\")\n",
        "\n",
        "    # é…ç½®\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    EPOCHS = 20\n",
        "\n",
        "    # åˆå§‹åŒ–æ¨¡å‹\n",
        "    # æ³¨æ„ï¼špad_idx è¦ä» tokenizer çš„ vocab ä¸­è·å–\n",
        "    pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "    model = BiLSTMRegressor(vocab_size=len(tokenizer.vocab), pad_idx=pad_id)\n",
        "    trainer = Trainer(model, DEVICE)\n",
        "\n",
        "    # è®°å½•æ—¥å¿—\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_r2': []}\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t_loss = trainer.train_epoch(train_dl)\n",
        "        v_loss, v_r2, _, _ = trainer.evaluate(val_dl)\n",
        "\n",
        "        history['train_loss'].append(t_loss)\n",
        "        history['val_loss'].append(v_loss)\n",
        "        history['val_r2'].append(v_r2)\n",
        "\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print(f\"Epoch {epoch+1:02d}/{EPOCHS} | Train Loss: {t_loss:.4f} | Val Loss: {v_loss:.4f} | Val R2: {v_r2:.4f}\")\n",
        "\n",
        "    print(f\"âœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time()-start_time:.1f}s\")\n",
        "\n",
        "    # --- æœ€ç»ˆè¯„ä¼° ---\n",
        "    test_loss, test_r2, y_true, y_pred = trainer.evaluate(test_dl)\n",
        "    print(f\"\\nğŸ§ª æµ‹è¯•é›†æœ€ç»ˆç»“æœ -> MSE: {test_loss:.4f} | R2 Score: {test_r2:.4f}\")\n",
        "\n",
        "    # --- å¯è§†åŒ– ---\n",
        "    plot_lstm_results(history, y_true, y_pred)\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_lstm_results(history, y_true, y_pred):\n",
        "    plt.figure(figsize=(12, 5), dpi=100)\n",
        "\n",
        "    # å›¾ 1: è®­ç»ƒæ›²çº¿ (Loss)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss', color=COLOR_BLUE, linestyle='--')\n",
        "    plt.plot(history['val_loss'], label='Val Loss', color=COLOR_BLUE, linewidth=2)\n",
        "    plt.title('Bi-LSTM Learning Curve', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # å›¾ 2: çœŸå®å€¼ vs é¢„æµ‹å€¼\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(y_true, y_pred, alpha=0.6, color=COLOR_BLUE, s=20, label='Test Samples')\n",
        "\n",
        "    # ç”»å¯¹è§’çº¿ (å®Œç¾é¢„æµ‹çº¿)\n",
        "    min_val = min(min(y_true), min(y_pred))\n",
        "    max_val = max(max(y_true), max(y_pred))\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], color='gray', linestyle='--', label='Perfect Prediction')\n",
        "\n",
        "    plt.title(f'Prediction Scatter (R2={r2_score(y_true, y_pred):.3f})', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('True Sweetness')\n",
        "    plt.ylabel('Predicted Sweetness')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 4. æ‰§è¡Œå…¥å£\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # å‡è®¾ä¹‹å‰çš„å˜é‡ (tokenizer, train_dl...) è¿˜åœ¨å†…å­˜ä¸­\n",
        "    # å¦‚æœæ²¡æœ‰ï¼Œè¯·å…ˆè¿è¡Œç¬¬ä¸€éƒ¨åˆ†ä»£ç \n",
        "    if 'tokenizer' in globals() and 'train_dl' in globals():\n",
        "        lstm_history = run_lstm_baseline(tokenizer, train_dl, val_dl, test_dl)\n",
        "    else:\n",
        "        print(\"âš ï¸ è¯·å…ˆè¿è¡Œç¬¬ä¸€éƒ¨åˆ†ä»£ç ä»¥å‡†å¤‡æ•°æ®ï¼\")"
      ],
      "metadata": {
        "id": "JJw1-fH0Y-Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¬¬ä¸‰éƒ¨åˆ†ï¼šTransformer ç”œåº¦é¢„æµ‹ (Encoder Only)**"
      ],
      "metadata": {
        "id": "1zNgfssDaPEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_squared_error  # è¡¥å…¨äº† mean_squared_error\n",
        "\n",
        "# æ²¿ç”¨ä¹‹å‰çš„é…è‰²\n",
        "COLOR_RED = (219 / 255, 49 / 255, 36 / 255)\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ ¸å¿ƒç»„ä»¶: Positional Encoding\n",
        "# ==========================================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    ç»å…¸çš„ Sinusoidal ä½ç½®ç¼–ç ã€‚\n",
        "    æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œè®© Transformer çŸ¥é“ Token çš„é¡ºåºã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿé•¿çš„ PE çŸ©é˜µ [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # æ³¨å†Œä¸º buffer (ä¸æ˜¯å‚æ•°ï¼Œä¸éœ€è¦æ¢¯åº¦æ›´æ–°ï¼Œä½†éšæ¨¡å‹ä¿å­˜)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0)) # [1, max_len, d_model]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Seq_Len, D_Model]\n",
        "        # åˆ‡ç‰‡å–å¯¹åº”é•¿åº¦çš„ä½ç½®ç¼–ç ç›¸åŠ \n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# ==========================================\n",
        "# 2. æ¨¡å‹å®šä¹‰: Transformer Regressor\n",
        "# ==========================================\n",
        "class TransformerRegressor(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, pad_idx=0):\n",
        "        super(TransformerRegressor, self).__init__()\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        # 1. Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "        # 2. Positional Encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # 3. Transformer Encoder å †å \n",
        "        # batch_first=True è®©è¾“å…¥ä¿æŒ [Batch, Seq, Feature]\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            batch_first=True,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # 4. å›å½’å¤´ (MLP)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(d_model, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # åˆå§‹åŒ–å‚æ•° (å¯¹äº Transformer å¾ˆé‡è¦)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.regressor[0].bias.data.zero_()\n",
        "        self.regressor[0].weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Seq_Len]\n",
        "\n",
        "        # ç”Ÿæˆ Padding Mask (å…³é”®!)\n",
        "        # src_key_padding_mask: [Batch, Seq_Len], True è¡¨ç¤ºæ˜¯ paddingï¼Œéœ€è¦è¢«å¿½ç•¥\n",
        "        src_key_padding_mask = (x == self.pad_idx)\n",
        "\n",
        "        # [Step 1] Embedding + Positional Encoding\n",
        "        # out: [Batch, Seq, D_Model]\n",
        "        x = self.embedding(x) * math.sqrt(x.size(-1)) # ç¼©æ”¾ embedding (Attention Is All You Need åŸæ–‡æ“ä½œ)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # [Step 2] Transformer Encoder\n",
        "        # out: [Batch, Seq, D_Model]\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # [Step 3] Mean Pooling\n",
        "        # æ³¨æ„ï¼šè¿™é‡Œç²—æš´å¹³å‡åŒ…å«äº† padding ä½ç½®ï¼Œä¸¥æ ¼æ¥è¯´åº”è¯¥ mask æ‰å†å¹³å‡\n",
        "        # ä½†ç”±äº padding çš„ embedding é€šå¸¸æ˜¯ 0 æˆ–ä¸æ›´æ–°ï¼Œä¸”æœ‰ mask æœºåˆ¶ï¼Œå½±å“è¾ƒå°ï¼Œä½œä¸º Demo å¯æ¥å—\n",
        "        # æ›´å¥½çš„åšæ³•æ˜¯: (x * ~mask.unsqueeze(-1)).sum(1) / (~mask).sum(1).unsqueeze(-1)\n",
        "\n",
        "        # ä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬å– mask ä¸º False (é padding) çš„ token çš„å¹³å‡å€¼\n",
        "        mask_expanded = (~src_key_padding_mask).unsqueeze(-1).float() # [Batch, Seq, 1]\n",
        "        sum_embeddings = torch.sum(x * mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9) # é˜²æ­¢é™¤é›¶\n",
        "        feature = sum_embeddings / sum_mask\n",
        "\n",
        "        # [Step 4] MLP Regression\n",
        "        score = self.regressor(feature)\n",
        "\n",
        "        return score.squeeze(-1)\n",
        "\n",
        "# ==========================================\n",
        "# 3. è®­ç»ƒä¸å¯è§†åŒ– (å·²æ·»åŠ ä¿å­˜åŠŸèƒ½)\n",
        "# ==========================================\n",
        "def run_transformer_challenger(tokenizer, train_dl, val_dl, test_dl):\n",
        "    print(\"\\nâš”ï¸  å¯åŠ¨ Challenger: Transformer æ¨¡å‹è®­ç»ƒ...\")\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    EPOCHS = 20 # ä¿æŒå’Œ RNN ä¸€è‡´ä»¥ä¾¿å¯¹æ¯”\n",
        "\n",
        "    # è·å– pad_id ç”¨äº mask\n",
        "    pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "\n",
        "    model = TransformerRegressor(\n",
        "        vocab_size=len(tokenizer.vocab),\n",
        "        pad_idx=pad_id,\n",
        "        d_model=128,    # ä¿æŒç»´åº¦è§„æ¨¡ç›¸è¿‘\n",
        "        nhead=4,        # 4å¤´æ³¨æ„åŠ›\n",
        "        num_layers=3    # 3å±‚ Encoder\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # å®šä¹‰ä¼˜åŒ–å™¨ (Transformer é€šå¸¸éœ€è¦æ›´å°çš„ lr æˆ–è€… warmupï¼Œè¿™é‡Œå…ˆç”¨æ ‡å‡† Adam)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005) # æ¯” RNN ç¨å¾®å°ä¸€ç‚¹çš„å­¦ä¹ ç‡æ›´ç¨³\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_r2': []}\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # --- Train Loop ---\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_dl:\n",
        "            ids = batch['input_ids'].to(DEVICE)\n",
        "            labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(ids)\n",
        "            loss = criterion(preds, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œå¯¹ Transformer å¾ˆé‡è¦)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dl)\n",
        "\n",
        "        # --- Val Loop ---\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        val_loss_sum = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dl:\n",
        "                ids = batch['input_ids'].to(DEVICE)\n",
        "                labels = batch['labels'].to(DEVICE)\n",
        "                preds = model(ids)\n",
        "                loss = criterion(preds, labels)\n",
        "                val_loss_sum += loss.item()\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss_sum / len(val_dl)\n",
        "        val_r2 = r2_score(val_labels, val_preds)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_r2'].append(val_r2)\n",
        "\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print(f\"Epoch {epoch+1:02d}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val R2: {val_r2:.4f}\")\n",
        "\n",
        "    print(f\"âœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time()-start_time:.1f}s\")\n",
        "\n",
        "    # --- [æ–°å¢] ä¿å­˜ Checkpoint ---\n",
        "    save_path = \"transformer_sweetness.pt\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"ğŸ’¾ æ¨¡å‹å‚æ•°å·²ä¿å­˜è‡³: {save_path}\")\n",
        "\n",
        "    # --- Final Test ---\n",
        "    model.eval()\n",
        "    test_preds, test_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dl:\n",
        "            ids = batch['input_ids'].to(DEVICE)\n",
        "            labels = batch['labels'].to(DEVICE)\n",
        "            preds = model(ids)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_r2 = r2_score(test_labels, test_preds)\n",
        "    test_mse = mean_squared_error(test_labels, test_preds)\n",
        "    print(f\"\\nğŸ§ª æµ‹è¯•é›†æœ€ç»ˆç»“æœ -> MSE: {test_mse:.4f} | R2 Score: {test_r2:.4f}\")\n",
        "\n",
        "    # --- Visualization ---\n",
        "    plot_transformer_results(history, test_labels, test_preds)\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_transformer_results(history, y_true, y_pred):\n",
        "    plt.figure(figsize=(12, 5), dpi=100)\n",
        "\n",
        "    # å›¾ 1: è®­ç»ƒæ›²çº¿ (çº¢è‰²ç³»)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss', color=COLOR_RED, linestyle='--')\n",
        "    plt.plot(history['val_loss'], label='Val Loss', color=COLOR_RED, linewidth=2)\n",
        "    plt.title('Transformer Learning Curve', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # å›¾ 2: é¢„æµ‹æ•£ç‚¹å›¾\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(y_true, y_pred, alpha=0.6, color=COLOR_RED, s=20, label='Test Samples')\n",
        "\n",
        "    min_val = min(min(y_true), min(y_pred))\n",
        "    max_val = max(max(y_true), max(y_pred))\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], color='gray', linestyle='--', label='Perfect Prediction')\n",
        "\n",
        "    plt.title(f'Prediction Scatter (R2={r2_score(y_true, y_pred):.3f})', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('True Sweetness')\n",
        "    plt.ylabel('Predicted Sweetness')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 4. æ‰§è¡Œå…¥å£\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    if 'tokenizer' in globals() and 'train_dl' in globals():\n",
        "        transformer_history = run_transformer_challenger(tokenizer, train_dl, val_dl, test_dl)\n",
        "    else:\n",
        "        print(\"âš ï¸ è¯·å…ˆè¿è¡Œç¬¬ä¸€éƒ¨åˆ†ä»£ç ä»¥å‡†å¤‡æ•°æ®ï¼\")"
      ],
      "metadata": {
        "id": "f3_2himpC4I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¬¬å››éƒ¨åˆ†ï¼šå·…å³°å¯¹å†³ (Comparison)**"
      ],
      "metadata": {
        "id": "VZ9UmMoqb2gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# æ²¿ç”¨ä¹‹å‰çš„é…è‰²\n",
        "COLOR_BLUE = (75 / 255, 116 / 255, 178 / 255) # Bi-LSTM\n",
        "COLOR_RED = (219 / 255, 49 / 255, 36 / 255)   # Transformer\n",
        "\n",
        "def visualize_peak_showdown(lstm_hist, transformer_hist):\n",
        "    \"\"\"\n",
        "    ç»˜åˆ¶é«˜å¤§ä¸Šçš„æ¨¡å‹å¯¹æ¯”å›¾\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(lstm_hist['val_loss']) + 1)\n",
        "\n",
        "    # åˆ›å»ºç”»å¸ƒ\n",
        "    fig = plt.figure(figsize=(16, 7), dpi=150)\n",
        "    plt.subplots_adjust(wspace=0.25)\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. Loss æ›²çº¿å¯¹æ¯” (è¶Šä½è¶Šå¥½)\n",
        "    # ==========================================\n",
        "    ax1 = plt.subplot(1, 2, 1)\n",
        "\n",
        "    # ç»˜åˆ¶æ›²çº¿\n",
        "    l1 = ax1.plot(epochs, lstm_hist['val_loss'], 'o-', color=COLOR_BLUE, label='Bi-LSTM (Baseline)', linewidth=2, markersize=5)\n",
        "    l2 = ax1.plot(epochs, transformer_hist['val_loss'], 's-', color=COLOR_RED, label='Transformer (Challenger)', linewidth=2, markersize=5)\n",
        "\n",
        "    # å¡«å……å·®è·åŒºåŸŸ\n",
        "    ax1.fill_between(epochs, lstm_hist['val_loss'], transformer_hist['val_loss'],\n",
        "                     where=(np.array(transformer_hist['val_loss']) < np.array(lstm_hist['val_loss'])),\n",
        "                     interpolate=True, color=COLOR_RED, alpha=0.1, label='Transformer Wins')\n",
        "    ax1.fill_between(epochs, lstm_hist['val_loss'], transformer_hist['val_loss'],\n",
        "                     where=(np.array(transformer_hist['val_loss']) >= np.array(lstm_hist['val_loss'])),\n",
        "                     interpolate=True, color=COLOR_BLUE, alpha=0.1, label='RNN Wins')\n",
        "\n",
        "    # æ ‡æ³¨æœ€å°å€¼\n",
        "    min_lstm = min(lstm_hist['val_loss'])\n",
        "    min_trans = min(transformer_hist['val_loss'])\n",
        "    ax1.axhline(min_trans, color=COLOR_RED, linestyle=':', alpha=0.5)\n",
        "\n",
        "    ax1.set_title(\"Validation Loss Comparison (Lower is Better)\", fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"MSE Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. R2 Score æ›²çº¿å¯¹æ¯” (è¶Šé«˜è¶Šå¥½)\n",
        "    # ==========================================\n",
        "    ax2 = plt.subplot(1, 2, 2)\n",
        "\n",
        "    ax2.plot(epochs, lstm_hist['val_r2'], 'o-', color=COLOR_BLUE, label='Bi-LSTM', linewidth=2)\n",
        "    ax2.plot(epochs, transformer_hist['val_r2'], 's-', color=COLOR_RED, label='Transformer', linewidth=2)\n",
        "\n",
        "    # æ ‡æ³¨æœ€ä½³ç‚¹\n",
        "    best_lstm_idx = np.argmax(lstm_hist['val_r2'])\n",
        "    best_trans_idx = np.argmax(transformer_hist['val_r2'])\n",
        "\n",
        "    ax2.annotate(f'Best LSTM: {lstm_hist[\"val_r2\"][best_lstm_idx]:.3f}',\n",
        "                 xy=(epochs[best_lstm_idx], lstm_hist['val_r2'][best_lstm_idx]),\n",
        "                 xytext=(epochs[best_lstm_idx], lstm_hist['val_r2'][best_lstm_idx]-0.1),\n",
        "                 arrowprops=dict(facecolor=COLOR_BLUE, shrink=0.05), color=COLOR_BLUE)\n",
        "\n",
        "    ax2.annotate(f'Best Trans: {transformer_hist[\"val_r2\"][best_trans_idx]:.3f}',\n",
        "                 xy=(epochs[best_trans_idx], transformer_hist['val_r2'][best_trans_idx]),\n",
        "                 xytext=(epochs[best_trans_idx], transformer_hist['val_r2'][best_trans_idx]-0.1),\n",
        "                 arrowprops=dict(facecolor=COLOR_RED, shrink=0.05), color=COLOR_RED)\n",
        "\n",
        "    ax2.set_title(\"Validation R2 Score Comparison (Higher is Better)\", fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel(\"Epochs\")\n",
        "    ax2.set_ylabel(\"R2 Score\")\n",
        "    ax2.legend(loc='lower right')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def analyze_results(lstm_hist, trans_hist):\n",
        "    \"\"\"\n",
        "    è‡ªåŠ¨ç”Ÿæˆç®€å•çš„åˆ†ææŠ¥å‘Š\n",
        "    \"\"\"\n",
        "    best_lstm = max(lstm_hist['val_r2'])\n",
        "    best_trans = max(trans_hist['val_r2'])\n",
        "\n",
        "    diff = best_trans - best_lstm\n",
        "\n",
        "    print(\"\\nğŸ§ ç»“æœæ·±åº¦åˆ†æ (Analysis Report)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ğŸ† æœ€ä½³ Bi-LSTM R2:    {best_lstm:.4f}\")\n",
        "    print(f\"ğŸ† æœ€ä½³ Transformer R2: {best_trans:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    if diff > 0.01:\n",
        "        print(\"âœ… ç»“è®ºï¼šTransformer èƒœå‡ºï¼\")\n",
        "        print(\"   åŸå› åˆ†æï¼š\")\n",
        "        print(\"   1. å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ (Self-Attention) æˆåŠŸæ•æ‰äº†åˆ†å­ä¸­è¿œè·ç¦»åŸå­é—´çš„ç›¸äº’ä½œç”¨ã€‚\")\n",
        "        print(\"   2. ä½ç½®ç¼–ç å¸®åŠ©æ¨¡å‹ç†è§£äº†åˆ†å­çš„æ‹“æ‰‘ç»“æ„é¡ºåºã€‚\")\n",
        "    elif diff < -0.01:\n",
        "        print(\"ğŸ”µ ç»“è®ºï¼šBi-LSTM èƒœå‡ºï¼ˆæˆ–å·®è·ä¸å¤§ï¼‰ã€‚\")\n",
        "        print(\"   åŸå› åˆ†æï¼š\")\n",
        "        print(\"   1. æ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼šTransformer æ˜¯æ•°æ®é¥¥æ¸´å‹æ¨¡å‹ï¼Œåœ¨å°æ ·æœ¬ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆã€‚\")\n",
        "        print(\"   2. å½’çº³åç½® (Inductive Bias)ï¼šRNN å¤©ç”Ÿé€‚åˆå¤„ç†åºåˆ—ï¼Œå¯¹äºç®€å•åˆ†å­ç»“æ„å¯èƒ½æ•ˆç‡æ›´é«˜ã€‚\")\n",
        "        print(\"   3. è°ƒå‚ç©ºé—´ï¼šTransformer å¯¹è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€Warmupï¼‰éå¸¸æ•æ„Ÿï¼Œå¯èƒ½éœ€è¦æ›´ç²¾ç»†çš„è°ƒä¼˜ã€‚\")\n",
        "    else:\n",
        "        print(\"ğŸ¤ ç»“è®ºï¼šä¸¤è€…æ——é¼“ç›¸å½“ã€‚\")\n",
        "        print(\"   è¿™è¡¨æ˜åœ¨è¯¥ç‰¹å®šæ•°æ®é›†å’Œç‰¹å¾å°ºåº¦ä¸‹ï¼Œåºåˆ—å»ºæ¨¡çš„ç“¶é¢ˆå¯èƒ½ä¸åœ¨äºé•¿è·ç¦»ä¾èµ–ï¼Œ\")\n",
        "        print(\"   æˆ–è€…å½“å‰çš„ Tokenizer å·²ç»è¶³å¤Ÿé«˜æ•ˆï¼Œè®©ç®€å•çš„ RNN ä¹Ÿèƒ½å­¦å¾—å¾ˆå¥½ã€‚\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# æ‰§è¡Œå…¥å£\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # ç¡®ä¿ä¹‹å‰çš„è®­ç»ƒå†å²å˜é‡å­˜åœ¨\n",
        "    # lstm_history = ... (æ¥è‡ªç¬¬äºŒéƒ¨åˆ†)\n",
        "    # transformer_history = ... (æ¥è‡ªç¬¬ä¸‰éƒ¨åˆ†)\n",
        "\n",
        "    if 'lstm_history' in globals() and 'transformer_history' in globals():\n",
        "        # 1. ç»˜åˆ¶å¯¹æ¯”å›¾\n",
        "        visualize_peak_showdown(lstm_history, transformer_history)\n",
        "\n",
        "        # 2. è¾“å‡ºåˆ†ææ–‡æœ¬\n",
        "        analyze_results(lstm_history, transformer_history)\n",
        "    else:\n",
        "        print(\"âš ï¸ ç¼ºå°‘è®­ç»ƒå†å²æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œç¬¬äºŒå’Œç¬¬ä¸‰éƒ¨åˆ†ä»£ç ï¼\")"
      ],
      "metadata": {
        "id": "MEKSxg2vcBzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¬¬äº”éƒ¨åˆ†ï¼šTransformer æ¶æ„å¤§æ‹“å±• (The \"Advanced\" Part)**"
      ],
      "metadata": {
        "id": "TQWstQ1JdzwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Decoder-only (GPT-style): æ— æ¡ä»¶åˆ†å­ç”Ÿæˆä¸æ¡ä»¶æ§åˆ¶åˆ†å­ç”Ÿæˆ"
      ],
      "metadata": {
        "id": "tHZp_3_ad6X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ==========================================\n",
        "# 1. MolGPT æ¨¡å‹å®šä¹‰ (Decoder-only)\n",
        "# ==========================================\n",
        "class MolGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, max_len=128, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        # 1. Embedding & Positional Encoding\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_encoder = nn.Embedding(max_len, d_model) # GPTå¸¸ç”¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç \n",
        "\n",
        "        # 2. Transformer Decoder (GPT style)\n",
        "        # ä½¿ç”¨ TransformerEncoder æ¥å®ç° GPTï¼Œé…åˆ causal mask å³å¯\n",
        "        # batch_first=True\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=256, batch_first=True, dropout=0.1)\n",
        "        self.transformer_decoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # 3. Output Head\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _generate_causal_mask(self, sz, device):\n",
        "        \"\"\"ç”Ÿæˆä¸Šä¸‰è§’æ©ç ï¼Œé˜²æ­¢å·çœ‹æœªæ¥\"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz, device=device)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Seq_Len]\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # Padding Mask (å¿½ç•¥ pad)\n",
        "        src_key_padding_mask = (x == self.pad_idx)\n",
        "\n",
        "        # Causal Mask (å¿½ç•¥æœªæ¥)\n",
        "        causal_mask = self._generate_causal_mask(seq_len, x.device)\n",
        "\n",
        "        # Embedding + Position\n",
        "        # ç”Ÿæˆä½ç½®ç´¢å¼•: [0, 1, 2, ..., seq_len-1]\n",
        "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model) + self.pos_encoder(positions)\n",
        "\n",
        "        # Transformer Forward\n",
        "        # æ³¨æ„ï¼šPyTorch çš„ TransformerEncoder å…è®¸ä¼ å…¥ mask åšå› æœæ¨æ–­\n",
        "        out = self.transformer_decoder(x, mask=causal_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Logits: [Batch, Seq_Len, Vocab_Size]\n",
        "        logits = self.fc_out(out)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, start_token_idx, max_new_tokens, tokenizer, temperature=1.0, stop_token_idx=None):\n",
        "        \"\"\"\n",
        "        ç”Ÿæˆå‡½æ•°ï¼šç»™å®šèµ·å§‹ tokenï¼Œè‡ªåŠ¨ç»­å†™\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        # idx: å½“å‰ç”Ÿæˆçš„åºåˆ— [1, 1] (å‡è®¾ batch=1)\n",
        "        idx = torch.tensor([[start_token_idx]], dtype=torch.long, device=next(self.parameters()).device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ä»¥å…ä½ç½®ç¼–ç æŠ¥é”™\n",
        "            idx_cond = idx if idx.size(1) <= 100 else idx[:, -100:]\n",
        "\n",
        "            # å‰å‘ä¼ æ’­\n",
        "            logits = self(idx_cond)\n",
        "\n",
        "            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # é‡‡æ · (Multinomial Sampling)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # æ‹¼æ¥åˆ°åºåˆ—ä¸­\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "            # é‡åˆ° EOS åœæ­¢\n",
        "            if stop_token_idx is not None and idx_next.item() == stop_token_idx:\n",
        "                break\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Oo5Jo2v-eAlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ— æ¡ä»¶åˆ†å­ç”Ÿæˆ"
      ],
      "metadata": {
        "id": "LDHryUcUAO5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ä»»åŠ¡ 1: æ— æ¡ä»¶ç”Ÿæˆ (Unconditional Generation) - ä¿®å¤ç‰ˆ\n",
        "# ==========================================\n",
        "\n",
        "# 1. å‡†å¤‡ç”Ÿæˆä»»åŠ¡çš„æ•°æ®é›†\n",
        "class GenDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.smiles = df['Smiles'].values\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.smiles[idx]\n",
        "        # ç¼–ç : [SOS, t1, t2, ..., EOS]\n",
        "        full_ids = self.tokenizer.encode(s)\n",
        "\n",
        "        # è¾“å…¥: [SOS, t1, t2, ..., tn] (å»æ‰æœ€åä¸€ä¸ª EOS)\n",
        "        # æ ‡ç­¾: [t1, t2, ..., tn, EOS] (å»æ‰ç¬¬ä¸€ä¸ª SOS)\n",
        "        # è¿™æ · input[i] é¢„æµ‹çš„å°±æ˜¯ label[i] (å³ next token)\n",
        "        inp = torch.tensor(full_ids[:-1], dtype=torch.long)\n",
        "        tgt = torch.tensor(full_ids[1:], dtype=torch.long)\n",
        "\n",
        "        return inp, tgt\n",
        "\n",
        "def train_molgpt_unconditional(tokenizer, df):\n",
        "    print(\"\\nğŸ§ª ä»»åŠ¡ä¸€ï¼šå¯åŠ¨ MolGPT æ— æ¡ä»¶ç”Ÿæˆè®­ç»ƒ...\")\n",
        "\n",
        "    # --- ä¿®å¤æ ¸å¿ƒï¼šcollate_fn ---\n",
        "    # è¿™é‡Œçš„ collate_fn å¾ˆé‡è¦ï¼Œéœ€è¦å¯¹ batch è¿›è¡ŒåŠ¨æ€ padding\n",
        "    def collate_fn(batch):\n",
        "        # æ‰¾å‡ºæœ¬ batch æœ€é•¿åºåˆ—\n",
        "        max_len = max([len(item[0]) for item in batch])\n",
        "        pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "\n",
        "        inps = []\n",
        "        tgts = []\n",
        "        for inp, tgt in batch:\n",
        "            # Padding\n",
        "            pad_len = max_len - len(inp)\n",
        "\n",
        "            # æ‹¼æ¥\n",
        "            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ˜¾å¼è½¬æ¢ padding tensor ä¸º longï¼Œé˜²æ­¢è‡ªåŠ¨æ¨æ–­ä¸º float\n",
        "            pad_tensor = torch.tensor([pad_id]*pad_len, dtype=torch.long)\n",
        "\n",
        "            inps.append(torch.cat([inp, pad_tensor]))\n",
        "            tgts.append(torch.cat([tgt, pad_tensor])) # target ä¹Ÿè¦ pad\n",
        "\n",
        "        # å…³é”®ä¿®å¤ï¼šåœ¨è¿”å›å‰å¼ºåˆ¶è½¬æ¢ä¸º .long()\n",
        "        # è¿™æ ·å°±èƒ½å®Œç¾è§£å†³ \"Expected tensor for argument #1 ... to have ... Long\" çš„æŠ¥é”™\n",
        "        return torch.stack(inps).long(), torch.stack(tgts).long()\n",
        "\n",
        "    # Dataloader\n",
        "    ds = GenDataset(df, tokenizer)\n",
        "    dl = DataLoader(ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Init Model\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MolGPT(vocab_size=len(tokenizer.vocab), pad_idx=tokenizer.vocab[tokenizer.pad_token]).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.vocab[tokenizer.pad_token]) # å¿½ç•¥ PAD çš„ loss\n",
        "\n",
        "    # Training Loop\n",
        "    losses = []\n",
        "    model.train()\n",
        "    for epoch in range(30): # è®­ç»ƒ 15 è½®\n",
        "        total_loss = 0\n",
        "        for inp, tgt in dl:\n",
        "            # åŒé‡ä¿é™©ï¼šé€å…¥ GPU æ—¶å†æ¬¡ç¡®ä¿æ˜¯ long ç±»å‹\n",
        "            inp, tgt = inp.to(DEVICE).long(), tgt.to(DEVICE).long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(inp) # [Batch, Seq, Vocab]\n",
        "\n",
        "            # Flatten for loss\n",
        "            # logits: [Batch*Seq, Vocab], tgt: [Batch*Seq]\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dl)\n",
        "        losses.append(avg_loss)\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Demo Generation\n",
        "    print(\"\\nğŸ² ç”Ÿæˆæ¼”ç¤º (Random Generation):\")\n",
        "    sos_id = tokenizer.vocab[tokenizer.sos_token]\n",
        "    eos_id = tokenizer.vocab[tokenizer.eos_token]\n",
        "\n",
        "    generated_mols = []\n",
        "    # ç¨å¾®ç”Ÿæˆå¤šå‡ ä¸ªæ¥çœ‹çœ‹æ•ˆæœ\n",
        "    for _ in range(5):\n",
        "        try:\n",
        "            gen_ids = model.generate(sos_id, max_new_tokens=50, tokenizer=tokenizer, stop_token_idx=eos_id)\n",
        "            # decode æ—¶è·³è¿‡ special tokens\n",
        "            smiles = tokenizer.decode(gen_ids[0].cpu().numpy(), skip_special_tokens=True)\n",
        "            generated_mols.append(smiles)\n",
        "            print(f\"  -> {smiles}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  -> ç”Ÿæˆå‡ºé”™: {e}\")\n",
        "\n",
        "    return model, losses\n",
        "\n",
        "# æ‰§è¡Œ\n",
        "if 'tokenizer' in globals() and 'raw_df' in globals():\n",
        "    gpt_model, gpt_losses = train_molgpt_unconditional(tokenizer, raw_df)\n",
        "\n",
        "    # ç”»ä¸ªç®€å•çš„ Loss æ›²çº¿\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    plt.plot(gpt_losses, color=COLOR_RED, marker='.')\n",
        "    plt.title(\"MolGPT Training Loss\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VBqiJpCHeD1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡\n",
        "torch.save(gpt_model.state_dict(), 'molgpt_unconditional.pt')\n",
        "print(\"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º molgpt_unconditional.pt\")"
      ],
      "metadata": {
        "id": "9FQhzaujfJFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem, rdBase\n",
        "from rdkit.Chem import Draw, Descriptors, QED, AllChem, DataStructs\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# ==========================================\n",
        "# 0. å…¨å±€é…ç½® (è­¦å‘Šå±è”½ & é…è‰²)\n",
        "# ==========================================\n",
        "# 1. å±è”½ RDKit çš„ \"please use MorganGenerator\" åˆ·å±è­¦å‘Š\n",
        "rdBase.DisableLog('rdApp.warning')\n",
        "rdBase.DisableLog('rdApp.error')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 2. ç»Ÿä¸€é…è‰²\n",
        "COLOR_BLUE = (75 / 255, 116 / 255, 178 / 255)\n",
        "COLOR_RED = (219 / 255, 49 / 255, 36 / 255)\n",
        "\n",
        "# 3. åŸºç¡€è®¾ç½®\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_PATH = 'molgpt_unconditional.pt'\n",
        "GEN_NUM = 500  # ç”Ÿæˆæ•°é‡\n",
        "\n",
        "# ==========================================\n",
        "# 1. åŠ è½½æ¨¡å‹ä¸æ‰¹é‡ç”Ÿæˆ\n",
        "# ==========================================\n",
        "print(\"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...\")\n",
        "# ç¡®ä¿ Tokenizer å·²åœ¨å†…å­˜ä¸­\n",
        "if 'tokenizer' not in globals():\n",
        "    raise ValueError(\"è¯·å…ˆè¿è¡Œç¬¬ä¸€éƒ¨åˆ†ä»£ç ä»¥åˆå§‹åŒ– tokenizerï¼\")\n",
        "\n",
        "pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "model = MolGPT(vocab_size=len(tokenizer.vocab), pad_idx=pad_id).to(DEVICE)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "    print(\"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_PATH}ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒä»£ç å¹¶ä¿å­˜æ¨¡å‹ã€‚\")\n",
        "\n",
        "def generate_molecules(model, tokenizer, num_generate=100):\n",
        "    generated_smiles = []\n",
        "    sos_id = tokenizer.vocab[tokenizer.sos_token]\n",
        "    eos_id = tokenizer.vocab[tokenizer.eos_token]\n",
        "\n",
        "    print(f\"\\nğŸ§ª å¼€å§‹ç”Ÿæˆ {num_generate} ä¸ªåˆ†å­...\")\n",
        "    # è¿›åº¦æ¡\n",
        "    for _ in tqdm(range(num_generate), desc=\"Generating\"):\n",
        "        try:\n",
        "            gen_ids = model.generate(sos_id, max_new_tokens=100, tokenizer=tokenizer, stop_token_idx=eos_id)\n",
        "            s = tokenizer.decode(gen_ids[0].cpu().numpy(), skip_special_tokens=True)\n",
        "            generated_smiles.append(s)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return generated_smiles\n",
        "\n",
        "# æ‰§è¡Œç”Ÿæˆ\n",
        "if 'model' in locals():\n",
        "    gen_smiles_list = generate_molecules(model, tokenizer, num_generate=GEN_NUM)\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. åˆæ³•æ€§æ£€æŸ¥ä¸æ€§è´¨è®¡ç®—\n",
        "    # ==========================================\n",
        "    print(\"\\nâš—ï¸ æ­£åœ¨è¿›è¡ŒåŒ–å­¦æ€§è´¨è®¡ç®—ä¸åˆæ³•æ€§æ£€æŸ¥...\")\n",
        "    valid_mols = []\n",
        "    valid_smiles = []\n",
        "    properties = {'QED': [], 'LogP': [], 'TPSA': []}\n",
        "\n",
        "    for s in gen_smiles_list:\n",
        "        mol = Chem.MolFromSmiles(s)\n",
        "        if mol is not None:\n",
        "            valid_mols.append(mol)\n",
        "            valid_smiles.append(s)\n",
        "            properties['QED'].append(QED.qed(mol))\n",
        "            properties['LogP'].append(Descriptors.MolLogP(mol))\n",
        "            properties['TPSA'].append(Descriptors.TPSA(mol))\n",
        "\n",
        "    validity = len(valid_mols) / GEN_NUM\n",
        "    print(f\"âœ… ç”Ÿæˆç»“æŸï¼š\")\n",
        "    print(f\"   - æ€»å°è¯•: {GEN_NUM}\")\n",
        "    print(f\"   - æœ‰æ•ˆåˆ†å­: {len(valid_mols)}\")\n",
        "    print(f\"   - æœ‰æ•ˆç‡ (Validity): {validity:.2%}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. ç»“æœå¯è§†åŒ– (ä¸¥æ ¼ä½¿ç”¨æŒ‡å®šé…è‰²)\n",
        "    # ==========================================\n",
        "\n",
        "    # --- 3.1 åˆ†å­ç»“æ„å±•ç¤º ---\n",
        "    print(\"\\nğŸ¨ ç»˜åˆ¶ç”Ÿæˆçš„åˆ†å­ç»“æ„ç¤ºä¾‹...\")\n",
        "    if len(valid_mols) > 0:\n",
        "        display_mols = valid_mols[:24] if len(valid_mols) >= 24 else valid_mols\n",
        "        img = Draw.MolsToGridImage(display_mols, molsPerRow=6, subImgSize=(200, 200),\n",
        "                                   legends=[f\"Gen #{i+1}\" for i in range(len(display_mols))])\n",
        "        display(img)\n",
        "\n",
        "    # --- 3.2 æ€§è´¨åˆ†å¸ƒå¯¹æ¯” (KDE Plot) ---\n",
        "    print(\"\\nğŸ“Š ç»˜åˆ¶æ€§è´¨åˆ†å¸ƒå¯¹æ¯”å›¾ (Train vs Generated)...\")\n",
        "    if len(valid_mols) > 0 and 'raw_df' in globals():\n",
        "        # å‡†å¤‡è®­ç»ƒé›†æ•°æ®ï¼ˆé‡‡æ ·ï¼‰\n",
        "        train_props = {'QED': [], 'LogP': [], 'TPSA': []}\n",
        "        sample_df = raw_df.sample(n=min(1000, len(raw_df)), random_state=42)\n",
        "\n",
        "        for s in sample_df['Smiles']:\n",
        "            m = Chem.MolFromSmiles(s)\n",
        "            if m:\n",
        "                train_props['QED'].append(QED.qed(m))\n",
        "                train_props['LogP'].append(Descriptors.MolLogP(m))\n",
        "                train_props['TPSA'].append(Descriptors.TPSA(m))\n",
        "\n",
        "        # ç»˜å›¾\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=120)\n",
        "\n",
        "        def plot_dist(ax, train_data, gen_data, title):\n",
        "            # è®­ç»ƒé›†ç”¨è“è‰² (Base)\n",
        "            sns.kdeplot(train_data, fill=True, color=COLOR_BLUE, label='Train Set', ax=ax, alpha=0.3, linewidth=0)\n",
        "            # ç”Ÿæˆé›†ç”¨çº¢è‰² (Generated)\n",
        "            sns.kdeplot(gen_data, fill=True, color=COLOR_RED, label='Generated', ax=ax, alpha=0.5, linewidth=2)\n",
        "            ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.2)\n",
        "\n",
        "        plot_dist(axes[0], train_props['QED'], properties['QED'], 'QED Distribution')\n",
        "        plot_dist(axes[1], train_props['LogP'], properties['LogP'], 'LogP Distribution')\n",
        "        plot_dist(axes[2], train_props['TPSA'], properties['TPSA'], 'TPSA Distribution')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # --- 3.3 ç›¸ä¼¼åº¦åˆ†å¸ƒ (Novelty) ---\n",
        "    print(\"\\nğŸ” è®¡ç®—ä¸è®­ç»ƒé›†çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ...\")\n",
        "    if len(valid_mols) > 0 and 'raw_df' in globals():\n",
        "        # è®¡ç®—æŒ‡çº¹\n",
        "        gen_fps = [AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=1024) for m in valid_mols]\n",
        "\n",
        "        train_mols_sample = [Chem.MolFromSmiles(s) for s in sample_df['Smiles']]\n",
        "        train_mols_sample = [m for m in train_mols_sample if m is not None]\n",
        "        train_fps = [AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=1024) for m in train_mols_sample]\n",
        "\n",
        "        max_sims = []\n",
        "        for g_fp in tqdm(gen_fps, desc=\"Computing Similarity\"):\n",
        "            if not train_fps: break\n",
        "            sims = DataStructs.BulkTanimotoSimilarity(g_fp, train_fps)\n",
        "            max_sims.append(max(sims) if sims else 0.0)\n",
        "\n",
        "        plt.figure(figsize=(8, 5), dpi=120)\n",
        "        # ä½¿ç”¨çº¢è‰²ç»˜åˆ¶åˆ†å¸ƒ\n",
        "        sns.histplot(max_sims, bins=30, kde=True, color=COLOR_RED, edgecolor='white', alpha=0.7)\n",
        "        plt.axvline(np.mean(max_sims), color=COLOR_BLUE, linestyle='--', linewidth=2, label=f'Mean: {np.mean(max_sims):.2f}')\n",
        "\n",
        "        plt.title(\"Max Similarity to Training Set (Novelty Check)\", fontsize=12, fontweight='bold')\n",
        "        plt.xlabel(\"Max Tanimoto Similarity\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.2)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ncLgpiw2fNtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¡ä»¶æ§åˆ¶åˆ†å­ç”Ÿæˆ"
      ],
      "metadata": {
        "id": "rtf8fiFSAS9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# ä»»åŠ¡ 2: å±æ€§æ¡ä»¶ç”Ÿæˆ (ä¿®å¤ç‰ˆ)\n",
        "# ==========================================\n",
        "\n",
        "def train_conditional_molgpt_and_save(base_tokenizer, df):\n",
        "    print(\"\\nğŸ›ï¸ ä»»åŠ¡äºŒï¼šå¯åŠ¨å±æ€§æ¡ä»¶ç”Ÿæˆè®­ç»ƒ (Ctrl+C for Molecules)...\")\n",
        "\n",
        "    # 1. æ‰©å±• Tokenizer (Add Control Tags)\n",
        "    cond_tokenizer = base_tokenizer\n",
        "\n",
        "    # æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ ï¼Œé¿å…é‡å¤æ·»åŠ \n",
        "    if \"<high>\" not in cond_tokenizer.vocab:\n",
        "        new_id_high = len(cond_tokenizer.vocab)\n",
        "        cond_tokenizer.vocab[\"<high>\"] = new_id_high\n",
        "        cond_tokenizer.inv_vocab[new_id_high] = \"<high>\"\n",
        "\n",
        "        new_id_low = len(cond_tokenizer.vocab)\n",
        "        cond_tokenizer.vocab[\"<low>\"] = new_id_low\n",
        "        cond_tokenizer.inv_vocab[new_id_low] = \"<low>\"\n",
        "        print(f\"âœ… å·²æ·»åŠ æ§åˆ¶ç¬¦: <high> (ID: {new_id_high}), <low> (ID: {new_id_low})\")\n",
        "\n",
        "    # ä¿å­˜æ‰©å±•åçš„è¯è¡¨\n",
        "    cond_tokenizer.save_vocab(\"smiles_vocab_conditional.json\")\n",
        "\n",
        "    # 2. å‡†å¤‡å¸¦æ¡ä»¶çš„æ•°æ®é›† (æ ¸å¿ƒä¿®å¤å¤„)\n",
        "    class CondDataset(Dataset):\n",
        "        def __init__(self, df, tokenizer):\n",
        "            self.data = []\n",
        "            self.tokenizer = tokenizer\n",
        "            self.high_token = tokenizer.vocab[\"<high>\"]\n",
        "            self.low_token = tokenizer.vocab[\"<low>\"]\n",
        "            self.sos_token = tokenizer.vocab[tokenizer.sos_token]\n",
        "            self.eos_token = tokenizer.vocab[tokenizer.eos_token]\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                s = row['Smiles']\n",
        "                val = row['logSw']\n",
        "\n",
        "                # [å…³é”®ä¿®å¤]: padding=False\n",
        "                # æˆ‘ä»¬åªæƒ³è¦çº¯å‡€çš„åŸå­åºåˆ—ï¼Œä¸è¦åœ¨ä¸­é—´å¤¹æ‚ pad\n",
        "                base_ids = tokenizer.encode(s, add_special_tokens=False, padding=False, truncation=True)\n",
        "\n",
        "                cond_token = self.high_token if val > 3.0 else self.low_token\n",
        "\n",
        "                # æ­£ç¡®çš„ç»“æ„: [SOS, COND, Atom1, Atom2, ..., EOS]\n",
        "                full_ids = [self.sos_token, cond_token] + base_ids + [self.eos_token]\n",
        "                self.data.append(full_ids)\n",
        "\n",
        "        def __len__(self): return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            ids = self.data[idx]\n",
        "            # è½¬æ¢ä¸º tensor\n",
        "            return torch.tensor(ids[:-1], dtype=torch.long), torch.tensor(ids[1:], dtype=torch.long)\n",
        "\n",
        "    # Collate Fn (åœ¨è¿™é‡Œç»Ÿä¸€åš Padding)\n",
        "    def collate_fn(batch):\n",
        "        # æ‰¾å‡ºæœ¬ batch æœ€é•¿åºåˆ—\n",
        "        max_len = max([len(item[0]) for item in batch])\n",
        "        pad_id = cond_tokenizer.vocab[cond_tokenizer.pad_token]\n",
        "\n",
        "        inps, tgts = [], []\n",
        "        for inp, tgt in batch:\n",
        "            pad_len = max_len - len(inp)\n",
        "            pad_tensor = torch.tensor([pad_id] * pad_len, dtype=torch.long)\n",
        "\n",
        "            # æ‹¼æ¥: åºåˆ— + Pad\n",
        "            inps.append(torch.cat([inp, pad_tensor]))\n",
        "            tgts.append(torch.cat([tgt, pad_tensor]))\n",
        "\n",
        "        return torch.stack(inps).long(), torch.stack(tgts).long()\n",
        "\n",
        "    # 3. è®­ç»ƒæµç¨‹\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # ç¡®ä¿ pad_idx æ­£ç¡®ä¼ å…¥\n",
        "    pad_id = cond_tokenizer.vocab[cond_tokenizer.pad_token]\n",
        "    model = MolGPT(vocab_size=len(cond_tokenizer.vocab), pad_idx=pad_id).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "    ds = CondDataset(df, cond_tokenizer)\n",
        "    dl = DataLoader(ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    losses = []\n",
        "    model.train()\n",
        "    print(\"Training...\", end=\"\")\n",
        "\n",
        "    EPOCHS = 30\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for inp, tgt in dl:\n",
        "            inp, tgt = inp.to(DEVICE), tgt.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(inp)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            # [å»ºè®®]: åŠ ä¸Šæ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            print(f\" [Ep{epoch+1} L:{avg_loss:.4f}]\", end=\"\")\n",
        "\n",
        "    print(\"\\nâœ… Training Done!\")\n",
        "\n",
        "    # 4. ä¿å­˜æ¨¡å‹\n",
        "    torch.save(model.state_dict(), 'molgpt_conditional.pt')\n",
        "    print(\"ğŸ’¾ æ¡ä»¶ç”Ÿæˆæ¨¡å‹å·²ä¿å­˜ä¸º molgpt_conditional.pt\")\n",
        "\n",
        "    # 5. ç»˜åˆ¶ Loss æ›²çº¿\n",
        "    plt.figure(figsize=(8, 4), dpi=120)\n",
        "    plt.plot(range(1, EPOCHS+1), losses, 'o-', color=COLOR_RED, linewidth=2, markersize=4)\n",
        "    plt.title(\"Conditional MolGPT Training Loss\", fontsize=12, fontweight='bold')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# æ‰§è¡Œè®­ç»ƒ\n",
        "if 'tokenizer' in globals() and 'raw_df' in globals():\n",
        "    train_conditional_molgpt_and_save(tokenizer, raw_df)"
      ],
      "metadata": {
        "id": "JNNvn0nVeGUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rdkit import Chem\n",
        "\n",
        "# ==========================================\n",
        "# 0. é…ç½®\n",
        "# ==========================================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MOLGPT_PATH = 'molgpt_conditional.pt'\n",
        "GEN_NUM = 50  # ä¿®æ”¹ä¸º 50ï¼Œç¡®ä¿æœ€åæ¯ç±»éƒ½æœ‰ 50 ä¸ªæœ‰æ•ˆåˆ†å­\n",
        "\n",
        "# ==========================================\n",
        "# 1. åŠ è½½ MolGPT ç”Ÿæˆæ¨¡å‹\n",
        "# ==========================================\n",
        "print(\"ğŸš€ [1/2] æ­£åœ¨åŠ è½½ MolGPT æ¡ä»¶ç”Ÿæˆæ¨¡å‹...\")\n",
        "\n",
        "# ç¡®ä¿ Tokenizer åŒ…å«æ§åˆ¶ç¬¦\n",
        "if \"<high>\" not in tokenizer.vocab:\n",
        "    raise ValueError(\"Tokenizer ç¼ºå°‘æ§åˆ¶ç¬¦ï¼Œè¯·ç¡®ä¿ä¹‹å‰çš„è®­ç»ƒä»£ç å·²è¿è¡Œï¼\")\n",
        "\n",
        "pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "\n",
        "# åˆå§‹åŒ–æ¨¡å‹ç»“æ„\n",
        "model = MolGPT(vocab_size=len(tokenizer.vocab), pad_idx=pad_id).to(DEVICE)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MOLGPT_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "    print(\"âœ… MolGPT æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ MolGPT æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ç”Ÿæˆè¿‡ç¨‹ (Temperature + Top-K) - ä¿®æ”¹ç‰ˆ\n",
        "# ==========================================\n",
        "def generate_batch(target_tag, num_generate=50, temperature=0.8, top_k=5, max_attempts=1000):\n",
        "    \"\"\"\n",
        "    ä¿®æ”¹é€»è¾‘ï¼šå¾ªç¯ç”Ÿæˆç›´åˆ°æ”¶é›†åˆ° num_generate ä¸ªæœ‰æ•ˆåˆ†å­ä¸ºæ­¢ã€‚\n",
        "    max_attempts: é˜²æ­¢æ¨¡å‹å´©åå¯¼è‡´æ­»å¾ªç¯çš„æœ€å¤§å°è¯•æ¬¡æ•°\n",
        "    \"\"\"\n",
        "    tag_id = tokenizer.vocab[target_tag]\n",
        "    sos_id = tokenizer.vocab[tokenizer.sos_token]\n",
        "    eos_id = tokenizer.vocab[tokenizer.eos_token]\n",
        "\n",
        "    # åˆå§‹è¾“å…¥: [SOS, TAG]\n",
        "    start_seq = torch.tensor([[sos_id, tag_id]], dtype=torch.long).to(DEVICE)\n",
        "    generated_smiles = []\n",
        "\n",
        "    print(f\"\\nğŸ§¬ æ­£åœ¨ç”Ÿæˆ {num_generate} ä¸ª {target_tag} åˆ†å­...\")\n",
        "\n",
        "    # ä½¿ç”¨ tqdm æ‰‹åŠ¨ç®¡ç†è¿›åº¦æ¡\n",
        "    pbar = tqdm(total=num_generate, desc=f\"Generating {target_tag}\")\n",
        "    attempts = 0\n",
        "\n",
        "    while len(generated_smiles) < num_generate:\n",
        "        attempts += 1\n",
        "        if attempts > max_attempts:\n",
        "            print(f\"\\nâš ï¸ è­¦å‘Šï¼šå°è¯•æ¬¡æ•°è¶…è¿‡ {max_attempts}ï¼Œæå‰åœæ­¢ã€‚\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            idx = start_seq\n",
        "            with torch.no_grad():\n",
        "                for _ in range(100): # Max Len\n",
        "                    logits = model(idx)\n",
        "                    # Temperature Scaling\n",
        "                    logits = logits[:, -1, :] / temperature\n",
        "\n",
        "                    # Top-K Filtering\n",
        "                    if top_k is not None:\n",
        "                        v, _ = torch.topk(logits, top_k)\n",
        "                        logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    next_token = torch.multinomial(probs, 1)\n",
        "\n",
        "                    if next_token.item() == eos_id:\n",
        "                        break\n",
        "                    idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "            # Decode\n",
        "            out_ids = idx[0].cpu().numpy()\n",
        "            if len(out_ids) > 2:\n",
        "                # è·³è¿‡ SOS å’Œ TAG\n",
        "                smiles = tokenizer.decode(out_ids[2:], skip_special_tokens=True)\n",
        "                # æ£€æŸ¥åˆæ³•æ€§\n",
        "                if Chem.MolFromSmiles(smiles):\n",
        "                    generated_smiles.append(smiles)\n",
        "                    pbar.update(1) # æœ‰æ•ˆåˆ†å­+1ï¼Œè¿›åº¦æ¡æ›´æ–°\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    pbar.close()\n",
        "    return generated_smiles\n",
        "\n",
        "# æ‰§è¡Œç”Ÿæˆ\n",
        "high_smiles = generate_batch(\"<high>\", num_generate=GEN_NUM, temperature=0.8)\n",
        "low_smiles = generate_batch(\"<low>\", num_generate=GEN_NUM, temperature=0.8)\n",
        "\n",
        "print(f\"\\nâœ… ç”Ÿæˆç»Ÿè®¡: High Valid: {len(high_smiles)}, Low Valid: {len(low_smiles)}\")"
      ],
      "metadata": {
        "id": "y7zvnjwMghbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, QED, Draw\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from scipy import stats\n",
        "\n",
        "# ==========================================\n",
        "# 0. é…ç½®ä¸é…è‰²\n",
        "# ==========================================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- è¿™é‡Œç›´æ¥æŒ‡å®šå¥½è·¯å¾„ ---\n",
        "SWEETNESS_MODEL_PATH = '/content/transformer_sweetness.pt'\n",
        "\n",
        "COLOR_BLUE = (75 / 255, 116 / 255, 178 / 255) # Low Sweetness\n",
        "COLOR_RED = (219 / 255, 49 / 255, 36 / 255)   # High Sweetness\n",
        "\n",
        "# ==========================================\n",
        "# 1. ç†åŒ–æ€§è´¨åˆ†å¸ƒå¯¹æ¯” (LogP, QED, TPSA)\n",
        "# ==========================================\n",
        "print(\"\\nğŸ“Š [1/4] è®¡ç®—å¹¶ç»˜åˆ¶ç†åŒ–æ€§è´¨åˆ†å¸ƒ...\")\n",
        "data = []\n",
        "\n",
        "# æ”¶é›† High æ•°æ®\n",
        "for s in high_smiles:\n",
        "    mol = Chem.MolFromSmiles(s)\n",
        "    if mol:\n",
        "        data.append({\n",
        "            \"Type\": \"High Sweetness (Gen)\",\n",
        "            \"LogP\": Descriptors.MolLogP(mol),\n",
        "            \"QED\": QED.qed(mol),\n",
        "            \"TPSA\": Descriptors.TPSA(mol)\n",
        "        })\n",
        "\n",
        "# æ”¶é›† Low æ•°æ®\n",
        "for s in low_smiles:\n",
        "    mol = Chem.MolFromSmiles(s)\n",
        "    if mol:\n",
        "        data.append({\n",
        "            \"Type\": \"Low Sweetness (Gen)\",\n",
        "            \"LogP\": Descriptors.MolLogP(mol),\n",
        "            \"QED\": QED.qed(mol),\n",
        "            \"TPSA\": Descriptors.TPSA(mol)\n",
        "        })\n",
        "\n",
        "df_res = pd.DataFrame(data)\n",
        "\n",
        "if not df_res.empty:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=120)\n",
        "    my_palette = {\"High Sweetness (Gen)\": COLOR_RED, \"Low Sweetness (Gen)\": COLOR_BLUE}\n",
        "\n",
        "    sns.violinplot(data=df_res, x=\"Type\", y=\"LogP\", palette=my_palette, ax=axes[0])\n",
        "    axes[0].set_title(\"LogP Distribution\")\n",
        "\n",
        "    sns.violinplot(data=df_res, x=\"Type\", y=\"QED\", palette=my_palette, ax=axes[1])\n",
        "    axes[1].set_title(\"QED Distribution\")\n",
        "\n",
        "    sns.violinplot(data=df_res, x=\"Type\", y=\"TPSA\", palette=my_palette, ax=axes[2])\n",
        "    axes[2].set_title(\"TPSA Distribution\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œç»˜å›¾\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. å®šä¹‰é¢„æµ‹æ¨¡å‹ç»“æ„ (ç”¨äºåŠ è½½æƒé‡)\n",
        "# ==========================================\n",
        "# å¿…é¡»é‡æ–°å®šä¹‰ç±»ï¼Œå¦åˆ™ pickle åŠ è½½ä¼šæŠ¥é”™\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerRegressor(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True, dropout=0.1)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.regressor = nn.Sequential(nn.Linear(d_model, 64), nn.ReLU(), nn.Dropout(0.1), nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        src_key_padding_mask = (x == self.pad_idx)\n",
        "        x = self.embedding(x) * math.sqrt(x.size(-1))\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        mask_expanded = (~src_key_padding_mask).unsqueeze(-1).float()\n",
        "        sum_embeddings = torch.sum(x * mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "        feature = sum_embeddings / sum_mask\n",
        "        return self.regressor(feature).squeeze(-1)\n",
        "\n",
        "# ==========================================\n",
        "# 3. ç”œåº¦é¢„æµ‹ä¸éªŒè¯å¯è§†åŒ–\n",
        "# ==========================================\n",
        "print(\"\\nğŸ§ª [2/4] åŠ è½½é¢„æµ‹æ¨¡å‹å¹¶è¯„ä¼°ç”Ÿæˆåˆ†å­...\")\n",
        "\n",
        "pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "sweet_predictor = TransformerRegressor(vocab_size=46, pad_idx=pad_id).to(DEVICE)\n",
        "# sweet_predictor = TransformerRegressor(vocab_size=len(tokenizer.vocab), pad_idx=pad_id).to(DEVICE)\n",
        "\n",
        "try:\n",
        "    # åŠ è½½æŒ‡å®šçš„ Checkpoint\n",
        "    sweet_predictor.load_state_dict(torch.load(SWEETNESS_MODEL_PATH, map_location=DEVICE))\n",
        "    sweet_predictor.eval()\n",
        "    print(f\"âœ… ç”œåº¦é¢„æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ: {SWEETNESS_MODEL_PATH}\")\n",
        "\n",
        "    # å®šä¹‰é¢„æµ‹å‡½æ•°\n",
        "    def predict_sweetness(smiles_list):\n",
        "        if not smiles_list: return []\n",
        "        batch_ids = []\n",
        "        for s in smiles_list:\n",
        "            # è¿™é‡Œçš„ encode é»˜è®¤è¡Œä¸ºåº”è¯¥ä¸è®­ç»ƒ Regressor æ—¶ä¸€è‡´ (åŒ…å« SOS/EOS)\n",
        "            ids = tokenizer.encode(s)\n",
        "            batch_ids.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "        inp = pad_sequence(batch_ids, batch_first=True, padding_value=pad_id).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            preds = sweet_predictor(inp)\n",
        "        return preds.cpu().numpy()\n",
        "\n",
        "    # æ‰§è¡Œé¢„æµ‹\n",
        "    print(\"   æ­£åœ¨é¢„æµ‹ High ç»„ç”œåº¦...\")\n",
        "    pred_high = predict_sweetness(high_smiles)\n",
        "    print(\"   æ­£åœ¨é¢„æµ‹ Low ç»„ç”œåº¦...\")\n",
        "    pred_low = predict_sweetness(low_smiles)\n",
        "\n",
        "    # æ‰“å°ç»Ÿè®¡ç»“æœ\n",
        "    print(f\"\\nğŸ“ˆ ç»Ÿè®¡ç»“æœ:\")\n",
        "    print(f\"   Avg Sweetness (High Gen): {np.mean(pred_high):.3f}\")\n",
        "    print(f\"   Avg Sweetness (Low Gen):  {np.mean(pred_low):.3f}\")\n",
        "\n",
        "    # T-test æ£€éªŒ\n",
        "    if len(pred_high) > 1 and len(pred_low) > 1:\n",
        "        t_stat, p_val = stats.ttest_ind(pred_high, pred_low, equal_var=False)\n",
        "        print(f\"   T-test p-value: {p_val:.2e}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. ä¿å­˜æ•°æ®åˆ° CSV (æ–°å¢åŠŸèƒ½)\n",
        "    # ==========================================\n",
        "    print(\"\\nğŸ’¾ [3/4] æ­£åœ¨ä¿å­˜ç”Ÿæˆæ•°æ®åˆ° CSV...\")\n",
        "    df_high = pd.DataFrame({\n",
        "        'SMILES': high_smiles,\n",
        "        'Predicted_LogSw': pred_high,\n",
        "        'Type': 'High Sweetness'\n",
        "    })\n",
        "\n",
        "    df_low = pd.DataFrame({\n",
        "        'SMILES': low_smiles,\n",
        "        'Predicted_LogSw': pred_low,\n",
        "        'Type': 'Low Sweetness'\n",
        "    })\n",
        "\n",
        "    df_final = pd.concat([df_high, df_low], ignore_index=True)\n",
        "    csv_filename = 'generated_molecules_prediction.csv'\n",
        "    df_final.to_csv(csv_filename, index=False)\n",
        "    print(f\"âœ… æ–‡ä»¶å·²ä¿å­˜: {csv_filename} (åŒ…å« {len(df_final)} æ¡è®°å½•)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 5. ç»˜åˆ¶ç”œåº¦åˆ†å¸ƒå›¾\n",
        "    # ==========================================\n",
        "    print(\"\\nğŸ¨ [4/4] ç»˜åˆ¶ç”œåº¦åˆ†å¸ƒä¸åˆ†å­ç»“æ„...\")\n",
        "    plt.figure(figsize=(10, 6), dpi=120)\n",
        "\n",
        "    # ä½¿ç”¨ KDE Plot å¯è§†åŒ–åˆ†å¸ƒå·®å¼‚\n",
        "    sns.kdeplot(pred_high, fill=True, color=COLOR_RED, label=f'High Sweetness Gen (Mean:{np.mean(pred_high):.2f})', alpha=0.5)\n",
        "    sns.kdeplot(pred_low, fill=True, color=COLOR_BLUE, label=f'Low Sweetness Gen (Mean:{np.mean(pred_low):.2f})', alpha=0.5)\n",
        "\n",
        "    plt.axvline(3.0, color='gray', linestyle='--', label='Threshold (3.0)')\n",
        "    plt.title(\"Predicted Sweetness Distribution: High vs Low Control\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel(\"Predicted Log Sweetness (logSw)\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # ==========================================\n",
        "    # 6. éšæœºå¯è§†åŒ–åˆ†å­ç»“æ„ (æ–°å¢åŠŸèƒ½)\n",
        "    # ==========================================\n",
        "    def visualize_random_samples(smiles_list, preds, title, n=10):\n",
        "        # å¦‚æœæ•°é‡ä¸è¶³ nï¼Œåˆ™å…¨éƒ¨æ˜¾ç¤º\n",
        "        if len(smiles_list) < n:\n",
        "            n = len(smiles_list)\n",
        "\n",
        "        # éšæœºé‡‡æ ·ç´¢å¼•\n",
        "        indices = random.sample(range(len(smiles_list)), n)\n",
        "\n",
        "        mols = []\n",
        "        legends = []\n",
        "        for idx in indices:\n",
        "            s = smiles_list[idx]\n",
        "            p = preds[idx]\n",
        "            mol = Chem.MolFromSmiles(s)\n",
        "            if mol:\n",
        "                mols.append(mol)\n",
        "                legends.append(f\"Pred: {p:.2f}\")\n",
        "\n",
        "        if mols:\n",
        "            img = Draw.MolsToGridImage(mols, molsPerRow=5, subImgSize=(200, 200), legends=legends, returnPNG=False)\n",
        "            print(f\"\\nğŸ–¼ï¸ {title} (Random {n} Samples)\")\n",
        "            display(img)\n",
        "\n",
        "    # å¯è§†åŒ–é«˜ç”œåˆ†å­\n",
        "    visualize_random_samples(high_smiles, pred_high, \"High Sweetness Generated Molecules\")\n",
        "\n",
        "    # å¯è§†åŒ–ä½ç”œåˆ†å­\n",
        "    visualize_random_samples(low_smiles, pred_low, \"Low Sweetness Generated Molecules\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ æœªæ‰¾åˆ°é¢„æµ‹æ¨¡å‹æ–‡ä»¶ {SWEETNESS_MODEL_PATH}ï¼Œè¯·ç¡®è®¤è·¯å¾„æ­£ç¡®æˆ–å…ˆè¿è¡Œ Transformer è®­ç»ƒä»£ç ã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}\")"
      ],
      "metadata": {
        "id": "ogPP515yFZgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Encoder-Decoder (Seq2Seq):åˆ†å­ä¼˜åŒ–"
      ],
      "metadata": {
        "id": "irmlUVfTkYVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç† (é…å¯¹ä¸æ„å»ºæ•°æ®é›†)"
      ],
      "metadata": {
        "id": "AEErKZ1bofo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, DataStructs\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ„é€ ä¼˜åŒ–å¯¹ (Optimization Pairs)\n",
        "# ==========================================\n",
        "def construct_optimization_dataset(df):\n",
        "    print(\"MATCHING: æ­£åœ¨æ„å»ºåˆ†å­ä¼˜åŒ–å¯¹ (Low -> High)...\")\n",
        "\n",
        "    # 1. åˆ†ç»„\n",
        "    low_df = df[df['logSw'] <= 3.0].reset_index(drop=True)\n",
        "    high_df = df[df['logSw'] > 3.0].reset_index(drop=True)\n",
        "\n",
        "    print(f\"   - ä½ç”œåˆ†å­æ±  (Source): {len(low_df)}\")\n",
        "    print(f\"   - é«˜ç”œåˆ†å­æ±  (Target): {len(high_df)}\")\n",
        "\n",
        "    # 2. è®¡ç®—æŒ‡çº¹\n",
        "    def get_fp(smiles):\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol: return AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
        "        return None\n",
        "\n",
        "    print(\"   - è®¡ç®—åˆ†å­æŒ‡çº¹ä¸­...\")\n",
        "    low_fps = [get_fp(s) for s in low_df['Smiles']]\n",
        "    high_fps = [get_fp(s) for s in high_df['Smiles']]\n",
        "\n",
        "    # 3. å¯»æ‰¾æœ€è¿‘é‚» (Pairing)\n",
        "    pairs = []\n",
        "    # è¿‡æ»¤æ— æ•ˆæŒ‡çº¹\n",
        "    valid_high_indices = [i for i, fp in enumerate(high_fps) if fp is not None]\n",
        "    high_fps_valid = [high_fps[i] for i in valid_high_indices]\n",
        "\n",
        "    for i, fp_query in enumerate(tqdm(low_fps, desc=\"Pairing Molecules\")):\n",
        "        if fp_query is None: continue\n",
        "\n",
        "        # è®¡ç®—å½“å‰ä½ç”œåˆ†å­ä¸æ‰€æœ‰é«˜ç”œåˆ†å­çš„ç›¸ä¼¼åº¦\n",
        "        sims = DataStructs.BulkTanimotoSimilarity(fp_query, high_fps_valid)\n",
        "\n",
        "        if not sims: continue\n",
        "\n",
        "        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„é‚£ä¸ªé«˜ç”œåˆ†å­\n",
        "        best_local_idx = np.argmax(sims)\n",
        "        max_sim = sims[best_local_idx]\n",
        "\n",
        "        # ç­›é€‰é€»è¾‘ï¼š\n",
        "        # æ”¾å®½ä¸€ç‚¹ä¸‹é™ï¼Œä¿è¯æœ‰è¶³å¤Ÿçš„æ•°æ®è®­ç»ƒ\n",
        "        if 0.35 < max_sim < 0.98:\n",
        "            src_smiles = low_df.iloc[i]['Smiles']\n",
        "            true_high_idx = valid_high_indices[best_local_idx]\n",
        "            tgt_smiles = high_df.iloc[true_high_idx]['Smiles']\n",
        "            pairs.append((src_smiles, tgt_smiles))\n",
        "\n",
        "    print(f\"âœ… é…å¯¹å®Œæˆ: æ‰¾åˆ° {len(pairs)} å¯¹åˆç†çš„ä¼˜åŒ–è·¯å¾„\")\n",
        "    return pairs\n",
        "\n",
        "# ==========================================\n",
        "# 2. æ•°æ®é›†å®šä¹‰ (ä¿®å¤ Padding é—®é¢˜)\n",
        "# ==========================================\n",
        "class OptimizationDataset(Dataset):\n",
        "    def __init__(self, pairs, tokenizer):\n",
        "        self.pairs = pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sos_id = tokenizer.vocab[tokenizer.sos_token]\n",
        "        self.eos_id = tokenizer.vocab[tokenizer.eos_token]\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_s, tgt_s = self.pairs[idx]\n",
        "\n",
        "        # [å…³é”®ä¿®å¤]: padding=False\n",
        "        # ç¡®ä¿ Dataset è¿”å›çš„æ˜¯çº¯å‡€çš„ Token åºåˆ—ï¼Œä¸åŒ…å« pad\n",
        "        src_ids = self.tokenizer.encode(src_s, add_special_tokens=False, padding=False)\n",
        "        tgt_ids = self.tokenizer.encode(tgt_s, add_special_tokens=False, padding=False)\n",
        "\n",
        "        # æ„å»ºå¼ é‡\n",
        "        return {\n",
        "            'src': torch.tensor(src_ids + [self.eos_id], dtype=torch.long),\n",
        "            'tgt_in': torch.tensor([self.sos_id] + tgt_ids, dtype=torch.long),\n",
        "            'tgt_out': torch.tensor(tgt_ids + [self.eos_id], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "    # ä½¿ç”¨ pad_sequence ç»Ÿä¸€åœ¨ batch çº§åˆ«è¿›è¡Œ padding\n",
        "    srcs = pad_sequence([x['src'] for x in batch], batch_first=True, padding_value=pad_id)\n",
        "    tgt_ins = pad_sequence([x['tgt_in'] for x in batch], batch_first=True, padding_value=pad_id)\n",
        "    tgt_outs = pad_sequence([x['tgt_out'] for x in batch], batch_first=True, padding_value=pad_id)\n",
        "    return srcs, tgt_ins, tgt_outs\n",
        "\n",
        "# --- æ‰§è¡Œé¢„å¤„ç† ---\n",
        "if 'tokenizer' in globals() and 'raw_df' in globals():\n",
        "    opt_pairs = construct_optimization_dataset(raw_df)\n",
        "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
        "    if len(opt_pairs) > 0:\n",
        "        train_pairs, test_pairs = train_test_split(opt_pairs, test_size=0.1, random_state=42)\n",
        "\n",
        "        train_ds = OptimizationDataset(train_pairs, tokenizer)\n",
        "        train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "        print(f\"ğŸ“Š æ•°æ®å‡†å¤‡å°±ç»ª: Train={len(train_pairs)}, Test={len(test_pairs)}\")\n",
        "    else:\n",
        "        print(\"âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ°åŒ¹é…çš„åˆ†å­å¯¹ï¼Œè¯·æ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼æˆ–æ•°æ®é‡ã€‚\")"
      ],
      "metadata": {
        "id": "3aYEwJTwkabq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒä¸ä¿å­˜"
      ],
      "metadata": {
        "id": "DhusSFvfojHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 3. Seq2Seq æ¨¡å‹æ¶æ„\n",
        "# ==========================================\n",
        "class MolSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        # ç®€å•çš„ä½ç½®ç¼–ç  Embedding\n",
        "        self.pos_encoder = nn.Embedding(1024, d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            num_encoder_layers=num_layers, num_decoder_layers=num_layers,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True, dropout=0.1\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: [Batch, Src_Len]\n",
        "        # tgt: [Batch, Tgt_Len]\n",
        "\n",
        "        # 1. Masks\n",
        "        src_pad_mask = (src == self.pad_idx)\n",
        "        tgt_pad_mask = (tgt == self.pad_idx)\n",
        "        # Decoder è‡ªå›å½’æ©ç  (é˜²æ­¢çœ‹æœªæ¥)\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
        "\n",
        "        # 2. Embed + Pos\n",
        "        src_seq = torch.arange(src.size(1), device=src.device).unsqueeze(0)\n",
        "        tgt_seq = torch.arange(tgt.size(1), device=tgt.device).unsqueeze(0)\n",
        "\n",
        "        # é™åˆ¶ä½ç½®ç´¢å¼•èŒƒå›´ï¼Œé˜²æ­¢è¿‡é•¿æŠ¥é”™\n",
        "        src_seq = src_seq.clamp(max=1023)\n",
        "        tgt_seq = tgt_seq.clamp(max=1023)\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model) + self.pos_encoder(src_seq)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) + self.pos_encoder(tgt_seq)\n",
        "\n",
        "        # 3. Transformer Forward\n",
        "        out = self.transformer(\n",
        "            src=src_emb, tgt=tgt_emb,\n",
        "            src_key_padding_mask=src_pad_mask,\n",
        "            tgt_key_padding_mask=tgt_pad_mask,\n",
        "            memory_key_padding_mask=src_pad_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "\n",
        "        return self.fc_out(out)\n",
        "\n",
        "# ==========================================\n",
        "# 4. è®­ç»ƒå¾ªç¯\n",
        "# ==========================================\n",
        "def train_optimization_model():\n",
        "    print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒåˆ†å­ä¼˜åŒ–æ¨¡å‹ (Seq2Seq)...\")\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "\n",
        "    model = MolSeq2Seq(len(tokenizer.vocab), pad_idx=pad_id).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "    losses = []\n",
        "    model.train()\n",
        "\n",
        "    EPOCHS = 30\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for src, tgt_in, tgt_out in train_dl:\n",
        "            src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(src, tgt_in)\n",
        "\n",
        "            # Flatten for loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            print(f\"Epoch {epoch+1:02d} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"âœ… è®­ç»ƒå®Œæˆ\")\n",
        "\n",
        "    # ä¿å­˜æ¨¡å‹\n",
        "    torch.save(model.state_dict(), 'mol_optimization.pt')\n",
        "    print(\"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º mol_optimization.pt\")\n",
        "\n",
        "    # ç»˜åˆ¶ Loss\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.plot(losses, color='green', marker='o')\n",
        "    plt.title(\"Optimization Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- æ‰§è¡Œè®­ç»ƒ ---\n",
        "if 'train_dl' in globals():\n",
        "    opt_model = train_optimization_model()"
      ],
      "metadata": {
        "id": "NN1u0Yfyok0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹æ¨ç†ã€ç”Ÿæˆä¸ç»“æ„å¯è§†åŒ–"
      ],
      "metadata": {
        "id": "nsVMMMp_omra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# ==========================================\n",
        "# 0. é…ç½®ä¸ç±»å®šä¹‰ (ä¸ºäº†åŠ è½½ Sweetness æ¨¡å‹)\n",
        "# ==========================================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SWEETNESS_MODEL_PATH = '/content/transformer_sweetness.pt'\n",
        "OPT_MODEL_PATH = 'mol_optimization.pt'\n",
        "\n",
        "COLOR_BLUE = (75 / 255, 116 / 255, 178 / 255) # Low\n",
        "COLOR_GREEN = (46 / 255, 139 / 255, 87 / 255) # Optimized\n",
        "\n",
        "# --- é‡å¤å®šä¹‰ TransformerRegressor ä»¥ä¾¿åŠ è½½ Checkpoint ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerRegressor(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True, dropout=0.1)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.regressor = nn.Sequential(nn.Linear(d_model, 64), nn.ReLU(), nn.Dropout(0.1), nn.Linear(64, 1))\n",
        "    def forward(self, x):\n",
        "        src_key_padding_mask = (x == self.pad_idx)\n",
        "        x = self.embedding(x) * math.sqrt(x.size(-1))\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        mask_expanded = (~src_key_padding_mask).unsqueeze(-1).float()\n",
        "        sum_embeddings = torch.sum(x * mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "        feature = sum_embeddings / sum_mask\n",
        "        return self.regressor(feature).squeeze(-1)\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ ¸å¿ƒè¯„ä¼°å‡½æ•°\n",
        "# ==========================================\n",
        "def evaluate_optimization(test_pairs, tokenizer, num_samples=100):\n",
        "    print(\"\\nğŸ” æ­£åœ¨è¿›è¡Œåˆ†å­ä¼˜åŒ–æ¨ç†ä¸è¯„ä¼°...\")\n",
        "\n",
        "    pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "    sos_id = tokenizer.vocab[tokenizer.sos_token]\n",
        "    eos_id = tokenizer.vocab[tokenizer.eos_token]\n",
        "\n",
        "    # --- åŠ è½½ä¸¤ä¸ªæ¨¡å‹ ---\n",
        "    # 1. ä¼˜åŒ–æ¨¡å‹\n",
        "    opt_model = MolSeq2Seq(len(tokenizer.vocab), pad_idx=pad_id).to(DEVICE)\n",
        "    try:\n",
        "        opt_model.load_state_dict(torch.load(OPT_MODEL_PATH, map_location=DEVICE))\n",
        "        opt_model.eval()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°ä¼˜åŒ–æ¨¡å‹ {OPT_MODEL_PATH}\")\n",
        "        return\n",
        "\n",
        "    # 2. ç”œåº¦é¢„æµ‹æ¨¡å‹\n",
        "    # sweet_model = TransformerRegressor(vocab_size=len(tokenizer.vocab), pad_idx=pad_id).to(DEVICE)\n",
        "    sweet_model = TransformerRegressor(vocab_size=46, pad_idx=pad_id).to(DEVICE)\n",
        "    try:\n",
        "        sweet_model.load_state_dict(torch.load(SWEETNESS_MODEL_PATH, map_location=DEVICE))\n",
        "        sweet_model.eval()\n",
        "        print(\"âœ… æˆåŠŸåŠ è½½: ä¼˜åŒ–æ¨¡å‹ & ç”œåº¦é¢„æµ‹æ¨¡å‹\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°ç”œåº¦é¢„æµ‹æ¨¡å‹ {SWEETNESS_MODEL_PATH}\")\n",
        "        return\n",
        "\n",
        "    # --- è¾…åŠ©ï¼šç”œåº¦é¢„æµ‹å‡½æ•° ---\n",
        "    def get_pred_score(smiles):\n",
        "        if not smiles: return 0.0\n",
        "        ids = tokenizer.encode(smiles)\n",
        "        inp = torch.tensor([ids], dtype=torch.long).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            score = sweet_model(inp)\n",
        "        return score.item()\n",
        "\n",
        "    # --- å¼€å§‹æ¨ç† ---\n",
        "    results = []\n",
        "\n",
        "    # å¦‚æœæµ‹è¯•é›†ä¸å¤Ÿå¤§ï¼Œå°±å…¨éƒ¨ä½¿ç”¨\n",
        "    eval_indices = range(len(test_pairs))\n",
        "    if len(test_pairs) > num_samples:\n",
        "        eval_indices = random.sample(range(len(test_pairs)), num_samples)\n",
        "\n",
        "    print(f\"   å¤„ç† {len(eval_indices)} ä¸ªæ ·æœ¬...\")\n",
        "\n",
        "    for idx in eval_indices:\n",
        "        src_s, tgt_s = test_pairs[idx] # src=ä½ç”œ(Input), tgt=é«˜ç”œ(Ground Truth)\n",
        "\n",
        "        # 1. è¿è¡Œä¼˜åŒ–æ¨¡å‹ (Source -> Optimized)\n",
        "        src_ids = tokenizer.encode(src_s, add_special_tokens=False) + [eos_id]\n",
        "        src_tensor = torch.tensor([src_ids], dtype=torch.long).to(DEVICE)\n",
        "        curr_tgt = torch.tensor([[sos_id]], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        # Greedy Decode\n",
        "        with torch.no_grad():\n",
        "            for _ in range(100):\n",
        "                logits = opt_model(src_tensor, curr_tgt)\n",
        "                next_token = logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
        "                if next_token.item() == eos_id: break\n",
        "                curr_tgt = torch.cat([curr_tgt, next_token], dim=1)\n",
        "\n",
        "        pred_ids = curr_tgt[0].cpu().numpy()[1:]\n",
        "        opt_s = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "        # 2. æ£€æŸ¥æœ‰æ•ˆæ€§ & é¢„æµ‹ç”œåº¦\n",
        "        mol_src = Chem.MolFromSmiles(src_s)\n",
        "        mol_opt = Chem.MolFromSmiles(opt_s)\n",
        "\n",
        "        if mol_src and mol_opt:\n",
        "            score_src = get_pred_score(src_s)\n",
        "            score_opt = get_pred_score(opt_s)\n",
        "\n",
        "            results.append({\n",
        "                'Source_SMILES': src_s,\n",
        "                'Source_LogSw': score_src,\n",
        "                'Optimized_SMILES': opt_s,\n",
        "                'Optimized_LogSw': score_opt,\n",
        "                'Improvement': score_opt - score_src,\n",
        "                'Valid': True\n",
        "            })\n",
        "        else:\n",
        "             results.append({\n",
        "                'Source_SMILES': src_s,\n",
        "                'Optimized_SMILES': opt_s,\n",
        "                'Valid': False\n",
        "            })\n",
        "\n",
        "    # --- ä¿å­˜ç»“æœ ---\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_valid = df_res[df_res['Valid'] == True]\n",
        "\n",
        "    csv_path = 'optimization_results.csv'\n",
        "    df_valid.to_csv(csv_path, index=False)\n",
        "    print(f\"âœ… ç»“æœå·²ä¿å­˜è‡³ {csv_path} (æœ‰æ•ˆæ ·æœ¬: {len(df_valid)}/{len(df_res)})\")\n",
        "\n",
        "    if df_valid.empty:\n",
        "        print(\"âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ä¼˜åŒ–åˆ†å­ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è®­ç»ƒæƒ…å†µã€‚\")\n",
        "        return\n",
        "\n",
        "    # --- å¯è§†åŒ– 1: ç”œåº¦åˆ†å¸ƒå¯¹æ¯” ---\n",
        "    plt.figure(figsize=(10, 6), dpi=120)\n",
        "    sns.kdeplot(df_valid['Source_LogSw'], fill=True, color=COLOR_BLUE, label='Original (Low Sweet)', alpha=0.5)\n",
        "    sns.kdeplot(df_valid['Optimized_LogSw'], fill=True, color=COLOR_GREEN, label='Optimized (AI Generated)', alpha=0.5)\n",
        "    plt.axvline(3.0, color='gray', linestyle='--', label='Target Threshold (3.0)')\n",
        "    plt.title(\"Sweetness Optimization Effect\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel(\"Predicted Log Sweetness\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"   å¹³å‡æå‡ (Improvement): {df_valid['Improvement'].mean():.3f}\")\n",
        "\n",
        "    # --- å¯è§†åŒ– 2: éšæœºå±•ç¤º 5 å¯¹åˆ†å­ ---\n",
        "    print(\"\\nğŸ–¼ï¸ åˆ†å­ç»“æ„å¯¹æ¯” (Input -> Optimized)\")\n",
        "    sample_df = df_valid.sample(min(5, len(df_valid)))\n",
        "\n",
        "    mols = []\n",
        "    legends = []\n",
        "    for _, row in sample_df.iterrows():\n",
        "        mols.append(Chem.MolFromSmiles(row['Source_SMILES']))\n",
        "        mols.append(Chem.MolFromSmiles(row['Optimized_SMILES']))\n",
        "        legends.append(f\"Src: {row['Source_LogSw']:.2f}\")\n",
        "        legends.append(f\"Opt: {row['Optimized_LogSw']:.2f}\")\n",
        "\n",
        "    img = Draw.MolsToGridImage(mols, molsPerRow=4, subImgSize=(250, 250), legends=legends)\n",
        "    display(img)\n",
        "\n",
        "# --- æ‰§è¡Œ ---\n",
        "if 'test_pairs' in globals():\n",
        "    evaluate_optimization(test_pairs, tokenizer, num_samples=200)"
      ],
      "metadata": {
        "id": "7gJtJcgIooTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # å¼•å…¥ F ç”¨äº softmax\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# ==========================================\n",
        "# 0. é…ç½®ä¸æ¨¡å‹ç±»å®šä¹‰ (å¿…é¡»é‡æ–°å®šä¹‰ä»¥åŠ è½½ Checkpoint)\n",
        "# ==========================================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SWEETNESS_MODEL_PATH = '/content/transformer_sweetness.pt'\n",
        "OPT_MODEL_PATH = 'mol_optimization.pt'\n",
        "\n",
        "# --- é‡æ–°å®šä¹‰ TransformerRegressor (ç”¨äºç”œåº¦é¢„æµ‹) ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerRegressor(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True, dropout=0.1)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.regressor = nn.Sequential(nn.Linear(d_model, 64), nn.ReLU(), nn.Dropout(0.1), nn.Linear(64, 1))\n",
        "    def forward(self, x):\n",
        "        src_key_padding_mask = (x == self.pad_idx)\n",
        "        x = self.embedding(x) * math.sqrt(x.size(-1))\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        mask_expanded = (~src_key_padding_mask).unsqueeze(-1).float()\n",
        "        sum_embeddings = torch.sum(x * mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "        feature = sum_embeddings / sum_mask\n",
        "        return self.regressor(feature).squeeze(-1)\n",
        "\n",
        "# --- é‡æ–°å®šä¹‰ MolSeq2Seq (ç”¨äºåˆ†å­ä¼˜åŒ–) ---\n",
        "class MolSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_encoder = nn.Embedding(1024, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dim_feedforward=256, batch_first=True, dropout=0.1)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, src, tgt):\n",
        "        src_pad_mask = (src == self.pad_idx)\n",
        "        tgt_pad_mask = (tgt == self.pad_idx)\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
        "        src_seq = torch.arange(src.size(1), device=src.device).unsqueeze(0).clamp(max=1023)\n",
        "        tgt_seq = torch.arange(tgt.size(1), device=tgt.device).unsqueeze(0).clamp(max=1023)\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model) + self.pos_encoder(src_seq)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) + self.pos_encoder(tgt_seq)\n",
        "        out = self.transformer(src=src_emb, tgt=tgt_emb, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask, memory_key_padding_mask=src_pad_mask, tgt_mask=tgt_mask)\n",
        "        return self.fc_out(out)\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°\n",
        "# ==========================================\n",
        "def interactive_optimization(tokenizer):\n",
        "    print(\"\\nğŸ› ï¸ åˆå§‹åŒ–æ¨¡å‹ä¸­...\")\n",
        "    pad_id = tokenizer.vocab[tokenizer.pad_token]\n",
        "    sos_id = tokenizer.vocab[tokenizer.sos_token]\n",
        "    eos_id = tokenizer.vocab[tokenizer.eos_token]\n",
        "\n",
        "    # 1. åŠ è½½ä¼˜åŒ–æ¨¡å‹\n",
        "    opt_model = MolSeq2Seq(len(tokenizer.vocab), pad_idx=pad_id).to(DEVICE)\n",
        "    try:\n",
        "        opt_model.load_state_dict(torch.load(OPT_MODEL_PATH, map_location=DEVICE))\n",
        "        opt_model.eval()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°ä¼˜åŒ–æ¨¡å‹æ–‡ä»¶ {OPT_MODEL_PATH}\")\n",
        "        return\n",
        "\n",
        "    # 2. åŠ è½½ç”œåº¦é¢„æµ‹æ¨¡å‹\n",
        "    sweet_model = TransformerRegressor(vocab_size=46, pad_idx=pad_id).to(DEVICE)\n",
        "    try:\n",
        "        sweet_model.load_state_dict(torch.load(SWEETNESS_MODEL_PATH, map_location=DEVICE))\n",
        "        sweet_model.eval()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°ç”œåº¦é¢„æµ‹æ¨¡å‹æ–‡ä»¶ {SWEETNESS_MODEL_PATH}\")\n",
        "        return\n",
        "\n",
        "    # 3. è¾…åŠ©å‡½æ•°ï¼šé¢„æµ‹ç”œåº¦\n",
        "    def get_pred_score(smiles):\n",
        "        if not smiles: return 0.0\n",
        "        ids = tokenizer.encode(smiles)\n",
        "        inp = torch.tensor([ids], dtype=torch.long).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            score = sweet_model(inp)\n",
        "        return score.item()\n",
        "\n",
        "    # --- äº¤äº’å¾ªç¯ ---\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        input_smiles = input(\"ğŸ§ª è¯·è¾“å…¥è¦ä¼˜åŒ–çš„åˆ†å­ SMILES (è¾“å…¥ 'q' é€€å‡º): \").strip()\n",
        "\n",
        "        if input_smiles.lower() == 'q':\n",
        "            print(\"ğŸ‘‹ é€€å‡ºç¨‹åºã€‚\")\n",
        "            break\n",
        "\n",
        "        # æ£€æŸ¥è¾“å…¥åˆæ³•æ€§\n",
        "        mol_src = Chem.MolFromSmiles(input_smiles)\n",
        "        if not mol_src:\n",
        "            print(\"âŒ æ— æ•ˆçš„ SMILESï¼Œè¯·é‡æ–°è¾“å…¥ã€‚\")\n",
        "            continue\n",
        "\n",
        "        print(f\"ğŸ”„ æ­£åœ¨å°è¯•ä¼˜åŒ–: {input_smiles}\")\n",
        "\n",
        "        # å‡†å¤‡è¾“å…¥\n",
        "        src_ids = tokenizer.encode(input_smiles, add_special_tokens=False) + [eos_id]\n",
        "        src_tensor = torch.tensor([src_ids], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        valid_opt_smiles = None\n",
        "        attempts = 0\n",
        "        max_attempts = 20 # æœ€å¤šå°è¯•20æ¬¡\n",
        "\n",
        "        # --- å¾ªç¯ç”Ÿæˆç›´åˆ°åˆæ³• ---\n",
        "        while attempts < max_attempts:\n",
        "            attempts += 1\n",
        "            curr_tgt = torch.tensor([[sos_id]], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(100): # max len\n",
        "                    logits = opt_model(src_tensor, curr_tgt)\n",
        "\n",
        "                    # ä½¿ç”¨ Temperature Sampling å¢åŠ å¤šæ ·æ€§ï¼Œé¿å…æ¯æ¬¡ç”Ÿæˆä¸€æ ·çš„æ­»å¾ªç¯\n",
        "                    logits = logits[:, -1, :] / 1.0\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    next_token = torch.multinomial(probs, 1) # éšæœºé‡‡æ ·\n",
        "\n",
        "                    if next_token.item() == eos_id:\n",
        "                        break\n",
        "                    curr_tgt = torch.cat([curr_tgt, next_token], dim=1)\n",
        "\n",
        "            pred_ids = curr_tgt[0].cpu().numpy()[1:]\n",
        "            candidate_smiles = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "            # æ£€æŸ¥æ˜¯å¦åˆæ³•ä¸”ä¸æ˜¯åŸåˆ†å­\n",
        "            if Chem.MolFromSmiles(candidate_smiles) and candidate_smiles != input_smiles:\n",
        "                valid_opt_smiles = candidate_smiles\n",
        "                break # æˆåŠŸæ‰¾åˆ°ï¼\n",
        "\n",
        "        # --- ç»“æœå¤„ç† ---\n",
        "        if valid_opt_smiles:\n",
        "            print(f\"âœ… ä¼˜åŒ–æˆåŠŸ (å°è¯•æ¬¡æ•°: {attempts})\")\n",
        "            print(f\"   Input:     {input_smiles}\")\n",
        "            print(f\"   Optimized: {valid_opt_smiles}\")\n",
        "\n",
        "            # é¢„æµ‹ç”œåº¦\n",
        "            score_src = get_pred_score(input_smiles)\n",
        "            score_opt = get_pred_score(valid_opt_smiles)\n",
        "            diff = score_opt - score_src\n",
        "\n",
        "            print(f\"   ç”œåº¦å˜åŒ–: {score_src:.2f} -> {score_opt:.2f} (æå‡: {diff:+.2f})\")\n",
        "\n",
        "            # å¯è§†åŒ–\n",
        "            mols = [Chem.MolFromSmiles(input_smiles), Chem.MolFromSmiles(valid_opt_smiles)]\n",
        "            legends = [f\"Original\\nLogSw: {score_src:.2f}\", f\"Optimized\\nLogSw: {score_opt:.2f}\"]\n",
        "            img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(300, 300), legends=legends)\n",
        "            display(img)\n",
        "\n",
        "        else:\n",
        "            print(f\"âš ï¸ ä¼˜åŒ–å¤±è´¥ï¼šå°è¯•äº† {max_attempts} æ¬¡ä»æœªç”Ÿæˆæœ‰æ•ˆçš„ä¼˜åŒ–åˆ†å­ã€‚\")\n",
        "            print(\"   (æ¨¡å‹å¯èƒ½è®¤ä¸ºåŸåˆ†å­å·²ç»æ˜¯å±€éƒ¨æœ€ä¼˜ï¼Œæˆ–è€…è®­ç»ƒæ•°æ®ä¸è¶³å¯¼è‡´æ— æ³•æ³›åŒ–)\")\n",
        "\n",
        "# --- å¯åŠ¨äº¤äº’ ---\n",
        "if __name__ == \"__main__\":\n",
        "    if 'tokenizer' in globals():\n",
        "        interactive_optimization(tokenizer)\n",
        "    else:\n",
        "        print(\"âŒ è¯·å…ˆåŠ è½½ Tokenizer\")"
      ],
      "metadata": {
        "id": "MvD6V9d8Io82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Encoder-Decoder (Seq2Seq):é€†åˆæˆ (Retrosynthesis)é¢„æµ‹"
      ],
      "metadata": {
        "id": "ItGLk25gjYc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¬¬ä¸€éƒ¨åˆ†:æ•°æ®é¢„å¤„ç†(Data Preprocessing)"
      ],
      "metadata": {
        "id": "zB8UOkgbkQ-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# 0. é…ç½®ä¸æ¸…æ´—å·¥å…·\n",
        "# ==========================================\n",
        "BATCH_SIZE = 32\n",
        "CSV_PATH = 'USPTO_1000.csv' # è¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•\n",
        "\n",
        "def remove_atom_mapping(smiles):\n",
        "    \"\"\"ç§»é™¤åŸå­æ˜ å°„ç¼–å· (ä¾‹å¦‚ [CH2:12] -> [CH2])\"\"\"\n",
        "    return re.sub(r':\\d+]', ']', smiles)\n",
        "\n",
        "def preprocess_uspto(csv_path):\n",
        "    print(\"ğŸ§¹ æ­£åœ¨æ¸…æ´— USPTO æ•°æ®...\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° {csv_path}ã€‚è¯·ç¡®ä¿æ•°æ®é›†å·²ä¸Šä¼ ã€‚\")\n",
        "        return [], []\n",
        "\n",
        "    products = []\n",
        "    reactants = []\n",
        "\n",
        "    # è§£æ USPTO æ ¼å¼: reactants>reagents>production\n",
        "    col_name = 'reactants>reagents>production'\n",
        "    if col_name not in df.columns:\n",
        "        # å°è¯•å¯»æ‰¾å¯èƒ½çš„åˆ—å\n",
        "        col_name = df.columns[0]\n",
        "\n",
        "    for rxn in df[col_name]:\n",
        "        parts = rxn.split('>')\n",
        "        if len(parts) >= 3:\n",
        "            r = parts[0] # ååº”ç‰©\n",
        "            p = parts[-1] # äº§ç‰©\n",
        "            # æ¸…æ´—\n",
        "            p = remove_atom_mapping(p)\n",
        "            r = remove_atom_mapping(r)\n",
        "            products.append(p)\n",
        "            reactants.append(r)\n",
        "\n",
        "    return products, reactants\n",
        "\n",
        "# ==========================================\n",
        "# 1. Tokenizer (æ”¯æŒä¿å­˜/åŠ è½½)\n",
        "# ==========================================\n",
        "class RetrosynthesisTokenizer:\n",
        "    def __init__(self, vocab=None):\n",
        "        # æ‰©å±•æ­£åˆ™\n",
        "        self.pattern =  r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "        self.regex = re.compile(self.pattern)\n",
        "\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "\n",
        "        self.special_tokens = [self.pad_token, self.unk_token, self.sos_token, self.eos_token]\n",
        "\n",
        "        if vocab:\n",
        "            self.vocab = vocab\n",
        "        else:\n",
        "            self.vocab = {t: i for i, t in enumerate(self.special_tokens)}\n",
        "\n",
        "        self.inv_vocab = {i: t for t, i in self.vocab.items()}\n",
        "\n",
        "    def tokenize(self, smiles):\n",
        "        return self.regex.findall(smiles)\n",
        "\n",
        "    def build_vocab(self, smiles_list):\n",
        "        print(\"ğŸ—ï¸ æ­£åœ¨æ„å»ºé€†åˆæˆè¯è¡¨...\")\n",
        "        unique_tokens = set()\n",
        "        for s in tqdm(smiles_list, desc=\"Scanning Tokens\"):\n",
        "            unique_tokens.update(self.tokenize(s))\n",
        "\n",
        "        for token in sorted(list(unique_tokens)):\n",
        "            if token not in self.vocab:\n",
        "                idx = len(self.vocab)\n",
        "                self.vocab[token] = idx\n",
        "                self.inv_vocab[idx] = token\n",
        "        print(f\"âœ… è¯è¡¨æ„å»ºå®Œæˆ | Size: {len(self.vocab)}\")\n",
        "\n",
        "    def encode(self, smiles):\n",
        "        tokens = self.tokenize(smiles)\n",
        "        return [self.vocab.get(t, self.vocab[self.unk_token]) for t in tokens]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        tokens = []\n",
        "        for i in ids:\n",
        "            if i == self.vocab[self.eos_token]: break\n",
        "            if i in [self.vocab[self.sos_token], self.vocab[self.pad_token]]: continue\n",
        "            tokens.append(self.inv_vocab.get(i, \"\"))\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def save_vocab(self, path):\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.vocab, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load_vocab(cls, path):\n",
        "        with open(path, 'r') as f:\n",
        "            vocab = json.load(f)\n",
        "        return cls(vocab=vocab)\n",
        "\n",
        "# ==========================================\n",
        "# 2. Dataset\n",
        "# ==========================================\n",
        "class RetroDataset(Dataset):\n",
        "    def __init__(self, products, reactants, tokenizer):\n",
        "        self.data = list(zip(products, reactants))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sos_id = tokenizer.vocab[tokenizer.sos_token]\n",
        "        self.eos_id = tokenizer.vocab[tokenizer.eos_token]\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prod, react = self.data[idx]\n",
        "\n",
        "        # ç¼–ç  (æ³¨æ„ï¼šä¸åœ¨æ­¤å¤„ Padding)\n",
        "        src_ids = self.tokenizer.encode(prod)\n",
        "        tgt_ids = self.tokenizer.encode(react)\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_ids + [self.eos_id], dtype=torch.long),\n",
        "            'tgt_in': torch.tensor([self.sos_id] + tgt_ids, dtype=torch.long),\n",
        "            'tgt_out': torch.tensor(tgt_ids + [self.eos_id], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pad_id = tokenizer.vocab[\"<pad>\"]\n",
        "    srcs = pad_sequence([x['src'] for x in batch], batch_first=True, padding_value=pad_id)\n",
        "    tgt_ins = pad_sequence([x['tgt_in'] for x in batch], batch_first=True, padding_value=pad_id)\n",
        "    tgt_outs = pad_sequence([x['tgt_out'] for x in batch], batch_first=True, padding_value=pad_id)\n",
        "    return srcs, tgt_ins, tgt_outs\n",
        "\n",
        "# --- æ‰§è¡Œé¢„å¤„ç† ---\n",
        "if __name__ == \"__main__\":\n",
        "    products, reactants = preprocess_uspto(CSV_PATH)\n",
        "\n",
        "    if products:\n",
        "        tokenizer = RetrosynthesisTokenizer()\n",
        "        tokenizer.build_vocab(products + reactants)\n",
        "        tokenizer.save_vocab(\"retro_vocab.json\")\n",
        "        print(\"ğŸ’¾ è¯è¡¨å·²ä¿å­˜è‡³ retro_vocab.json\")\n",
        "\n",
        "        # åˆ’åˆ†æ•°æ®é›†\n",
        "        train_p, test_p, train_r, test_r = train_test_split(products, reactants, test_size=0.1, random_state=42)\n",
        "\n",
        "        train_dataset = RetroDataset(train_p, train_r, tokenizer)\n",
        "        test_dataset = RetroDataset(test_p, test_r, tokenizer)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "        # æµ‹è¯•é›†ç”¨äºåç»­ç‹¬ç«‹è¯„ä¼°ï¼Œè¿™é‡Œå¯ä»¥å…ˆä¸åˆ›å»º loader\n",
        "\n",
        "        # ä¿å­˜æµ‹è¯•æ•°æ®ä¾›åç»­è¯„ä¼°ä½¿ç”¨\n",
        "        pd.DataFrame({'Product': test_p, 'Reactant': test_r}).to_csv('retro_test_set.csv', index=False)\n",
        "        print(f\"ğŸ“Š æ•°æ®å‡†å¤‡å°±ç»ª: Train={len(train_dataset)}, Test={len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "uVty6IAVkI_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒä¸ä¿å­˜ (Model Training)"
      ],
      "metadata": {
        "id": "DWiaQ_9ykUC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# 0. æ£€æŸ¥ä¾èµ–\n",
        "# ==========================================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if 'tokenizer' not in globals():\n",
        "    # å°è¯•åŠ è½½\n",
        "    try:\n",
        "        with open('retro_vocab.json', 'r') as f:\n",
        "            vocab = json.load(f)\n",
        "        tokenizer = RetrosynthesisTokenizer(vocab=vocab)\n",
        "    except:\n",
        "        raise ValueError(\"è¯·å…ˆè¿è¡Œç¬¬ä¸€éƒ¨åˆ†ä»£ç ç”Ÿæˆè¯è¡¨ï¼\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ¨¡å‹å®šä¹‰\n",
        "# ==========================================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class MolSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=512,\n",
        "            batch_first=True,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        src_padding_mask = (src == self.pad_idx)\n",
        "        tgt_padding_mask = (tgt == self.pad_idx)\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_len).to(src.device)\n",
        "        return src_padding_mask, tgt_padding_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_pad_mask, tgt_pad_mask, tgt_mask = self.create_masks(src, tgt)\n",
        "        src_emb = self.pos_encoder(self.embedding(src) * math.sqrt(self.d_model))\n",
        "        tgt_emb = self.pos_encoder(self.embedding(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        output = self.transformer(\n",
        "            src=src_emb, tgt=tgt_emb,\n",
        "            src_key_padding_mask=src_pad_mask,\n",
        "            tgt_key_padding_mask=tgt_pad_mask,\n",
        "            memory_key_padding_mask=src_pad_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "        return self.fc_out(output)\n",
        "\n",
        "# ==========================================\n",
        "# 2. è®­ç»ƒæµç¨‹\n",
        "# ==========================================\n",
        "def train_retrosynthesis():\n",
        "    print(\"\\nğŸ§ª å¯åŠ¨é€†åˆæˆæ¨¡å‹è®­ç»ƒ (Seq2Seq)...\")\n",
        "\n",
        "    pad_idx = tokenizer.vocab[\"<pad>\"]\n",
        "    # å¢åŠ æ¨¡å‹å®¹é‡ä»¥åº”å¯¹é€†åˆæˆçš„å¤æ‚æ€§\n",
        "    model = MolSeq2Seq(len(tokenizer.vocab), pad_idx=pad_idx, d_model=32, nhead=2, num_layers=2).to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "\n",
        "    losses = []\n",
        "    epochs = 10\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "        for src, tgt_in, tgt_out in pbar:\n",
        "            src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_in)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "        scheduler.step(avg_loss)\n",
        "        print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # ä¿å­˜\n",
        "    torch.save(model.state_dict(), 'retro_seq2seq.pt')\n",
        "    print(\"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º retro_seq2seq.pt\")\n",
        "\n",
        "    # ç»˜å›¾\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(losses, label='Train Loss', color='purple')\n",
        "    plt.title('Retrosynthesis Training Curve')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if 'train_loader' in globals():\n",
        "    train_retrosynthesis()\n",
        "else:\n",
        "    print(\"âŒ è¯·å…ˆè¿è¡Œç¬¬ä¸€éƒ¨åˆ†å‡†å¤‡æ•°æ®ï¼\")"
      ],
      "metadata": {
        "id": "MVxPmIqWkLYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨ç†ã€è¯„ä¼°ä¸å¯è§†åŒ– (Evaluation)"
      ],
      "metadata": {
        "id": "ouordZzKkWSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 0. é…ç½®\n",
        "# ==========================================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_PATH = 'retro_seq2seq.pt'\n",
        "TEST_CSV = 'retro_test_set.csv'\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ ¸å¿ƒæ¨ç†å‡½æ•°\n",
        "# ==========================================\n",
        "def predict_reactants(model, product_smiles, tokenizer, max_len=120):\n",
        "    model.eval()\n",
        "\n",
        "    # ç¼–ç  Product\n",
        "    src_ids = tokenizer.encode(product_smiles)\n",
        "    # [SOS] not needed for encoder input in this architecture, just [Product]\n",
        "    # But encoder doesn't use SOS/EOS usually, decoder does.\n",
        "    # Let's keep it consistent with training: src was just ids + EOS\n",
        "    eos_id = tokenizer.vocab[tokenizer.eos_token]\n",
        "    src = torch.tensor([src_ids + [eos_id]], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    # Decoder è¾“å…¥åˆå§‹ä¸º [SOS]\n",
        "    sos_id = tokenizer.vocab[tokenizer.sos_token]\n",
        "    tgt = torch.tensor([[sos_id]], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    generated_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            output = model(src, tgt)\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            next_token_id = next_token_logits.argmax(dim=-1).item()\n",
        "\n",
        "            if next_token_id == eos_id:\n",
        "                break\n",
        "\n",
        "            generated_ids.append(next_token_id)\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_token_id]], device=DEVICE)], dim=1)\n",
        "\n",
        "    return tokenizer.decode(generated_ids)\n",
        "\n",
        "# ==========================================\n",
        "# 2. æ‰¹é‡è¯„ä¼°\n",
        "# ==========================================\n",
        "def run_evaluation():\n",
        "    print(\"\\nğŸ” æ­£åœ¨è¯„ä¼°æµ‹è¯•é›† (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...\")\n",
        "\n",
        "    # 1. åŠ è½½ Tokenizer\n",
        "    try:\n",
        "        tokenizer = RetrosynthesisTokenizer.load_vocab(\"retro_vocab.json\")\n",
        "    except:\n",
        "        print(\"âŒ æ— æ³•åŠ è½½è¯è¡¨ï¼Œè¯·æ£€æŸ¥ retro_vocab.json\")\n",
        "        return\n",
        "\n",
        "    # 2. åŠ è½½æ¨¡å‹\n",
        "    pad_id = tokenizer.vocab[\"<pad>\"]\n",
        "    model = MolSeq2Seq(len(tokenizer.vocab), pad_idx=pad_id, d_model=32, nhead=2, num_layers=2).to(DEVICE)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_PATH}\")\n",
        "        return\n",
        "\n",
        "    # 3. åŠ è½½æµ‹è¯•æ•°æ®\n",
        "    try:\n",
        "        df_test = pd.read_csv(TEST_CSV)\n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ æ‰¾ä¸åˆ°æµ‹è¯•é›† CSVï¼Œè¯·å…ˆè¿è¡Œç¬¬ä¸€éƒ¨åˆ†ã€‚\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "    correct_count = 0\n",
        "\n",
        "    # éšæœºå– 100 ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿè¯„ä¼° (å…¨é‡è¯„ä¼°å¯èƒ½å¤ªæ…¢)\n",
        "    sample_size = min(100, len(df_test))\n",
        "    df_sample = df_test.sample(sample_size, random_state=42)\n",
        "\n",
        "    for _, row in df_sample.iterrows():\n",
        "        prod = row['Product']\n",
        "        true_react = row['Reactant']\n",
        "\n",
        "        pred_react = predict_reactants(model, prod, tokenizer)\n",
        "\n",
        "        # ä½¿ç”¨ RDKit Canonicalize è¿›è¡Œå¯¹æ¯” (å¿½ç•¥å†™æ³•å·®å¼‚)\n",
        "        try:\n",
        "            can_true = Chem.MolToSmiles(Chem.MolFromSmiles(true_react), isomericSmiles=False)\n",
        "            can_pred = Chem.MolToSmiles(Chem.MolFromSmiles(pred_react), isomericSmiles=False)\n",
        "            is_match = (can_true == can_pred)\n",
        "        except:\n",
        "            is_match = False\n",
        "\n",
        "        if is_match: correct_count += 1\n",
        "\n",
        "        results.append({\n",
        "            'Product': prod,\n",
        "            'True_Reactants': true_react,\n",
        "            'Pred_Reactants': pred_react,\n",
        "            'Match': is_match\n",
        "        })\n",
        "\n",
        "    acc = correct_count / sample_size\n",
        "    print(f\"âœ… è¯„ä¼°å®Œæˆ (Sample N={sample_size})\")\n",
        "    print(f\"ğŸ¯ ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ (Exact Match Accuracy): {acc:.2%}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. å¯è§†åŒ–\n",
        "    # ==========================================\n",
        "    print(\"\\nğŸ–¼ï¸ é€†åˆæˆç»“æœå¯è§†åŒ– (Product -> Predicted Reactants)\")\n",
        "    # æŒ‘é€‰ 3 ä¸ªæˆåŠŸçš„å’Œ 3 ä¸ªå¤±è´¥çš„\n",
        "    df_res = pd.DataFrame(results)\n",
        "\n",
        "    # å°è¯•æ··åˆå±•ç¤º\n",
        "    to_show = pd.concat([\n",
        "        df_res[df_res['Match'] == True].head(3),\n",
        "        df_res[df_res['Match'] == False].head(3)\n",
        "    ])\n",
        "\n",
        "    mols = []\n",
        "    legends = []\n",
        "\n",
        "    for _, row in to_show.iterrows():\n",
        "        m_prod = Chem.MolFromSmiles(row['Product'])\n",
        "        m_pred = Chem.MolFromSmiles(row['Pred_Reactants'])\n",
        "\n",
        "        if m_prod and m_pred:\n",
        "            mols.extend([m_prod, m_pred])\n",
        "            status = \"âœ… Correct\" if row['Match'] else \"âŒ Wrong\"\n",
        "            legends.extend([\"Product\", f\"Pred Reactants\\n({status})\"])\n",
        "\n",
        "    if mols:\n",
        "        img = Draw.MolsToGridImage(mols, molsPerRow=4, subImgSize=(300, 300), legends=legends)\n",
        "        display(img)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_evaluation()"
      ],
      "metadata": {
        "id": "85-SrRSpkM-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¬¬å››éƒ¨åˆ†ï¼šå•åˆ†å­äº¤äº’æ¨ç† (User Input)"
      ],
      "metadata": {
        "id": "yqFUBfPEJnMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "# ==========================================\n",
        "# äº¤äº’å¼å·¥å…·\n",
        "# ==========================================\n",
        "def interactive_retrosynthesis():\n",
        "    print(\"\\nğŸ§ª äº¤äº’å¼é€†åˆæˆåˆ†æå·¥å…·\")\n",
        "    print(\"=========================\")\n",
        "\n",
        "    # 1. èµ„æºåŠ è½½\n",
        "    try:\n",
        "        tokenizer = RetrosynthesisTokenizer.load_vocab(\"retro_vocab.json\")\n",
        "        pad_id = tokenizer.vocab[\"<pad>\"]\n",
        "        model = MolSeq2Seq(len(tokenizer.vocab), pad_idx=pad_id, d_model=32, nhead=2, num_layers=2).to(DEVICE)\n",
        "        model.load_state_dict(torch.load('retro_seq2seq.pt', map_location=DEVICE))\n",
        "        model.eval()\n",
        "        print(\"âœ… æ¨¡å‹å·²åŠ è½½ Ready.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. æ¨ç†è¾…åŠ©å‡½æ•°\n",
        "    def infer(smiles):\n",
        "        # ç®€å•é¢„å¤„ç†\n",
        "        smiles = re.sub(r':\\d+]', ']', smiles)\n",
        "\n",
        "        src_ids = tokenizer.encode(smiles)\n",
        "        eos_id = tokenizer.vocab[\"<eos>\"]\n",
        "        src = torch.tensor([src_ids + [eos_id]], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        sos_id = tokenizer.vocab[\"<sos>\"]\n",
        "        tgt = torch.tensor([[sos_id]], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        res_ids = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(150):\n",
        "                out = model(src, tgt)\n",
        "                next_id = out[:, -1, :].argmax().item()\n",
        "                if next_id == eos_id: break\n",
        "                res_ids.append(next_id)\n",
        "                tgt = torch.cat([tgt, torch.tensor([[next_id]], device=DEVICE)], dim=1)\n",
        "\n",
        "        return tokenizer.decode(res_ids)\n",
        "\n",
        "    # 3. å¾ªç¯\n",
        "    while True:\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        user_input = input(\"è¯·è¾“å…¥ç›®æ ‡äº§ç‰© SMILES (è¾“å…¥ 'q' é€€å‡º): \").strip()\n",
        "\n",
        "        if user_input.lower() == 'q':\n",
        "            print(\"ğŸ‘‹ Bye!\")\n",
        "            break\n",
        "\n",
        "        if not Chem.MolFromSmiles(user_input):\n",
        "            print(\"âš ï¸ æ— æ•ˆçš„ SMILESï¼Œè¯·æ£€æŸ¥è¾“å…¥ã€‚\")\n",
        "            continue\n",
        "\n",
        "        print(f\"ğŸ”„ æ­£åœ¨åˆ†æé€†åˆæˆè·¯å¾„...\")\n",
        "        try:\n",
        "            pred_reactants = infer(user_input)\n",
        "            print(f\"âœ… é¢„æµ‹ååº”ç‰©: {pred_reactants}\")\n",
        "\n",
        "            # å¯è§†åŒ–\n",
        "            m_prod = Chem.MolFromSmiles(user_input)\n",
        "            m_react = Chem.MolFromSmiles(pred_reactants)\n",
        "\n",
        "            if m_react:\n",
        "                print(\"ğŸ–¼ï¸ ç”Ÿæˆååº”å›¾ç¤º:\")\n",
        "                img = Draw.MolsToGridImage(\n",
        "                    [m_prod, m_react],\n",
        "                    molsPerRow=2,\n",
        "                    subImgSize=(400, 300),\n",
        "                    legends=[\"Target Product\", \"Predicted Reactants\"]\n",
        "                )\n",
        "                display(img)\n",
        "            else:\n",
        "                print(\"âš ï¸ é¢„æµ‹ç»“æœä¸æ˜¯æœ‰æ•ˆçš„åˆ†å­ç»“æ„ã€‚\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}\")\n",
        "\n",
        "# å¯åŠ¨\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_retrosynthesis()"
      ],
      "metadata": {
        "id": "hNdm5odTJoj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}