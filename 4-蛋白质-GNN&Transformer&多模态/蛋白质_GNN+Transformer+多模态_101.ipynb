{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "X7AN78JLc32N",
        "6QIYUh0E2dZw",
        "7RahmMfxUWFX",
        "tX-pG0y2r3Kr",
        "Q9dLPtc5uMAp",
        "UkYu3HHquSqH",
        "ALeMWNOMfgTR",
        "xb992dcTfo5f"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **å®‰è£…ä¾èµ–åº“**"
      ],
      "metadata": {
        "id": "X7AN78JLc32N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QIYUh0E2dZw"
      },
      "source": [
        "## **ä½œè€…ç®€ä»‹**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X91JIVkqUNMV"
      },
      "source": [
        "#### ä½œè€…ï¼š**âš¡å°é—ªç”µâš¡**\n",
        "\n",
        "#### Bç«™ä¸»é¡µ\n",
        "- [å°é—ªç”µçš„Bç«™ä¸»é¡µ](https://space.bilibili.com/122699831?spm_id_from=333.1007.0.0)\n",
        "\n",
        "#### äº¤æµç¾¤\n",
        "æ¬¢è¿åŠ å…¥AIDDäº¤æµç¾¤ï¼  \n",
        "åŠ æˆ‘å¾®ä¿¡ï¼ˆå¾®ä¿¡å·: `xxxFLASHxxx`ï¼‰ï¼Œé‚€è¯·ä½ è¿›ç¾¤ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RahmMfxUWFX"
      },
      "source": [
        "## **å®‰è£…ä¾èµ–åº“**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cf3YlQPJkqE"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install rdkit\n",
        "!pip install networkx pandas\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.datasets import KarateClub\n",
        "dataset = KarateClub()\n",
        "print(f'Dataset: {dataset}:')\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¬¬1éƒ¨åˆ†ï¼šå…ˆè¯•è¯•GNN**"
      ],
      "metadata": {
        "id": "tX-pG0y2r3Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **æ•°æ®å¯è§†åŒ–**"
      ],
      "metadata": {
        "id": "Q9dLPtc5uMAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# from tqdm import tqdm # æ¼”ç¤ºæ•°æ®ä¸éœ€è¦\n",
        "from mpl_toolkits.mplot3d import Axes3D  # å¿…é¡»å¯¼å…¥ç”¨äº3Dç»˜å›¾\n",
        "\n",
        "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 120\n",
        "# ç¡®ä¿å›¾è¡¨ä¸­ä¸ä½¿ç”¨ä¸­æ–‡å­—ä½“ï¼Œé˜²æ­¢ä¹±ç  (è™½ç„¶æœ¬ä¾‹è¦æ±‚å…¨è‹±æ–‡ï¼Œä½†è¿™æ˜¯å¥½ä¹ æƒ¯)\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# ==========================================\n",
        "# 1. æ ¸å¿ƒå·¥å…·å‡½æ•° (ä¿æŒä¸å˜)\n",
        "# ==========================================\n",
        "\n",
        "def build_window_matrix(seq, window=1):\n",
        "    \"\"\"æ„å»ºæ»‘åŠ¨çª—å£çš„é‚»æ¥çŸ©é˜µ (Baseline)\"\"\"\n",
        "    L = len(seq)\n",
        "    matrix = np.zeros((L, L))\n",
        "    for i in range(L):\n",
        "        start = max(0, i - window)\n",
        "        end = min(L, i + window + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                matrix[i, j] = 1\n",
        "    return matrix\n",
        "\n",
        "# ESM æ¨¡å‹åŠ è½½\n",
        "ESM_MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
        "print(f\"â³ Loading ESM-2 Model ({ESM_MODEL_NAME})...\") # ä¸­æ–‡æ³¨é‡Šï¼šæ§åˆ¶å°è¾“å‡ºæ”¹ä¸ºè‹±æ–‡ä»¥é˜²ä¸‡ä¸€ï¼Œæ³¨é‡Šä¿ç•™ä¸­æ–‡\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ESM_MODEL_NAME)\n",
        "    esm_model = AutoModel.from_pretrained(ESM_MODEL_NAME, attn_implementation=\"eager\").eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    esm_model = esm_model.to(device)\n",
        "    print(\"âœ… Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Model load failed: {e}\")\n",
        "    exit()\n",
        "\n",
        "def predict_esm_matrix(seq, k_neighbors=10):\n",
        "    \"\"\"ESM + Top-K + Backbone æ··åˆå»ºå›¾\"\"\"\n",
        "    inputs = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1022).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = esm_model(**inputs, output_attentions=True)\n",
        "\n",
        "    # 1. è·å– Attention Map (å–æœ€åä¸€å±‚ï¼Œå¹³å‡æ‰€æœ‰å¤´)\n",
        "    attentions = outputs.attentions[-1].squeeze(0).mean(dim=0).cpu()\n",
        "    # å»æ‰ç‰¹æ®Š token <cls>, <eos>\n",
        "    raw_matrix = attentions[1:-1, 1:-1].numpy()\n",
        "    L = raw_matrix.shape[0]\n",
        "\n",
        "    # 2. å¯¹ç§°åŒ– & å»è‡ªç¯ (ç”¨äº Raw å±•ç¤ºå’Œåç»­è®¡ç®—)\n",
        "    raw_matrix_sym = (raw_matrix + raw_matrix.T) / 2\n",
        "    np.fill_diagonal(raw_matrix_sym, 0)\n",
        "\n",
        "    # 3. Top-K äºŒå€¼åŒ–\n",
        "    adj_matrix = np.zeros_like(raw_matrix_sym)\n",
        "    k = min(k_neighbors, L - 1)\n",
        "    # å¯¹æ¯ä¸€è¡Œæ‰¾æœ€å¤§çš„Kä¸ªç´¢å¼•\n",
        "    topk_indices = np.argsort(raw_matrix_sym, axis=1)[:, -k:]\n",
        "    for i in range(L):\n",
        "        adj_matrix[i, topk_indices[i]] = 1\n",
        "    # å†æ¬¡ç¡®ä¿äºŒå€¼å›¾å¯¹ç§°\n",
        "    adj_matrix = ((adj_matrix + adj_matrix.T) > 0).astype(float)\n",
        "\n",
        "    # 4. æ³¨å…¥ Backbone (ç¡®ä¿ä¸æ–­è£‚)\n",
        "    backbone = build_window_matrix(seq, window=1)\n",
        "    final_adj_matrix = ((adj_matrix + backbone) > 0).astype(float)\n",
        "\n",
        "    # ä¸­æ–‡æ³¨é‡Šï¼šå…³é”®ä¿®æ”¹ï¼ŒåŒæ—¶è¿”å›å¯¹ç§°åŒ–åçš„åŸå§‹Mapå’Œæœ€ç»ˆäºŒå€¼Map\n",
        "    return raw_matrix_sym, final_adj_matrix\n",
        "\n",
        "# ==========================================\n",
        "# 2. é«˜çº§å›¾ç»“æ„å¯è§†åŒ–å‡½æ•° (2D & 3D)\n",
        "# ==========================================\n",
        "\n",
        "def plot_graph_structure(adj_matrix, title, ax_2d, ax_3d, seq_len):\n",
        "    \"\"\"\n",
        "    å°†é‚»æ¥çŸ©é˜µè½¬æ¢ä¸º Graph å¹¶ç»˜åˆ¶ 2D å’Œ 3D ç»“æ„å›¾ (å…¨è‹±æ–‡æ ‡ç­¾)\n",
        "    \"\"\"\n",
        "    # 1. è½¬æ¢ä¸º NetworkX å›¾\n",
        "    rows, cols = np.where(adj_matrix > 0)\n",
        "    edges = zip(rows.tolist(), cols.tolist())\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(seq_len))\n",
        "    G.add_edges_from(edges)\n",
        "\n",
        "    # è®¾ç½®é¢œè‰²æ˜ å°„ (æ ¹æ®åºåˆ—ä½ç½® Nç«¯->Cç«¯ æ¸å˜)\n",
        "    # ä½¿ç”¨ spectral æˆ– viridis æ¨¡æ‹Ÿå½©è™¹è‰²ï¼Œå†·è‰²è°ƒä¸ºNç«¯ï¼Œæš–è‰²è°ƒä¸ºCç«¯\n",
        "    node_colors = list(range(seq_len))\n",
        "    cmap = plt.cm.Spectral\n",
        "\n",
        "    # --- 2D ç»˜å›¾ ---\n",
        "    # ä½¿ç”¨ Kamada-Kawai å¸ƒå±€ï¼Œå®ƒæ¯” spring_layout æ›´èƒ½ä½“ç°åŒ–å­¦åˆ†å­çš„å‡ ä½•ç»“æ„\n",
        "    try:\n",
        "        pos_2d = nx.kamada_kawai_layout(G)\n",
        "    except:\n",
        "        pos_2d = nx.spring_layout(G, k=0.15, iterations=20)\n",
        "\n",
        "    nx.draw_networkx_nodes(G, pos_2d, ax=ax_2d, node_size=50,\n",
        "                           node_color=node_colors, cmap=cmap, alpha=0.9)\n",
        "    nx.draw_networkx_edges(G, pos_2d, ax=ax_2d, edge_color='gray', alpha=0.4, width=0.8)\n",
        "    # ä¸­æ–‡æ³¨é‡Šï¼šç¡®ä¿æ ‡é¢˜æ˜¯è‹±æ–‡\n",
        "    ax_2d.set_title(f\"{title}\\n(2D Projection)\", fontsize=11, fontweight='bold')\n",
        "    ax_2d.axis('off')\n",
        "\n",
        "    # --- 3D ç»˜å›¾ ---\n",
        "    # ä½¿ç”¨ 3D Spring Layout\n",
        "    pos_3d = nx.spring_layout(G, dim=3, k=0.5, iterations=50, seed=42)\n",
        "\n",
        "    # æå–åæ ‡\n",
        "    xs = [pos_3d[i][0] for i in range(seq_len)]\n",
        "    ys = [pos_3d[i][1] for i in range(seq_len)]\n",
        "    zs = [pos_3d[i][2] for i in range(seq_len)]\n",
        "\n",
        "    # ç»˜åˆ¶èŠ‚ç‚¹\n",
        "    ax_3d.scatter(xs, ys, zs, c=node_colors, cmap=cmap, s=40, depthshade=True)\n",
        "\n",
        "    # ç»˜åˆ¶è¾¹ (éœ€è¦æ‰‹åŠ¨å¾ªç¯ç”»çº¿)\n",
        "    for u, v in G.edges():\n",
        "        ax_3d.plot([xs[u], xs[v]], [ys[u], ys[v]], [zs[u], zs[v]],\n",
        "                   color='gray', alpha=0.3, linewidth=0.5)\n",
        "\n",
        "    # ä¸­æ–‡æ³¨é‡Šï¼šç¡®ä¿æ ‡é¢˜æ˜¯è‹±æ–‡\n",
        "    ax_3d.set_title(f\"{title}\\n(3D Force-Directed)\", fontsize=11, fontweight='bold')\n",
        "    # éšè—åæ ‡è½´åˆ»åº¦ï¼Œåªä¿ç•™ç©ºé—´æ„Ÿ\n",
        "    ax_3d.set_xticklabels([])\n",
        "    ax_3d.set_yticklabels([])\n",
        "    ax_3d.set_zticklabels([])\n",
        "    ax_3d.grid(False)\n",
        "\n",
        "# ==========================================\n",
        "# 3. ä¸»ç¨‹åºä¸å¯è§†åŒ–æ‰§è¡Œ\n",
        "# ==========================================\n",
        "\n",
        "# å‡†å¤‡æ•°æ® (ä¼ªæ•°æ®)\n",
        "data = {'sequence': ['MLFSFFRNLCRVLYRVRVTGDTQALKGERVLITPNHVSFIDGILLGLFLPVRPVFAVYTSISQQWYMRWLKSFIDFVPLDPTQPMAIKHLVRLVEQGRPVVIFPEGRITTTGSLMKIYDGAGFVAAKSGATVIPVRIEGAELTHFSRLKGLVKRRLFPQITLHILPPTQVAMPDAPRARDRRKIAGEMLHQIMMEARMAVRPRETLYESLLSAMYRFGAGKKCVEDVNFTPDSYRKLLTKTLFVGRILEKYSVEGERIGLMLPNAGISAAVIFGAIARRRMPAMMNYTAGVKGLTSAITAAEIKTIFTSRQFLDKGKLWHLPEQLTQVRWVYLEDLKADVTTADKVWIFAHLLMPRLAQVKQQPEEEALILFTSGSEGHPKGVVHSHKSILANVEQIKTIADFTTNDRFMSALPLFHSFGLTVGLFTPLLTGAEVFLYPSPLHYR'*1]} # ç¨é•¿ä¸€ç‚¹ä»¥ä¾¿è§‚å¯ŸæŠ˜å \n",
        "train_df = pd.DataFrame(data)\n",
        "\n",
        "# å–ä¸€ä¸ªæ ·æœ¬\n",
        "sample_seq = train_df.iloc[0]['sequence']\n",
        "viz_len = min(len(sample_seq), 80) # å–å‰80ä¸ªæ®‹åŸºå¯è§†åŒ–ï¼Œå¤ªé•¿ä¼šå¾ˆä¹±\n",
        "viz_seq = sample_seq[:viz_len]\n",
        "\n",
        "print(f\"ğŸ” Analyzing sequence (length {viz_len})...\")\n",
        "\n",
        "# --- è®¡ç®— ---\n",
        "# 1. Baseline çŸ©é˜µ\n",
        "mat_window = build_window_matrix(viz_seq, window=2)\n",
        "\n",
        "# 2. ESM çŸ©é˜µ (ä¸­æ–‡æ³¨é‡Šï¼šå…³é”®ä¿®æ”¹ï¼Œæ¥æ”¶ä¸¤ä¸ªè¿”å›å€¼ï¼šåŸå§‹Map å’Œ æœ€ç»ˆäºŒå€¼Map)\n",
        "# k_neighbors=4 è¡¨ç¤ºæ¯ä¸ªæ®‹åŸºåªçœ‹æœ€å¼ºçš„4ä¸ªè¿œç¨‹å…³è”\n",
        "mat_raw_esm, mat_esm_final = predict_esm_matrix(viz_seq, k_neighbors=4)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4. ç»˜å›¾ï¼šåˆ†ä¸ºä¸¤ä¸ª Figure é¿å…æ‹¥æŒ¤\n",
        "# ==========================================\n",
        "\n",
        "# --- Figure 1: çŸ©é˜µçƒ­åŠ›å›¾å¯¹æ¯” (ä¸­æ–‡æ³¨é‡Šï¼šä¿®æ”¹ä¸º3ä¸ªå­å›¾ä»¥åŒ…å« Raw Map) ---\n",
        "fig1, axes1 = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Subplot 1: Baseline\n",
        "sns.heatmap(mat_window, cmap=\"Greys\", cbar=False, ax=axes1[0], square=True, vmin=0, vmax=1)\n",
        "axes1[0].set_title(\"1. Baseline: Window Graph\\n(Local Only)\", fontweight='bold')\n",
        "axes1[0].set_ylabel(\"Residue Index\")\n",
        "\n",
        "# Subplot 2: ESM Raw Attention (ä¸­æ–‡æ³¨é‡Šï¼šæ–°å¢çš„åŸå§‹çƒ­åŠ›å›¾)\n",
        "# ä½¿ç”¨ viridis é¢œè‰²å›¾æ¥å±•ç¤ºè¿ç»­çš„æ³¨æ„åŠ›å¼ºåº¦å€¼\n",
        "sns.heatmap(mat_raw_esm, cmap=\"viridis\", cbar=True, ax=axes1[1], square=True,\n",
        "            cbar_kws={'label': 'Attention Intensity', 'shrink': 0.8})\n",
        "axes1[1].set_title(\"2. ESM Raw Attention\\n(Dense & Noisy)\", fontweight='bold')\n",
        "axes1[1].set_yticks([]) # ç§»é™¤Yè½´æ ‡ç­¾ä¿æŒæ•´æ´\n",
        "\n",
        "# Subplot 3: ESM Final Binary Matrix\n",
        "sns.heatmap(mat_esm_final, cmap=\"Blues\", cbar=False, ax=axes1[2], square=True, vmin=0, vmax=1)\n",
        "axes1[2].set_title(\"3. ESM Final Graph\\n(Sparse: Top-K + Backbone)\", fontweight='bold')\n",
        "axes1[2].set_yticks([])\n",
        "\n",
        "# ä¸­æ–‡æ³¨é‡Šï¼šç¡®ä¿æ€»æ ‡é¢˜æ˜¯è‹±æ–‡\n",
        "plt.suptitle(f\"Contact Map Processing Flow (Seq Len: {viz_len})\", fontsize=14, y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Figure 2: å›¾ç»“æ„å¯è§†åŒ– (2D & 3D) ---\n",
        "# åˆ›å»º 2x2 çš„ç½‘æ ¼ï¼šä¸Šé¢æ˜¯ Window Graphï¼Œä¸‹é¢æ˜¯ ESM Graph\n",
        "fig2 = plt.figure(figsize=(16, 12))\n",
        "\n",
        "# å¸ƒå±€å®šä¹‰\n",
        "# Row 1: Window Graph (2D, 3D)\n",
        "ax_win_2d = fig2.add_subplot(2, 2, 1)\n",
        "ax_win_3d = fig2.add_subplot(2, 2, 2, projection='3d')\n",
        "\n",
        "# Row 2: ESM Graph (2D, 3D)\n",
        "ax_esm_2d = fig2.add_subplot(2, 2, 3)\n",
        "ax_esm_3d = fig2.add_subplot(2, 2, 4, projection='3d')\n",
        "\n",
        "print(\"ğŸ¨ Rendering 2D/3D graph structures...\")\n",
        "\n",
        "# ç»˜åˆ¶ Window Graph (ä½¿ç”¨è‹±æ–‡æ ‡é¢˜)\n",
        "plot_graph_structure(mat_window, \"Baseline (Window-2)\", ax_win_2d, ax_win_3d, viz_len)\n",
        "\n",
        "# ç»˜åˆ¶ ESM Graph (ä½¿ç”¨æœ€ç»ˆçš„äºŒå€¼çŸ©é˜µï¼Œä½¿ç”¨è‹±æ–‡æ ‡é¢˜)\n",
        "plot_graph_structure(mat_esm_final, \"ESM-2 (Predicted Fold)\", ax_esm_2d, ax_esm_3d, viz_len)\n",
        "\n",
        "# æ·»åŠ é¢œè‰²æ¡è¯´æ˜ (ä¸­æ–‡æ³¨é‡Šï¼šç¡®ä¿æ ‡ç­¾æ˜¯è‹±æ–‡)\n",
        "sm = plt.cm.ScalarMappable(cmap=plt.cm.Spectral, norm=plt.Normalize(vmin=0, vmax=viz_len))\n",
        "sm.set_array([])\n",
        "cbar_ax = fig2.add_axes([0.92, 0.15, 0.02, 0.7]) #å³ä¾§é¢œè‰²æ¡\n",
        "cbar = fig2.colorbar(sm, cax=cbar_ax)\n",
        "cbar.set_label('Residue Position (N-term -> C-term)')\n",
        "\n",
        "# ä¸­æ–‡æ³¨é‡Šï¼šç¡®ä¿æ€»æ ‡é¢˜æ˜¯è‹±æ–‡\n",
        "plt.suptitle(f\"Graph Structure Visualization (Sequence Length: {viz_len})\", fontsize=16, y=0.95)\n",
        "plt.subplots_adjust(left=0.05, right=0.9, wspace=0.1, hspace=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "impm41yKkmI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **å¯¹æ¯”è®­ç»ƒ**"
      ],
      "metadata": {
        "id": "UkYu3HHquSqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.metrics import r2_score, mean_squared_error # ğŸ”¥ æ–°å¢ï¼šç”¨äºè®¡ç®—R2å’ŒMSE\n",
        "\n",
        "# ==========================================\n",
        "# 1. ç¯å¢ƒé…ç½®ä¸æ¨¡å‹åŠ è½½\n",
        "# ==========================================\n",
        "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 120\n",
        "plt.rcParams['font.sans-serif'] = ['Arial'] # é˜²æ­¢ä¸­æ–‡ä¹±ç (å¦‚æœæœ‰)\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# æ£€æŸ¥ GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# åŠ è½½ ESM-2 æ¨¡å‹\n",
        "ESM_MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
        "print(f\"â³ Loading ESM Model: {ESM_MODEL_NAME}...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ESM_MODEL_NAME)\n",
        "    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ  attn_implementation=\"eager\" ä»¥ç¡®ä¿èƒ½è¿”å› attention weights\n",
        "    esm_model = AutoModel.from_pretrained(\n",
        "        ESM_MODEL_NAME,\n",
        "        attn_implementation=\"eager\"\n",
        "    ).eval().to(device)\n",
        "    print(\"âœ… ESM Model loaded successfully (with Eager Attention)!\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ESM Model load failed: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==========================================\n",
        "# 2. å›¾æ„å»ºä¸ç‰¹å¾æå–å‡½æ•°\n",
        "# ==========================================\n",
        "\n",
        "def build_window_matrix(seq, window=2):\n",
        "    \"\"\"Baseline: æ»‘åŠ¨çª—å£é‚»æ¥çŸ©é˜µ\"\"\"\n",
        "    L = len(seq)\n",
        "    matrix = np.eye(L) # åˆå§‹åŒ–å¸¦è‡ªç¯\n",
        "    for i in range(L):\n",
        "        start = max(0, i - window)\n",
        "        end = min(L, i + window + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                matrix[i, j] = 1\n",
        "    return matrix\n",
        "\n",
        "def predict_esm_matrix(seq, k_neighbors=5):\n",
        "    \"\"\"Advanced: ESM Attention æ¥è§¦å›¾\"\"\"\n",
        "    # æˆªæ–­ä»¥é˜² OOM\n",
        "    seq = seq[:1020]\n",
        "    inputs = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1022).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = esm_model(**inputs, output_attentions=True)\n",
        "\n",
        "    # è·å–æœ€åä¸€å±‚ Attentionï¼Œå¹³å‡æ‰€æœ‰å¤´\n",
        "    attentions = outputs.attentions[-1].squeeze(0).mean(dim=0).cpu()\n",
        "    raw_matrix = attentions[1:-1, 1:-1].numpy()\n",
        "    L = raw_matrix.shape[0]\n",
        "\n",
        "    # å¯¹ç§°åŒ–\n",
        "    raw_matrix = (raw_matrix + raw_matrix.T) / 2\n",
        "    np.fill_diagonal(raw_matrix, 0)\n",
        "\n",
        "    # Top-K ç¨€ç–åŒ–\n",
        "    adj_matrix = np.zeros_like(raw_matrix)\n",
        "    k = min(k_neighbors, L - 1)\n",
        "    if k > 0:\n",
        "        topk_indices = np.argsort(raw_matrix, axis=1)[:, -k:]\n",
        "        for i in range(L):\n",
        "            adj_matrix[i, topk_indices[i]] = 1\n",
        "\n",
        "    # ä¿è¯å¯¹ç§° + èåˆ Backbone + è‡ªç¯\n",
        "    adj_matrix = ((adj_matrix + adj_matrix.T) > 0).astype(float)\n",
        "    backbone = build_window_matrix(seq, window=1)\n",
        "    final_adj = ((adj_matrix + backbone) > 0).astype(float)\n",
        "    np.fill_diagonal(final_adj, 1.0)\n",
        "\n",
        "    return final_adj\n",
        "\n",
        "AA_VOCAB = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "aa_to_int = {aa: i for i, aa in enumerate(AA_VOCAB)}\n",
        "\n",
        "def sequence_to_node_features(seq):\n",
        "    \"\"\"One-Hot ç¼–ç \"\"\"\n",
        "    seq = seq[:1020]\n",
        "    L = len(seq)\n",
        "    features = torch.zeros(L, 20)\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in aa_to_int:\n",
        "            features[i, aa_to_int[aa]] = 1.0\n",
        "    return features\n",
        "\n",
        "# ==========================================\n",
        "# 3. æ•°æ®é›†æ„å»º (è®­ç»ƒé›† + æµ‹è¯•é›†)\n",
        "# ==========================================\n",
        "\n",
        "class ProteinGraphDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, graph_type='window', split_name='train'):\n",
        "        self.data_list = []\n",
        "        print(f\"ğŸ“¦ Building {split_name} Dataset ({graph_type})...\")\n",
        "\n",
        "        # éå†æ•°æ®\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            seq = row['sequence']\n",
        "            try:\n",
        "                label = float(row['solubility'])\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # A. ç‰¹å¾\n",
        "            x = sequence_to_node_features(seq)\n",
        "\n",
        "            # B. é‚»æ¥çŸ©é˜µ\n",
        "            if graph_type == 'window':\n",
        "                adj = build_window_matrix(seq, window=2)\n",
        "            else:\n",
        "                adj = predict_esm_matrix(seq, k_neighbors=5)\n",
        "\n",
        "            # C. è½¬ Edge Index\n",
        "            src, dst = np.nonzero(adj)\n",
        "            edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
        "            y = torch.tensor([[label]], dtype=torch.float)\n",
        "\n",
        "            self.data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data_list[idx]\n",
        "\n",
        "# è¯»å– CSV\n",
        "if os.path.exists('eSol_train.csv') and os.path.exists('eSol_test.csv'):\n",
        "    train_df = pd.read_csv('eSol_train.csv')\n",
        "    test_df = pd.read_csv('eSol_test.csv')\n",
        "else:\n",
        "    print(\"âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶ï¼Œç”Ÿæˆä¼ªæ•°æ®ç”¨äºæ¼”ç¤º...\")\n",
        "    fake_data = {'sequence': ['MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG']*20, 'solubility': np.random.rand(20)}\n",
        "    train_df = pd.DataFrame(fake_data)\n",
        "    test_df = pd.DataFrame(fake_data)\n",
        "\n",
        "# é™åˆ¶æ•°æ®é‡ä»¥ä¾¿æ¼”ç¤º (å®é™…è¿è¡Œæ—¶è¯·æ³¨é‡Šæ‰è¿™ä¸¤è¡Œ)\n",
        "# train_df = train_df.iloc[:50]\n",
        "# test_df = test_df.iloc[:20]\n",
        "\n",
        "print(f\"æ•°æ®é‡: Train={len(train_df)}, Test={len(test_df)}\")\n",
        "\n",
        "# æ„å»º 4 ä¸ªæ•°æ®é›†å¯¹è±¡\n",
        "print(\"\\n--- æ„å»º Baseline (Window) ---\")\n",
        "train_dataset_win = ProteinGraphDataset(train_df, 'window', 'Train')\n",
        "test_dataset_win = ProteinGraphDataset(test_df, 'window', 'Test')\n",
        "\n",
        "print(\"\\n--- æ„å»º Advanced (ESM) ---\")\n",
        "train_dataset_esm = ProteinGraphDataset(train_df, 'esm', 'Train')\n",
        "test_dataset_esm = ProteinGraphDataset(test_df, 'esm', 'Test')\n",
        "\n",
        "# DataLoader\n",
        "BATCH_SIZE = 32\n",
        "train_loader_win = DataLoader(train_dataset_win, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_win = DataLoader(test_dataset_win, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "train_loader_esm = DataLoader(train_dataset_esm, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_esm = DataLoader(test_dataset_esm, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ==========================================\n",
        "# 4. GCN æ¨¡å‹å®šä¹‰\n",
        "# ==========================================\n",
        "\n",
        "class GCNPredictor(torch.nn.Module):\n",
        "    def __init__(self, num_node_features=20, hidden_dim=64):\n",
        "        super(GCNPredictor, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, 32)\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ==========================================\n",
        "# 5. è®­ç»ƒä¸è¯„ä¼°å‡½æ•°\n",
        "# ==========================================\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch)\n",
        "            loss = criterion(out, batch.y)\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def train_experiment(train_loader, test_loader, model_name=\"Model\", epochs=30):\n",
        "    model = GCNPredictor().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    history = {'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    print(f\"ğŸš€ Training {model_name}...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = criterion(out, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "        avg_train = total_loss / len(train_loader.dataset)\n",
        "        avg_test = evaluate(model, test_loader)\n",
        "\n",
        "        history['train_loss'].append(avg_train)\n",
        "        history['test_loss'].append(avg_test)\n",
        "\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"   Epoch {epoch+1}/{epochs} | Train: {avg_train:.4f} | Test: {avg_test:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# ==========================================\n",
        "# 6. æ‰§è¡Œè®­ç»ƒ\n",
        "# ==========================================\n",
        "\n",
        "# A. è®­ç»ƒ Baseline\n",
        "model_win, hist_win = train_experiment(train_loader_win, test_loader_win, \"Baseline (Window)\", epochs=30)\n",
        "torch.save(model_win.state_dict(), \"model_window.pth\")\n",
        "\n",
        "# B. è®­ç»ƒ ESM\n",
        "model_esm, hist_esm = train_experiment(train_loader_esm, test_loader_esm, \"Advanced (ESM)\", epochs=30)\n",
        "torch.save(model_esm.state_dict(), \"model_esm.pth\")\n",
        "\n",
        "# ==========================================\n",
        "# 7. ç»“æœè¯„ä¼°ä¸å¯è§†åŒ– (Loss + MSE/R2)\n",
        "# ==========================================\n",
        "\n",
        "# ğŸ”¥ æ–°å¢å‡½æ•°ï¼šè·å–æ‰€æœ‰é¢„æµ‹å€¼ä»¥è®¡ç®— R2/MSE\n",
        "def get_all_preds(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch)\n",
        "            all_preds.append(out.cpu().numpy())\n",
        "            all_labels.append(batch.y.cpu().numpy())\n",
        "    return np.concatenate(all_preds), np.concatenate(all_labels)\n",
        "\n",
        "# è·å–é¢„æµ‹ç»“æœ\n",
        "preds_win, y_win = get_all_preds(model_win, test_loader_win)\n",
        "preds_esm, y_esm = get_all_preds(model_esm, test_loader_esm)\n",
        "\n",
        "# è®¡ç®—æŒ‡æ ‡\n",
        "mse_win = mean_squared_error(y_win, preds_win)\n",
        "r2_win = r2_score(y_win, preds_win)\n",
        "mse_esm = mean_squared_error(y_esm, preds_esm)\n",
        "r2_esm = r2_score(y_esm, preds_esm)\n",
        "\n",
        "# --- ç»˜å›¾ (Loss + Scatter with Metrics) ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Subplot 1: Test Loss Curve\n",
        "axes[0].plot(hist_win['test_loss'], '--', color='gray', label='Baseline (Window)', linewidth=2)\n",
        "axes[0].plot(hist_esm['test_loss'], '-', color='#1f77b4', label='Advanced (ESM)', linewidth=2)\n",
        "axes[0].set_title(\"Test Loss Curve (Generalization)\", fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"MSE Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Predicted vs Actual Scatter\n",
        "axes[1].scatter(y_win, preds_win, color='gray', alpha=0.5, label='Baseline', s=30)\n",
        "axes[1].scatter(y_esm, preds_esm, color='#1f77b4', alpha=0.5, label='ESM', s=30)\n",
        "axes[1].plot([0, 1], [0, 1], 'r--', label='Perfect Fit') # å¯¹è§’çº¿\n",
        "\n",
        "# ğŸ”¥ æ˜¾ç¤º MSE å’Œ R2\n",
        "text_win = f\"Baseline:\\nMSE = {mse_win:.4f}\\n$R^2$ = {r2_win:.4f}\"\n",
        "text_esm = f\"ESM:\\nMSE = {mse_esm:.4f}\\n$R^2$ = {r2_esm:.4f}\"\n",
        "\n",
        "# åœ¨å›¾ä¸­æ·»åŠ æ–‡æœ¬æ¡†\n",
        "props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
        "axes[1].text(0.05, 0.95, text_win, transform=axes[1].transAxes, fontsize=10,\n",
        "             verticalalignment='top', bbox=props, color='gray', fontweight='bold')\n",
        "axes[1].text(0.05, 0.75, text_esm, transform=axes[1].transAxes, fontsize=10,\n",
        "             verticalalignment='top', bbox=props, color='#1f77b4', fontweight='bold')\n",
        "\n",
        "axes[1].set_title(f\"Predicted vs Actual Solubility\\n(Metric Evaluation)\", fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel(\"True Solubility\")\n",
        "axes[1].set_ylabel(\"Predicted Solubility\")\n",
        "axes[1].legend(loc='lower right')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 8. å¯è§£é‡Šæ€§åˆ†æ (Heatmap + Stacked Bars)\n",
        "# ==========================================\n",
        "\n",
        "def compute_saliency(model, data):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    data.x.requires_grad_() # å¼€å¯æ¢¯åº¦\n",
        "    out = model(data)\n",
        "    out.backward()\n",
        "    grads = data.x.grad\n",
        "    # L2 Norm\n",
        "    saliency = grads.pow(2).sum(dim=1).sqrt()\n",
        "    # Normalize 0-1\n",
        "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
        "    return saliency.cpu().detach().numpy(), out.item()\n",
        "\n",
        "# é€‰å–æµ‹è¯•é›†æ ·æœ¬\n",
        "test_idx = 0\n",
        "sample_win = test_dataset_win[test_idx]\n",
        "sample_esm = test_dataset_esm[test_idx]\n",
        "\n",
        "# è®¡ç®—\n",
        "model_win.load_state_dict(torch.load(\"model_window.pth\"))\n",
        "model_esm.load_state_dict(torch.load(\"model_esm.pth\"))\n",
        "s_win, p_win = compute_saliency(model_win, sample_win)\n",
        "s_esm, p_esm = compute_saliency(model_esm, sample_esm)\n",
        "\n",
        "# æˆªå–å‰ 50 ä¸ªæ®‹åŸºç»˜å›¾ (ä¸ºäº†æ¸…æ™°)\n",
        "viz_len = min(len(s_win), 50)\n",
        "residues = list(range(1, viz_len+1))\n",
        "s_win = s_win[:viz_len]\n",
        "s_esm = s_esm[:viz_len]\n",
        "\n",
        "# ğŸ”¥ ç»˜å›¾ï¼šçƒ­åŠ›å›¾ + æ‹¼åœ¨ä¸€èµ·çš„æŸ±çŠ¶å›¾ (Winåœ¨ä¸Š, ESMåœ¨ä¸‹)\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True, gridspec_kw={'height_ratios': [1, 2, 2], 'hspace': 0.1})\n",
        "\n",
        "# A. çƒ­åŠ›å›¾\n",
        "sns.heatmap(np.vstack([s_win, s_esm]), cmap=\"Reds\", ax=axes[0], cbar_kws={'label': 'Imp.'},\n",
        "            yticklabels=[\"Baseline\", \"ESM\"], xticklabels=residues)\n",
        "axes[0].set_title(f\"A. Saliency Heatmap (Predictions: Win={p_win:.2f}, ESM={p_esm:.2f})\", fontweight='bold')\n",
        "axes[0].set_yticks([0.5, 1.5])\n",
        "\n",
        "# B. Baseline Bar (Top)\n",
        "axes[1].bar(residues, s_win, color='gray', alpha=0.6, label='Baseline (Local)')\n",
        "axes[1].set_ylabel(\"Importance Score\")\n",
        "# ç§»é™¤ titleï¼Œæ”¾åœ¨å›¾ä¾‹æˆ–æ–‡å­—è¯´æ˜é‡Œæ›´ç®€æ´ï¼Œæˆ–è€…ä½¿ç”¨è¿™ç§æ–¹å¼ï¼š\n",
        "axes[1].text(0.01, 0.9, \"B. Baseline Model (Window Graph)\", transform=axes[1].transAxes, fontweight='bold')\n",
        "axes[1].grid(axis='y', linestyle='--', alpha=0.5)\n",
        "axes[1].legend(loc='upper right')\n",
        "\n",
        "# C. ESM Bar (Bottom) - æ‹¼åœ¨ Win ä¸‹é¢\n",
        "colors = ['red' if s > 0.6 else '#1f77b4' for s in s_esm]\n",
        "axes[2].bar(residues, s_esm, color=colors, alpha=0.8, label='ESM (Structure)')\n",
        "axes[2].set_ylabel(\"Importance Score\")\n",
        "axes[2].set_xlabel(\"Residue Position\")\n",
        "axes[2].text(0.01, 0.9, \"C. ESM Model (Structure Graph)\", transform=axes[2].transAxes, fontweight='bold')\n",
        "axes[2].grid(axis='y', linestyle='--', alpha=0.5)\n",
        "axes[2].legend(loc='upper right')\n",
        "# æ·»åŠ é«˜äº®è¯´æ˜\n",
        "axes[2].text(0.02, 0.8, \"Red = High Importance (>0.6)\", transform=axes[2].transAxes,\n",
        "             fontsize=9, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.suptitle(\"Interpretability Analysis: Gradient Saliency Comparison\", fontsize=16, y=0.96)\n",
        "plt.tight_layout() # å¯èƒ½ä¼šæŠ¥ warning ä½†ä¸å½±å“\n",
        "plt.subplots_adjust(top=0.92) # ç•™å‡º title ç©ºé—´\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E2TOqf_ywxbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¬¬2éƒ¨åˆ†ï¼šæ¥ä¸‹æ¥è¯•è¯•Transformer**"
      ],
      "metadata": {
        "id": "ALeMWNOMfgTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import os\n",
        "from sklearn.metrics import r2_score, mean_squared_error # ğŸ”¥ æ–°å¢ï¼šç”¨äºè®¡ç®—R2å’ŒMSE\n",
        "\n",
        "# ==========================================\n",
        "# 1. é…ç½®ä¸æ•°æ®åŠ è½½\n",
        "# ==========================================\n",
        "# è®¾ç½®éšæœºç§å­\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 120\n",
        "plt.rcParams['font.sans-serif'] = ['Arial'] # é˜²æ­¢ä¸­æ–‡ä¹±ç \n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# è®¾å¤‡é…ç½®\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# è¯»å–æ•°æ®å‡½æ•°\n",
        "def load_data():\n",
        "    if os.path.exists('eSol_train.csv') and os.path.exists('eSol_test.csv'):\n",
        "        print(\"ğŸ“¦ æ­£åœ¨åŠ è½½ CSV æ–‡ä»¶...\")\n",
        "        train_df = pd.read_csv('eSol_train.csv')\n",
        "        test_df = pd.read_csv('eSol_test.csv')\n",
        "    else:\n",
        "        print(\"âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶ï¼Œç”Ÿæˆä¼ªæ•°æ®ç”¨äºæ¼”ç¤º...\")\n",
        "        # ç”Ÿæˆéšæœºé•¿åº¦çš„åºåˆ—\n",
        "        def gen_seq():\n",
        "            length = np.random.randint(50, 500)\n",
        "            return ''.join(np.random.choice(list(\"ACDEFGHIKLMNPQRSTVWY\"), length))\n",
        "\n",
        "        dummy_train = {'sequence': [gen_seq() for _ in range(100)], 'solubility': np.random.rand(100)}\n",
        "        dummy_test = {'sequence': [gen_seq() for _ in range(20)], 'solubility': np.random.rand(20)}\n",
        "        train_df = pd.DataFrame(dummy_train)\n",
        "        test_df = pd.DataFrame(dummy_test)\n",
        "    return train_df, test_df\n",
        "\n",
        "# åŠ è½½æ•°æ®\n",
        "train_df, test_df = load_data()\n",
        "\n",
        "# ğŸ”¥ã€å…³é”®æ­¥éª¤ã€‘åŠ¨æ€è®¡ç®—æœ€å¤§åºåˆ—é•¿åº¦\n",
        "# è·å–æ‰€æœ‰åºåˆ—\n",
        "all_sequences = train_df['sequence'].tolist() + test_df['sequence'].tolist()\n",
        "# æ‰¾åˆ°æœ€é•¿åºåˆ—çš„é•¿åº¦\n",
        "max_seq_len_in_data = max([len(seq) for seq in all_sequences])\n",
        "# åŠ ä¸Šç‰¹æ®Š token (<cls>, <eos>) çš„é¢„ç•™ç©ºé—´\n",
        "MAX_LEN = max_seq_len_in_data + 2\n",
        "\n",
        "print(f\"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\")\n",
        "print(f\"   - è®­ç»ƒæ ·æœ¬æ•°: {len(train_df)}\")\n",
        "print(f\"   - æµ‹è¯•æ ·æœ¬æ•°: {len(test_df)}\")\n",
        "print(f\"   - æ•°æ®ä¸­æœ€é•¿åºåˆ—: {max_seq_len_in_data}\")\n",
        "print(f\"   - è®¾å®šæ¨¡å‹ MAX_LEN: {MAX_LEN} (å«ç‰¹æ®Štoken)\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Tokenizer ä¸ Dataset\n",
        "# ==========================================\n",
        "AA_VOCAB = \"<pad> <cls> <eos> <unk> A C D E F G H I K L M N P Q R S T V W Y\".split()\n",
        "aa_to_id = {aa: i for i, aa in enumerate(AA_VOCAB)}\n",
        "id_to_aa = {i: aa for aa, i in aa_to_id.items()}\n",
        "\n",
        "class ProteinTokenizer:\n",
        "    def __init__(self, max_len):\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def encode(self, seq):\n",
        "        # æˆªæ–­ (ä»¥é˜²ä¸‡ä¸€æœ‰æ–°æ•°æ®è¶…è¿‡ç»Ÿè®¡é•¿åº¦)\n",
        "        seq = seq[:self.max_len - 2]\n",
        "\n",
        "        ids = [aa_to_id.get(aa, aa_to_id['<unk>']) for aa in seq]\n",
        "        ids = [aa_to_id['<cls>']] + ids + [aa_to_id['<eos>']]\n",
        "\n",
        "        # Padding\n",
        "        n_pads = self.max_len - len(ids)\n",
        "        mask = [False] * len(ids) + [True] * n_pads # True è¡¨ç¤ºæ˜¯ Paddingï¼Œéœ€è¦è¢« Mask\n",
        "        ids = ids + [aa_to_id['<pad>']] * n_pads\n",
        "\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.bool)\n",
        "\n",
        "class ProteinSequenceDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.seqs = df['sequence'].tolist()\n",
        "        self.labels = pd.to_numeric(df['solubility'], errors='coerce').fillna(0.0).tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "        label = self.labels[idx]\n",
        "        input_ids, padding_mask = self.tokenizer.encode(seq)\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'padding_mask': padding_mask,\n",
        "            'labels': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# åˆå§‹åŒ– Tokenizer å’Œ Dataset\n",
        "tokenizer = ProteinTokenizer(max_len=MAX_LEN)\n",
        "train_ds = ProteinSequenceDataset(train_df, tokenizer)\n",
        "test_ds = ProteinSequenceDataset(test_df, tokenizer)\n",
        "\n",
        "# DataLoader\n",
        "BATCH_SIZE = 16 # å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œè¯·è°ƒå°æ­¤å€¼\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Transformer æ¨¡å‹æ­å»º\n",
        "# ==========================================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # åˆ›å»ºè¶³å¤Ÿå¤§çš„ä½ç½®ç¼–ç çŸ©é˜µ\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Seq_Len, Dim]\n",
        "        # åŠ¨æ€æˆªå–å¯¹åº”é•¿åº¦çš„ä½ç½®ç¼–ç \n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class ProteinTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, d_model=128, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # ä¼ å…¥ max_len ç¡®ä¿ PE çŸ©é˜µè¶³å¤Ÿå¤§\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                   dim_feedforward=256,\n",
        "                                                   batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, 1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, return_embedding=False):\n",
        "        # 1. Embedding\n",
        "        x = self.embedding(src)\n",
        "\n",
        "        # Hook for Interpretability\n",
        "        if return_embedding:\n",
        "            return x\n",
        "\n",
        "        x = x * math.sqrt(self.embedding.embedding_dim)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 2. Transformer\n",
        "        output = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # 3. Global Pooling (å¿½ç•¥ Padding)\n",
        "        if src_key_padding_mask is not None:\n",
        "            mask = (~src_key_padding_mask).unsqueeze(-1).float() # [B, L, 1]\n",
        "            output = output * mask\n",
        "            pooled = output.sum(dim=1) / (mask.sum(dim=1) + 1e-9)\n",
        "        else:\n",
        "            pooled = output.mean(dim=1)\n",
        "\n",
        "        # 4. Regression\n",
        "        return self.fc_out(pooled)\n",
        "\n",
        "    def forward_with_embeddings(self, embeddings, src_key_padding_mask=None):\n",
        "        \"\"\"ç”¨äºåŸºäºæ¢¯åº¦çš„å¯è§£é‡Šæ€§åˆ†æ\"\"\"\n",
        "        x = embeddings * math.sqrt(self.embedding.embedding_dim)\n",
        "        x = self.pos_encoder(x)\n",
        "        output = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        if src_key_padding_mask is not None:\n",
        "            mask = (~src_key_padding_mask).unsqueeze(-1).float()\n",
        "            output = output * mask\n",
        "            pooled = output.sum(dim=1) / (mask.sum(dim=1) + 1e-9)\n",
        "        else:\n",
        "            pooled = output.mean(dim=1)\n",
        "\n",
        "        return self.fc_out(pooled)\n",
        "\n",
        "# åˆå§‹åŒ–æ¨¡å‹\n",
        "model = ProteinTransformer(vocab_size=len(AA_VOCAB), max_len=MAX_LEN).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# ==========================================\n",
        "# 4. è®­ç»ƒä¸è¯„ä¼°\n",
        "# ==========================================\n",
        "def train_step(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        ids = batch['input_ids'].to(device)\n",
        "        mask = batch['padding_mask'].to(device)\n",
        "        labels = batch['labels'].to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(ids, src_key_padding_mask=mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def eval_step(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch['input_ids'].to(device)\n",
        "            mask = batch['padding_mask'].to(device)\n",
        "            labels = batch['labels'].to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(ids, src_key_padding_mask=mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "print(\"ğŸš€ å¼€å§‹è®­ç»ƒ...\")\n",
        "epochs = 30\n",
        "history = {'train': [], 'test': []}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_step(model, train_loader)\n",
        "    test_loss = eval_step(model, test_loader)\n",
        "\n",
        "    history['train'].append(train_loss)\n",
        "    history['test'].append(test_loss)\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# ğŸ”¥ è·å–æ‰€æœ‰é¢„æµ‹ç»“æœä»¥è®¡ç®— R2 å’Œ MSE\n",
        "def get_all_preds(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch['input_ids'].to(device)\n",
        "            mask = batch['padding_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(ids, src_key_padding_mask=mask)\n",
        "            all_preds.append(outputs.cpu().numpy().flatten())\n",
        "            all_labels.append(labels.cpu().numpy().flatten())\n",
        "\n",
        "    return np.concatenate(all_preds), np.concatenate(all_labels)\n",
        "\n",
        "# è®¡ç®—æŒ‡æ ‡\n",
        "y_pred, y_true = get_all_preds(model, test_loader)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "# --- ç»˜å›¾ï¼šLoss æ›²çº¿ + æ•£ç‚¹å›å½’å›¾ ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# å›¾1ï¼šè®­ç»ƒè¿‡ç¨‹ Loss\n",
        "axes[0].plot(history['train'], label='Train Loss', marker='.')\n",
        "axes[0].plot(history['test'], label='Test Loss', marker='.')\n",
        "axes[0].set_title(f\"Transformer Training Loss (MaxLen={MAX_LEN})\", fontweight='bold')\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"MSE Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# å›¾2ï¼šçœŸå®å€¼ vs é¢„æµ‹å€¼ (æ•£ç‚¹å›¾)\n",
        "axes[1].scatter(y_true, y_pred, alpha=0.6, color='#1f77b4', edgecolor='k', s=40)\n",
        "axes[1].plot([0, 1], [0, 1], 'r--', label='Perfect Fit') # å¯¹è§’çº¿\n",
        "axes[1].set_title(f\"Predicted vs True Solubility\\n(Test Set Evaluation)\", fontweight='bold')\n",
        "axes[1].set_xlabel(\"True Solubility\")\n",
        "axes[1].set_ylabel(\"Predicted Solubility\")\n",
        "\n",
        "# ğŸ”¥ åœ¨å›¾ä¸­æ˜¾ç¤ºæŒ‡æ ‡\n",
        "text_str = f\"MSE = {mse:.4f}\\n$R^2$ = {r2:.4f}\"\n",
        "props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
        "axes[1].text(0.05, 0.95, text_str, transform=axes[1].transAxes, fontsize=12,\n",
        "             verticalalignment='top', bbox=props, fontweight='bold', color='darkred')\n",
        "\n",
        "axes[1].legend(loc='lower right')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 5. å¯è§£é‡Šæ€§åˆ†æ (Saliency Map)\n",
        "# ==========================================\n",
        "print(\"ğŸ” ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æå›¾...\")\n",
        "\n",
        "def compute_saliency(model, idx, dataset):\n",
        "    model.eval()\n",
        "    sample = dataset[idx]\n",
        "    ids = sample['input_ids'].unsqueeze(0).to(device)\n",
        "    mask = sample['padding_mask'].unsqueeze(0).to(device)\n",
        "\n",
        "    # 1. Get Embeddings & Require Grad\n",
        "    embeddings = model(ids, src_key_padding_mask=mask, return_embedding=True)\n",
        "    embeddings.retain_grad()\n",
        "\n",
        "    # 2. Forward\n",
        "    output = model.forward_with_embeddings(embeddings, src_key_padding_mask=mask)\n",
        "\n",
        "    # 3. Backward\n",
        "    model.zero_grad()\n",
        "    output.backward()\n",
        "\n",
        "    # 4. Compute Saliency\n",
        "    grads = embeddings.grad[0] # [Seq, Dim]\n",
        "    saliency = grads.pow(2).sum(dim=1).sqrt().cpu().numpy() # L2 Norm\n",
        "\n",
        "    # Normalize\n",
        "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-9)\n",
        "    return saliency, sample['input_ids'], output.item()\n",
        "\n",
        "# é€‰å–ä¸€ä¸ªæµ‹è¯•æ ·æœ¬\n",
        "idx = 0\n",
        "saliency, token_ids, pred = compute_saliency(model, idx, test_ds)\n",
        "\n",
        "# è¿˜åŸåºåˆ— (å»é™¤ Pad)\n",
        "tokens = token_ids.numpy()\n",
        "real_len = 0\n",
        "for i, t in enumerate(tokens):\n",
        "    if t == aa_to_id['<pad>']: # æ‰¾åˆ°ç¬¬ä¸€ä¸ª pad\n",
        "        break\n",
        "    real_len = i + 1\n",
        "\n",
        "# æˆªå–æœ‰æ•ˆéƒ¨åˆ†\n",
        "saliency = saliency[:real_len]\n",
        "seq_chars = [id_to_aa[t] for t in tokens[:real_len]]\n",
        "\n",
        "# é™åˆ¶ç»˜å›¾é•¿åº¦ (å¦‚æœåºåˆ—ç‰¹åˆ«é•¿ï¼Œåªç”»å‰60ä¸ªä»¥ä¾¿å±•ç¤º)\n",
        "viz_limit = min(real_len, 60)\n",
        "saliency_viz = saliency[:viz_limit]\n",
        "chars_viz = seq_chars[:viz_limit]\n",
        "\n",
        "# --- ç»˜å›¾ ---\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 8), gridspec_kw={'height_ratios': [1, 2]})\n",
        "\n",
        "# 1. çƒ­åŠ›å›¾\n",
        "sns.heatmap([saliency_viz], cmap=\"Reds\", ax=axes[0], cbar_kws={'label': 'Gradient Imp.'},\n",
        "            xticklabels=chars_viz, yticklabels=['Seq'])\n",
        "axes[0].set_title(f\"A. Saliency Heatmap (Pred: {pred:.2f})\", fontweight='bold')\n",
        "\n",
        "# 2. æŸ±çŠ¶å›¾\n",
        "colors = ['red' if s > 0.5 else '#1f77b4' for s in saliency_viz]\n",
        "axes[1].bar(range(len(saliency_viz)), saliency_viz, color=colors, alpha=0.8)\n",
        "axes[1].set_xticks(range(len(saliency_viz)))\n",
        "axes[1].set_xticklabels(chars_viz, fontsize=9)\n",
        "axes[1].set_title(\"B. Residue Importance Profile\", fontweight='bold')\n",
        "axes[1].set_ylabel(\"Importance Score\")\n",
        "axes[1].set_xlabel(\"Sequence Position\")\n",
        "\n",
        "# é«˜äº®æ ‡æ³¨\n",
        "axes[1].text(0.01, 0.9, \"Red = High Impact (>0.5)\", transform=axes[1].transAxes,\n",
        "             bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… å®Œæˆã€‚\")"
      ],
      "metadata": {
        "id": "kodkxxydnIN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¬¬3éƒ¨åˆ†ï¼šå¾ˆå¥½ï¼Œæœ€åè¯•è¯•å¤šæ¨¡æ€**"
      ],
      "metadata": {
        "id": "xb992dcTfo5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ==========================================\n",
        "# 1. å…¨å±€é…ç½® & ESM æ¨¡å‹åŠ è½½\n",
        "# ==========================================\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 120\n",
        "plt.rcParams['font.sans-serif'] = ['Arial']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# åŠ è½½ ESM-2 æ¨¡å‹ (ç”¨äºæå– ESM ç‰¹å¾åˆ†æ”¯çš„è¾“å…¥)\n",
        "ESM_MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
        "print(f\"â³ Loading ESM Model: {ESM_MODEL_NAME}...\")\n",
        "try:\n",
        "    esm_tokenizer = AutoTokenizer.from_pretrained(ESM_MODEL_NAME)\n",
        "    esm_model = AutoModel.from_pretrained(ESM_MODEL_NAME, attn_implementation=\"eager\").eval().to(device)\n",
        "    print(\"âœ… ESM Model loaded!\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ESM Load Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==========================================\n",
        "# 2. ç‰¹å¾æå–å·¥å…·å‡½æ•°\n",
        "# ==========================================\n",
        "\n",
        "# --- A. GNN å·¥å…· (One-Hot + Window) ---\n",
        "AA_VOCAB = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "aa_to_int = {aa: i for i, aa in enumerate(AA_VOCAB)}\n",
        "\n",
        "def get_one_hot(seq):\n",
        "    L = len(seq)\n",
        "    x = torch.zeros(L, 20)\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in aa_to_int:\n",
        "            x[i, aa_to_int[aa]] = 1.0\n",
        "    return x\n",
        "\n",
        "def get_window_adj(seq, window=2):\n",
        "    L = len(seq)\n",
        "    src, dst = [], []\n",
        "    for i in range(L):\n",
        "        # Self loop\n",
        "        src.append(i); dst.append(i)\n",
        "        start = max(0, i - window)\n",
        "        end = min(L, i + window + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                src.append(i); dst.append(j)\n",
        "    return torch.tensor([src, dst], dtype=torch.long)\n",
        "\n",
        "# --- B. Transformer å·¥å…· (Tokenizer) ---\n",
        "TRANS_VOCAB = \"<pad> <cls> <eos> <unk> A C D E F G H I K L M N P Q R S T V W Y\".split()\n",
        "trans_to_id = {aa: i for i, aa in enumerate(TRANS_VOCAB)}\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, max_len):\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def encode(self, seq):\n",
        "        seq = seq[:self.max_len - 2]\n",
        "        ids = [trans_to_id.get(aa, trans_to_id['<unk>']) for aa in seq]\n",
        "        ids = [trans_to_id['<cls>']] + ids + [trans_to_id['<eos>']]\n",
        "        n_pads = self.max_len - len(ids)\n",
        "        mask = [False] * len(ids) + [True] * n_pads\n",
        "        ids = ids + [trans_to_id['<pad>']] * n_pads\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.bool)\n",
        "\n",
        "# --- C. ESM å·¥å…· (Pre-trained Embedding) ---\n",
        "def get_esm_embedding(seq):\n",
        "    \"\"\"æå–æ•´æ¡åºåˆ—çš„ ESM ç‰¹å¾å‘é‡ (320ç»´)\"\"\"\n",
        "    seq = seq[:1020]\n",
        "    inputs = esm_tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1022).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = esm_model(**inputs)\n",
        "    # å– [CLS] token æˆ–è€… Mean Pooling\n",
        "    # è¿™é‡Œä½¿ç”¨ Mean Pooling (å»é™¤ cls/eos)\n",
        "    embedding = outputs.last_hidden_state[0, 1:-1, :].mean(dim=0).cpu()\n",
        "    return embedding # [320]\n",
        "\n",
        "# ==========================================\n",
        "# 3. ç»¼åˆæ•°æ®é›† (All-in-One Dataset)\n",
        "# ==========================================\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.data = []\n",
        "        print(f\"ğŸ“¦ Processing {len(df)} sequences...\")\n",
        "\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building Data\"):\n",
        "            try:\n",
        "                seq = row['sequence']\n",
        "                label = float(row['solubility'])\n",
        "\n",
        "                # 1. GNN Data\n",
        "                gnn_x = get_one_hot(seq)\n",
        "                gnn_edge_index = get_window_adj(seq)\n",
        "\n",
        "                # 2. Transformer Data\n",
        "                trans_ids, trans_mask = tokenizer.encode(seq)\n",
        "\n",
        "                # 3. ESM Data\n",
        "                esm_emb = get_esm_embedding(seq)\n",
        "\n",
        "                self.data.append({\n",
        "                    'gnn_x': gnn_x,\n",
        "                    'gnn_edge_index': gnn_edge_index,\n",
        "                    'trans_ids': trans_ids,\n",
        "                    'trans_mask': trans_mask,\n",
        "                    'esm_emb': esm_emb,\n",
        "                    'y': torch.tensor([label], dtype=torch.float)\n",
        "                })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # GNN Batch\n",
        "    gnn_list = [Data(x=i['gnn_x'], edge_index=i['gnn_edge_index']) for i in batch]\n",
        "    gnn_batch = Batch.from_data_list(gnn_list)\n",
        "\n",
        "    # Trans Batch\n",
        "    trans_ids = torch.stack([i['trans_ids'] for i in batch])\n",
        "    trans_mask = torch.stack([i['trans_mask'] for i in batch])\n",
        "\n",
        "    # ESM Batch\n",
        "    esm_emb = torch.stack([i['esm_emb'] for i in batch])\n",
        "\n",
        "    # Label\n",
        "    y = torch.stack([i['y'] for i in batch])\n",
        "\n",
        "    return {\n",
        "        'gnn_batch': gnn_batch,\n",
        "        'trans_ids': trans_ids,\n",
        "        'trans_mask': trans_mask,\n",
        "        'esm_emb': esm_emb,\n",
        "        'y': y\n",
        "    }\n",
        "\n",
        "# --- Load Data ---\n",
        "if os.path.exists('eSol_train.csv'):\n",
        "    train_df = pd.read_csv('eSol_train.csv')#.iloc[:50] # Debug\n",
        "    test_df = pd.read_csv('eSol_test.csv')#.iloc[:20]\n",
        "else:\n",
        "    print(\"âš ï¸ Using Dummy Data\")\n",
        "    dummy = {'sequence': ['MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG']*20, 'solubility': np.random.rand(20)}\n",
        "    train_df = pd.DataFrame(dummy)\n",
        "    test_df = pd.DataFrame(dummy)\n",
        "\n",
        "# Dynamic Max Len\n",
        "all_seqs = train_df['sequence'].tolist() + test_df['sequence'].tolist()\n",
        "MAX_LEN = max([len(s) for s in all_seqs]) + 2\n",
        "print(f\"ğŸ“Š Max Len: {MAX_LEN}\")\n",
        "\n",
        "tokenizer = SimpleTokenizer(MAX_LEN)\n",
        "train_ds = MultiModalDataset(train_df, tokenizer)\n",
        "test_ds = MultiModalDataset(test_df, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ==========================================\n",
        "# 4. æ¨¡å—åŒ–æ¨¡å‹å®šä¹‰\n",
        "# ==========================================\n",
        "\n",
        "# --- Module 1: GNN (Window) ---\n",
        "class GNNBranch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(20, 64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.conv3 = GCNConv(64, 32)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_mean_pool(x, batch) # [B, 32]\n",
        "        return x\n",
        "\n",
        "# --- Module 2: Transformer (Seq) ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransBranch(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 64)\n",
        "        self.pos_encoder = PositionalEncoding(64, max_len)\n",
        "        layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=128, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(layer, num_layers=2)\n",
        "        self.fc = nn.Linear(64, 32)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embedding(src) * math.sqrt(64)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x, src_key_padding_mask=mask)\n",
        "        mask_expanded = (~mask).unsqueeze(-1).float()\n",
        "        x = (x * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-9)\n",
        "        return self.fc(x) # [B, 32]\n",
        "\n",
        "# --- Module 3: ESM (Pre-trained) ---\n",
        "class ESMBranch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(320, 32) # Project 320 -> 32\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.fc(x)) # [B, 32]\n",
        "\n",
        "# --- Main Wrapper ---\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self, use_gnn=False, use_trans=False, use_esm=False):\n",
        "        super().__init__()\n",
        "        self.use_gnn = use_gnn\n",
        "        self.use_trans = use_trans\n",
        "        self.use_esm = use_esm\n",
        "\n",
        "        if use_gnn: self.gnn = GNNBranch()\n",
        "        if use_trans: self.trans = TransBranch(len(TRANS_VOCAB), MAX_LEN)\n",
        "        if use_esm: self.esm = ESMBranch()\n",
        "\n",
        "        # Calculate Fusion Dim\n",
        "        dim = 0\n",
        "        if use_gnn: dim += 32\n",
        "        if use_trans: dim += 32\n",
        "        if use_esm: dim += 32\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch):\n",
        "        feats = []\n",
        "        if self.use_gnn:\n",
        "            feats.append(self.gnn(batch['gnn_batch']))\n",
        "        if self.use_trans:\n",
        "            feats.append(self.trans(batch['trans_ids'], batch['trans_mask']))\n",
        "        if self.use_esm:\n",
        "            feats.append(self.esm(batch['esm_emb']))\n",
        "\n",
        "        combined = torch.cat(feats, dim=1)\n",
        "        return self.regressor(combined)\n",
        "\n",
        "# ==========================================\n",
        "# 5. è®­ç»ƒå¾ªç¯ (Epochs=50)\n",
        "# ==========================================\n",
        "\n",
        "def run_experiment(name, use_gnn, use_trans, use_esm, epochs=50):\n",
        "    print(f\"\\nğŸš€ Experiment: {name}\")\n",
        "    model = UnifiedModel(use_gnn, use_trans, use_esm).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    # Progress Bar\n",
        "    pbar = tqdm(range(epochs), desc=f\"Training {name}\", unit=\"ep\")\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        ep_loss = 0\n",
        "        for batch in train_loader:\n",
        "            # Move to device\n",
        "            batch_data = {\n",
        "                'gnn_batch': batch['gnn_batch'].to(device),\n",
        "                'trans_ids': batch['trans_ids'].to(device),\n",
        "                'trans_mask': batch['trans_mask'].to(device),\n",
        "                'esm_emb': batch['esm_emb'].to(device)\n",
        "            }\n",
        "            y = batch['y'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch_data)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            ep_loss += loss.item()\n",
        "\n",
        "        avg_loss = ep_loss / len(train_loader)\n",
        "        loss_history.append(avg_loss)\n",
        "        pbar.set_postfix({'Loss': f\"{avg_loss:.4f}\"})\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch_data = {\n",
        "                'gnn_batch': batch['gnn_batch'].to(device),\n",
        "                'trans_ids': batch['trans_ids'].to(device),\n",
        "                'trans_mask': batch['trans_mask'].to(device),\n",
        "                'esm_emb': batch['esm_emb'].to(device)\n",
        "            }\n",
        "            y = batch['y'].to(device)\n",
        "            out = model(batch_data)\n",
        "            preds.append(out.cpu().numpy())\n",
        "            labels.append(y.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    labels = np.concatenate(labels)\n",
        "    mse = mean_squared_error(labels, preds)\n",
        "    r2 = r2_score(labels, preds)\n",
        "\n",
        "    print(f\"   âœ… Result: MSE={mse:.4f}, R2={r2:.4f}\")\n",
        "    return loss_history, mse, r2, preds, labels\n",
        "\n",
        "# --- Run 4 Experiments ---\n",
        "# 1. GNN Only\n",
        "h1, m1, r1, p1, y1 = run_experiment(\"GNN Only\", True, False, False)\n",
        "# 2. Transformer Only\n",
        "h2, m2, r2, p2, y2 = run_experiment(\"Trans Only\", False, True, False)\n",
        "# 3. ESM Only (Linear Probe)\n",
        "h3, m3, r3, p3, y3 = run_experiment(\"ESM Only\", False, False, True)\n",
        "# 4. Fusion (GNN+Trans+ESM)\n",
        "h4, m4, r4, p4, y4 = run_experiment(\"Fusion (All)\", True, True, True)\n",
        "\n",
        "# ==========================================\n",
        "# 6. å¯è§†åŒ–å¯¹æ¯”\n",
        "# ==========================================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. Loss Curves\n",
        "axes[0].plot(h1, label='GNN', linestyle=':')\n",
        "axes[0].plot(h2, label='Trans', linestyle='--')\n",
        "axes[0].plot(h3, label='ESM', linestyle='-.')\n",
        "axes[0].plot(h4, label='Fusion', linewidth=2, color='red')\n",
        "axes[0].set_title(\"Training Loss\")\n",
        "axes[0].legend()\n",
        "\n",
        "# 2. Metrics (MSE & R2)\n",
        "models = ['GNN', 'Trans', 'ESM', 'Fusion']\n",
        "mse_list = [m1, m2, m3, m4]\n",
        "r2_list = [r1, r2, r3, r4]\n",
        "x = np.arange(4); w = 0.35\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.bar(x - w/2, mse_list, w, label='MSE (Lower better)', color='salmon', alpha=0.8)\n",
        "ax2.bar(x + w/2, r2_list, w, label='R2 (Higher better)', color='skyblue', alpha=0.8)\n",
        "ax2.set_xticks(x); ax2.set_xticklabels(models)\n",
        "ax2.set_title(\"Model Comparison\")\n",
        "ax2.legend()\n",
        "# Labels\n",
        "for i, v in enumerate(mse_list): ax2.text(i-w/2, v, f\"{v:.3f}\", ha='center', va='bottom', fontsize=8)\n",
        "for i, v in enumerate(r2_list): ax2.text(i+w/2, v, f\"{v:.3f}\", ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 3. Scatter (Best Model: Fusion)\n",
        "ax3 = axes[2]\n",
        "ax3.scatter(y4, p4, alpha=0.6, c='purple', s=40)\n",
        "ax3.plot([0,1], [0,1], 'r--')\n",
        "ax3.set_title(f\"Fusion Model Pred (R2={r4:.3f})\")\n",
        "ax3.set_xlabel(\"True Solubility\")\n",
        "ax3.set_ylabel(\"Predicted Solubility\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yiVSJCgw0XxY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}