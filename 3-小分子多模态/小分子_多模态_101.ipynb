{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "X7AN78JLc32N",
        "6QIYUh0E2dZw",
        "7RahmMfxUWFX",
        "tX-pG0y2r3Kr",
        "vew-INB63gus",
        "6DYIj5B83owB",
        "KIFctOM-3rYt",
        "sBjRmCfO7jNY",
        "Cdl0a15A9rV-",
        "kNu2d0MbLlnx"
      ],
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ÂÆâË£Ö‰æùËµñÂ∫ì**"
      ],
      "metadata": {
        "id": "X7AN78JLc32N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QIYUh0E2dZw"
      },
      "source": [
        "## **‰ΩúËÄÖÁÆÄ‰ªã**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X91JIVkqUNMV"
      },
      "source": [
        "#### ‰ΩúËÄÖÔºö**‚ö°Â∞èÈó™Áîµ‚ö°**\n",
        "\n",
        "#### BÁ´ô‰∏ªÈ°µ\n",
        "- [Â∞èÈó™ÁîµÁöÑBÁ´ô‰∏ªÈ°µ](https://space.bilibili.com/122699831?spm_id_from=333.1007.0.0)\n",
        "\n",
        "#### ‰∫§ÊµÅÁæ§\n",
        "Ê¨¢ËøéÂä†ÂÖ•AIDD‰∫§ÊµÅÁæ§ÔºÅ  \n",
        "Âä†ÊàëÂæÆ‰ø°ÔºàÂæÆ‰ø°Âè∑: `xxxFLASHxxx`ÔºâÔºåÈÇÄËØ∑‰Ω†ËøõÁæ§„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RahmMfxUWFX"
      },
      "source": [
        "## **ÂÆâË£Ö‰æùËµñÂ∫ì**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cf3YlQPJkqE"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install rdkit\n",
        "!pip install networkx pandas\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.datasets import KarateClub\n",
        "dataset = KarateClub()\n",
        "print(f'Dataset: {dataset}:')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "WFAe6SibD0QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Á¨¨1ÈÉ®ÂàÜÔºö‰∏ÄÊ≠•‰∏ÄÊ≠•Êù•ÔºåÂÖàËØïËØïÁÆÄÂçïÁöÑ‰∫åÊ®°ÊÄÅËûçÂêà**"
      ],
      "metadata": {
        "id": "tX-pG0y2r3Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graph+ÊèèËø∞Á¨¶/ÊåáÁ∫π**"
      ],
      "metadata": {
        "id": "vew-INB63gus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# RDKit & Sklearn\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdchem, rdBase, Descriptors, AllChem\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# PyG (Ëá™Âä®Ê£ÄÊµãÂÆâË£Ö)\n",
        "try:\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    from torch_geometric.nn import GATConv, global_mean_pool\n",
        "except ImportError:\n",
        "    print(\"Ê≠£Âú®ÂÆâË£Ö PyG ‰æùËµñ...\")\n",
        "    os.system(\"pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\")\n",
        "    os.system(\"pip install -q torch-geometric\")\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    from torch_geometric.nn import GATConv, global_mean_pool\n",
        "\n",
        "# ÈÖçÁΩÆÁéØÂ¢É\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "rdBase.DisableLog('rdApp.warning')\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using Device: {DEVICE}\")\n",
        "\n",
        "# ÁªòÂõæÈ£éÊ†º\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ‰∏•Ê†ºÂ§çÂàªÂéü GNN ÁâπÂæÅÊèêÂèñ‰ª£Á†Å (Feature Extraction)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- ËæÖÂä©ÂáΩÊï∞ ---\n",
        "def one_hot_encoding(value, choices):\n",
        "    encoding = [0] * (len(choices) + 1)\n",
        "    index = choices.index(value) if value in choices else -1\n",
        "    encoding[index] = 1\n",
        "    return encoding\n",
        "\n",
        "# --- ÂéüÁâà MoleculeFeaturizer (‰øùÊåÅ‰∏çÂèò) ---\n",
        "class MoleculeFeaturizer(object):\n",
        "    def _atom_featurizer(self, atom):\n",
        "        # 37ÁßçÂéüÂ≠ê + 1ÁßçÂÖ∂‰ªñ\n",
        "        atomic_numer = [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 46, 47, 48, 49, 50, 51, 52, 53]\n",
        "        return (one_hot_encoding(atom.GetAtomicNum(), atomic_numer) +\n",
        "                one_hot_encoding(atom.GetTotalDegree(), list(range(5))) +\n",
        "                one_hot_encoding(int(atom.GetHybridization()), list(range(len(Chem.HybridizationType.names)-1))) +\n",
        "                one_hot_encoding(atom.GetChiralTag(), list(range(len(Chem.ChiralType.names)-1))) +\n",
        "                one_hot_encoding(atom.GetTotalNumHs(), list(range(5))) +\n",
        "                [1 if atom.GetIsAromatic() else 0])\n",
        "\n",
        "    def _bond_featurizer(self, bond):\n",
        "        bt = [\n",
        "            int(bond.GetBondType() == rdchem.BondType.SINGLE),\n",
        "            int(bond.GetBondType() == rdchem.BondType.DOUBLE),\n",
        "            int(bond.GetBondType() == rdchem.BondType.TRIPLE),\n",
        "            int(bond.GetBondType() == rdchem.BondType.AROMATIC)\n",
        "        ]\n",
        "        return bt + [int(bond.GetIsConjugated()), int(bond.IsInRing())]\n",
        "\n",
        "    def __call__(self, mol):\n",
        "        atom_features = [self._atom_featurizer(atom) for atom in mol.GetAtoms()]\n",
        "        x = torch.tensor(atom_features, dtype=torch.float32)\n",
        "\n",
        "        adj = Chem.GetAdjacencyMatrix(mol)\n",
        "        coo_adj = coo_matrix(adj)\n",
        "        row, col = coo_adj.row, coo_adj.col\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "\n",
        "        bond_features = []\n",
        "        for i, j in zip(row, col):\n",
        "            bond = mol.GetBondBetweenAtoms(int(i), int(j))\n",
        "            bond_features.append(self._bond_featurizer(bond))\n",
        "\n",
        "        edge_attr = torch.tensor(bond_features, dtype=torch.float32) if len(bond_features) > 0 else torch.empty((0, 6), dtype=torch.float32)\n",
        "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "# --- ËæÖÂä©Êï∞ÊçÆÊèêÂèñÂáΩÊï∞ ---\n",
        "def get_descriptors(mol):\n",
        "    # Â∏∏Áî® 9 ÁßçÊèèËø∞Á¨¶\n",
        "    return [\n",
        "        Descriptors.MolWt(mol), Descriptors.MolLogP(mol),\n",
        "        Descriptors.NumHDonors(mol), Descriptors.NumHAcceptors(mol),\n",
        "        Descriptors.TPSA(mol), Descriptors.NumRotatableBonds(mol),\n",
        "        Descriptors.FractionCSP3(mol), Descriptors.HallKierAlpha(mol),\n",
        "        Descriptors.RingCount(mol)\n",
        "    ]\n",
        "\n",
        "def get_fingerprint(mol, n_bits=1024):\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
        "    return np.array(fp)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Êï∞ÊçÆÈõÜÂÆö‰πâ\n",
        "# ==============================================================================\n",
        "class FusionDataset(Dataset):\n",
        "    def __init__(self, df, featurizer):\n",
        "        self.data = []\n",
        "        print(\"Ê≠£Âú®ÊûÑÂª∫Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜ...\")\n",
        "\n",
        "        # Êî∂ÈõÜÊèèËø∞Á¨¶‰ª•ËøõË°åÂΩí‰∏ÄÂåñ\n",
        "        all_descs = []\n",
        "        temp_data = []\n",
        "\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            mol = Chem.MolFromSmiles(row['Smiles'])\n",
        "            if mol is None: continue\n",
        "\n",
        "            # 1. Graph Data (DataÂØπË±°)\n",
        "            graph = featurizer(mol)\n",
        "\n",
        "            # 2. Descriptors (List)\n",
        "            desc = get_descriptors(mol)\n",
        "\n",
        "            # 3. Fingerprints (Array)\n",
        "            fp = get_fingerprint(mol, n_bits=1024)\n",
        "\n",
        "            all_descs.append(desc)\n",
        "            temp_data.append({\n",
        "                'graph': graph,\n",
        "                'fp': torch.tensor(fp, dtype=torch.float32),\n",
        "                'y': torch.tensor([row['logSw']], dtype=torch.float32)\n",
        "            })\n",
        "\n",
        "        # ÊèèËø∞Á¨¶Ê†áÂáÜÂåñ (Z-Score)\n",
        "        scaler = StandardScaler()\n",
        "        all_descs_scaled = scaler.fit_transform(all_descs)\n",
        "\n",
        "        for i, item in enumerate(temp_data):\n",
        "            item['desc'] = torch.tensor(all_descs_scaled[i], dtype=torch.float32)\n",
        "            self.data.append(item)\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    graphs = Batch.from_data_list([x['graph'] for x in batch])\n",
        "    descs = torch.stack([x['desc'] for x in batch])\n",
        "    fps = torch.stack([x['fp'] for x in batch])\n",
        "    labels = torch.cat([x['y'] for x in batch])\n",
        "    return graphs, descs, fps, labels\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Ê®°ÂûãÂÆö‰πâ (‰∏•Ê†ºÂ§çÂàª + ËûçÂêà)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 3.1 ÊùÉÈáçÂàùÂßãÂåñ (Âéü‰ª£Á†Å‰∏≠ÁöÑÂáΩÊï∞) ---\n",
        "def init_weight(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "# --- 3.2 GATNet + MLP (ÁªìÊûÑÂÆåÂÖ®‰∏ÄËá¥ÔºåÂè™ÊòØ Forward Á®çÂæÆÊîπÂä®‰ª•ËæìÂá∫ÁâπÂæÅ) ---\n",
        "class GAT_FeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_dim=69, hidden_dim=16, mlp_hidden=128, heads=2, edge_dim=6, dropout=0.4):\n",
        "        super(GAT_FeatureExtractor, self).__init__()\n",
        "\n",
        "        # === ‰∏•Ê†ºÂ§çÂàª GATNet ÁªìÊûÑ ===\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout, edge_dim=edge_dim)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout, edge_dim=edge_dim)\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # === ‰∏•Ê†ºÂ§çÂàª MLP ÁªìÊûÑ (Èô§‰∫ÜÊúÄÂêé‰∏ÄÂ±Ç) ===\n",
        "        # Âéü MLP: Linear(16->128) -> ReLU -> Drop(0.4) -> Linear(128->64) -> ReLU -> Drop(0.4) -> Linear(64->1)\n",
        "        # ËøôÈáåÊàë‰ª¨Âè™ÂèñÂâç‰∏§Â±Ç‰Ωú‰∏∫ÁâπÂæÅÊèêÂèñÂô®ÔºåËæìÂá∫ 64 Áª¥ÂêëÈáè\n",
        "        self.mlp_part1 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, mlp_hidden), # 16 -> 128\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden, mlp_hidden // 2), # 128 -> 64\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # ÂàùÂßãÂåñÊùÉÈáç\n",
        "        self.mlp_part1.apply(init_weight)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, edge_attr):\n",
        "        # GAT Forward\n",
        "        x = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
        "        x = F.elu(x)\n",
        "        x = self.gat2(x, edge_index, edge_attr=edge_attr)\n",
        "        x = F.elu(x)\n",
        "        x = self.pool(x, batch) # [Batch, 16]\n",
        "\n",
        "        # MLP Forward (Part 1)\n",
        "        feat = self.mlp_part1(x) # [Batch, 64]\n",
        "        return feat\n",
        "\n",
        "# --- 3.3 ËûçÂêàÊ®°Âûã (ÈÄÇÈÖç‰∏âÁßçÊ®°Âºè) ---\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, mode, aux_dim=0):\n",
        "        \"\"\"\n",
        "        mode: 'only' (‰ªÖÂõæ), 'desc' (Âõæ+ÊèèËø∞Á¨¶), 'fp' (Âõæ+ÊåáÁ∫π)\n",
        "        aux_dim: ËæÖÂä©ÁâπÂæÅÁª¥Â∫¶ (‰ªÖÂΩì mode != 'only' Êó∂ÊúâÁî®)\n",
        "        \"\"\"\n",
        "        super(FusionModel, self).__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        # 1. Graph Branch (ÂèÇÊï∞‰∏•Ê†ºÈîÅÂÆö‰∏∫Âéü‰ª£Á†ÅÈªòËÆ§ÂÄº)\n",
        "        # input_dim=69, hidden=16, mlp_hidden=128, heads=2, edge=6, drop=0.4\n",
        "        self.graph_encoder = GAT_FeatureExtractor()\n",
        "\n",
        "        # 2. Auxiliary Branch & Fusion Head\n",
        "        graph_feat_dim = 64\n",
        "\n",
        "        if mode == 'only':\n",
        "            # GNN Only: Áõ¥Êé•Êé•Âéü‰ª£Á†ÅÁöÑÊúÄÂêé‰∏ÄÂ±Ç MLP\n",
        "            self.head = nn.Linear(graph_feat_dim, 1) # Linear(64->1)\n",
        "            self.aux_encoder = None\n",
        "\n",
        "        else:\n",
        "            # GNN + Aux\n",
        "            self.aux_encoder = nn.Sequential(\n",
        "                nn.Linear(aux_dim, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.4), # ‰øùÊåÅ‰∏ÄËá¥ÁöÑ dropout\n",
        "                nn.Linear(128, 64),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "            # Fusion Head\n",
        "            # ËæìÂÖ•: Graph(64) + Aux(64) = 128\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(128, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.Linear(64, 1)\n",
        "            )\n",
        "\n",
        "        self.apply(init_weight)\n",
        "\n",
        "    def forward(self, graph, aux_data=None):\n",
        "        # Graph Feature\n",
        "        g_feat = self.graph_encoder(graph.x, graph.edge_index, graph.batch, graph.edge_attr)\n",
        "\n",
        "        if self.mode == 'only':\n",
        "            return self.head(g_feat).squeeze()\n",
        "\n",
        "        # Aux Feature\n",
        "        a_feat = self.aux_encoder(aux_data)\n",
        "\n",
        "        # Concat\n",
        "        combined = torch.cat([g_feat, a_feat], dim=1)\n",
        "\n",
        "        # Predict\n",
        "        return self.head(combined).squeeze()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. ËÆ≠ÁªÉ‰∏éËØÑ‰º∞ËÑöÊú¨\n",
        "# ==============================================================================\n",
        "def train_step(model, loader, optimizer, criterion, mode):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for graphs, descs, fps, labels in loader:\n",
        "        graphs = graphs.to(DEVICE)\n",
        "\n",
        "        if mode == 'desc': aux = descs.to(DEVICE)\n",
        "        elif mode == 'fp': aux = fps.to(DEVICE)\n",
        "        else: aux = None\n",
        "\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(graphs, aux)\n",
        "        loss = criterion(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_step(model, loader, mode):\n",
        "    model.eval()\n",
        "    preds_all, labels_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for graphs, descs, fps, labels in loader:\n",
        "            graphs = graphs.to(DEVICE)\n",
        "\n",
        "            if mode == 'desc': aux = descs.to(DEVICE)\n",
        "            elif mode == 'fp': aux = fps.to(DEVICE)\n",
        "            else: aux = None\n",
        "\n",
        "            preds = model(graphs, aux)\n",
        "            preds_all.extend(preds.cpu().numpy())\n",
        "            labels_all.extend(labels.numpy())\n",
        "\n",
        "    mse = mean_squared_error(labels_all, preds_all)\n",
        "    r2 = r2_score(labels_all, preds_all)\n",
        "    return mse, r2, np.array(labels_all), np.array(preds_all)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. ‰∏ªÁ®ãÂ∫è\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. ÂáÜÂ§áÊï∞ÊçÆ\n",
        "    csv_path = '/content/SweetpredDB.csv'\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(\"Creating dummy data...\")\n",
        "        df = pd.DataFrame({'Smiles': ['C']*20 + ['CC']*20, 'logSw': np.random.rand(40)})\n",
        "    else:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.rename(columns=str.lower, inplace=True)\n",
        "        df.rename(columns={'smiles': 'Smiles', 'logsw': 'logSw'}, inplace=True)\n",
        "\n",
        "    featurizer = MoleculeFeaturizer()\n",
        "    dataset = FusionDataset(df, featurizer)\n",
        "\n",
        "    # ÂàíÂàÜ (8:2) - ‰πüÂèØ‰ª•ÊîπÊàê 8:1:1Ôºå‰ΩÜ‰∏∫‰∫ÜÁÆÄÊ¥ÅÂØπÊØîËøôÈáåÁî® 8:2\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_ds, test_ds = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_ds, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "    # Ëé∑ÂèñÁª¥Â∫¶\n",
        "    sample = dataset[0]\n",
        "    desc_dim = sample['desc'].shape[0] # 9\n",
        "    fp_dim = sample['fp'].shape[0]     # 1024\n",
        "\n",
        "    print(f\"\\nDimensions -> Descriptors: {desc_dim}, Fingerprints: {fp_dim}\")\n",
        "\n",
        "    # 2. ÂàùÂßãÂåñ‰∏â‰∏™Ê®°Âûã\n",
        "    # Model A: GNN Only\n",
        "    model_only = FusionModel('only').to(DEVICE)\n",
        "    # Model B: Graph + Descriptors\n",
        "    model_desc = FusionModel('desc', desc_dim).to(DEVICE)\n",
        "    # Model C: Graph + Fingerprints\n",
        "    model_fp = FusionModel('fp', fp_dim).to(DEVICE)\n",
        "\n",
        "    # 3. ‰ºòÂåñÂô® (‰∏•Ê†ºÂ§çÂàªÂéü‰ª£Á†ÅÂèÇÊï∞: lr=0.001, weight_decay=2e-3)\n",
        "    opt_only = optim.Adam(model_only.parameters(), lr=0.001, weight_decay=2e-3)\n",
        "    opt_desc = optim.Adam(model_desc.parameters(), lr=0.001, weight_decay=2e-3)\n",
        "    opt_fp = optim.Adam(model_fp.parameters(), lr=0.001, weight_decay=2e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # 4. ËÆ≠ÁªÉÂæ™ÁéØ\n",
        "    epochs = 200\n",
        "    history = {'only_loss': [], 'desc_loss': [], 'fp_loss': [],\n",
        "               'only_r2': [], 'desc_r2': [], 'fp_r2': []}\n",
        "\n",
        "    print(\"\\nüöÄ ÂºÄÂßã‰∏âÊ®°ÂûãÂØπÊØîËÆ≠ÁªÉ...\")\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        loss_o = train_step(model_only, train_loader, opt_only, criterion, 'only')\n",
        "        loss_d = train_step(model_desc, train_loader, opt_desc, criterion, 'desc')\n",
        "        loss_f = train_step(model_fp, train_loader, opt_fp, criterion, 'fp')\n",
        "\n",
        "        # Eval (Test Set Monitoring)\n",
        "        _, r2_o, _, _ = eval_step(model_only, test_loader, 'only')\n",
        "        _, r2_d, _, _ = eval_step(model_desc, test_loader, 'desc')\n",
        "        _, r2_f, _, _ = eval_step(model_fp, test_loader, 'fp')\n",
        "\n",
        "        history['only_loss'].append(loss_o)\n",
        "        history['desc_loss'].append(loss_d)\n",
        "        history['fp_loss'].append(loss_f)\n",
        "\n",
        "        history['only_r2'].append(r2_o)\n",
        "        history['desc_r2'].append(r2_d)\n",
        "        history['fp_r2'].append(r2_f)\n",
        "\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            print(f\"Ep {epoch+1:03d} | [Only] R2:{r2_o:.3f} | [Desc] R2:{r2_d:.3f} | [FP] R2:{r2_f:.3f}\")\n",
        "\n",
        "    # 5. ÊúÄÁªàËØÑ‰º∞‰∏éÂèØËßÜÂåñ\n",
        "    print(\"\\n‚úÖ ËÆ≠ÁªÉÂÆåÊàêÔºåÁîüÊàêÂØπÊØîÂõæË°®...\")\n",
        "    _, final_r2_o, y_true, y_pred_o = eval_step(model_only, test_loader, 'only')\n",
        "    _, final_r2_d, _, y_pred_d = eval_step(model_desc, test_loader, 'desc')\n",
        "    _, final_r2_f, _, y_pred_f = eval_step(model_fp, test_loader, 'fp')\n",
        "\n",
        "    # === ÁªòÂõæ ===\n",
        "    fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Â≠êÂõæ1ÔºöLoss Êõ≤Á∫ø\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    ax1.plot(history['only_loss'], label='GNN Only', color='gray', linestyle='--')\n",
        "    ax1.plot(history['desc_loss'], label='GNN + Descriptors', color='tab:blue')\n",
        "    ax1.plot(history['fp_loss'], label='GNN + Fingerprints', color='tab:red')\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Â≠êÂõæ2ÔºöR2 Êõ≤Á∫ø\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    ax2.plot(history['only_r2'], label='GNN Only', color='gray', linestyle='--')\n",
        "    ax2.plot(history['desc_r2'], label='GNN + Descriptors', color='tab:blue')\n",
        "    ax2.plot(history['fp_r2'], label='GNN + Fingerprints', color='tab:red')\n",
        "    ax2.set_title('Test R2 Score')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Â≠êÂõæ3ÔºöÊúÄÁªàÈ¢ÑÊµãÊï£ÁÇπÂØπÊØî\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    # ÁªòÂà∂ GNN Only (Â∫ïËâ≤)\n",
        "    ax3.scatter(y_true, y_pred_o, alpha=0.3, color='gray', label=f'Only (R2={final_r2_o:.3f})', s=10)\n",
        "    # ÁªòÂà∂ GNN+Desc\n",
        "    ax3.scatter(y_true, y_pred_d, alpha=0.5, color='tab:blue', label=f'+Desc (R2={final_r2_d:.3f})', s=20, marker='^')\n",
        "    # ÁªòÂà∂ GNN+FP\n",
        "    ax3.scatter(y_true, y_pred_f, alpha=0.5, color='tab:red', label=f'+FP (R2={final_r2_f:.3f})', s=20, marker='x')\n",
        "\n",
        "    min_v, max_v = y_true.min(), y_true.max()\n",
        "    ax3.plot([min_v, max_v], [min_v, max_v], 'k--', lw=1.5)\n",
        "    ax3.set_title('Prediction Correlation')\n",
        "    ax3.set_xlabel('True Sweetness')\n",
        "    ax3.set_ylabel('Predicted')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nüèÜ ÊúÄÁªà R2 ÂØπÊØî:\")\n",
        "    print(f\"   - GNN Only: {final_r2_o:.4f}\")\n",
        "    print(f\"   - GNN + Descriptors: {final_r2_d:.4f}\")\n",
        "    print(f\"   - GNN + Fingerprints: {final_r2_f:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "4Vs3R4Iv1GMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SMIELS+ÊèèËø∞Á¨¶/ÊåáÁ∫π**"
      ],
      "metadata": {
        "id": "6DYIj5B83owB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# RDKit & Sklearn\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, AllChem, rdBase\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. Global Configuration\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "rdBase.DisableLog('rdApp.warning')\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using Device: {DEVICE}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Core Components: Tokenizer & Featurizers\n",
        "# ==============================================================================\n",
        "\n",
        "# --- SMILES Tokenizer ---\n",
        "class SMILESTokenizer:\n",
        "    def __init__(self, max_len=128):\n",
        "        self.max_len = max_len\n",
        "        self.pattern =  r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "        self.regex = re.compile(self.pattern)\n",
        "        self.special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "        self.vocab = {t: i for i, t in enumerate(self.special_tokens)}\n",
        "        self.unk_token = \"<unk>\"\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "        self.pad_token = \"<pad>\"\n",
        "\n",
        "    def tokenize(self, smiles):\n",
        "        return [token for token in self.regex.findall(smiles)]\n",
        "\n",
        "    def build_vocab(self, smiles_list):\n",
        "        counter = Counter()\n",
        "        for s in smiles_list:\n",
        "            counter.update(self.tokenize(s))\n",
        "        for token, _ in counter.most_common():\n",
        "            if token not in self.vocab:\n",
        "                self.vocab[token] = len(self.vocab)\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, smiles):\n",
        "        tokens = self.tokenize(smiles)[:self.max_len - 2]\n",
        "        ids = [self.vocab.get(t, self.vocab[self.unk_token]) for t in tokens]\n",
        "        return [self.vocab[self.sos_token]] + ids + [self.vocab[self.eos_token]]\n",
        "\n",
        "# --- Auxiliary Feature Extractors ---\n",
        "def get_descriptors(mol):\n",
        "    # 9 common physicochemical properties\n",
        "    return [\n",
        "        Descriptors.MolWt(mol), Descriptors.MolLogP(mol),\n",
        "        Descriptors.NumHDonors(mol), Descriptors.NumHAcceptors(mol),\n",
        "        Descriptors.TPSA(mol), Descriptors.NumRotatableBonds(mol),\n",
        "        Descriptors.FractionCSP3(mol), Descriptors.HallKierAlpha(mol),\n",
        "        Descriptors.RingCount(mol)\n",
        "    ]\n",
        "\n",
        "def get_fingerprint(mol, n_bits=1024):\n",
        "    # Morgan Fingerprint (ECFP4)\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
        "    return np.array(fp)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Dataset Construction\n",
        "# ==============================================================================\n",
        "class TransFusionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.data = []\n",
        "        print(\"Preprocessing data (SMILES Encoding + Descriptors + Fingerprints)...\")\n",
        "\n",
        "        all_descs = []\n",
        "        temp_data = []\n",
        "\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            mol = Chem.MolFromSmiles(row['Smiles'])\n",
        "            if mol is None: continue\n",
        "\n",
        "            # 1. Sequence (Transformer Input)\n",
        "            seq_ids = tokenizer.encode(row['Smiles'])\n",
        "\n",
        "            # 2. Descriptors (Aux A)\n",
        "            desc = get_descriptors(mol)\n",
        "\n",
        "            # 3. Fingerprints (Aux B)\n",
        "            fp = get_fingerprint(mol)\n",
        "\n",
        "            all_descs.append(desc)\n",
        "            temp_data.append({\n",
        "                'seq': torch.tensor(seq_ids, dtype=torch.long),\n",
        "                'fp': torch.tensor(fp, dtype=torch.float32),\n",
        "                'y': torch.tensor([row['logSw']], dtype=torch.float32)\n",
        "            })\n",
        "\n",
        "        # Standardize descriptors\n",
        "        scaler = StandardScaler()\n",
        "        all_descs_scaled = scaler.fit_transform(all_descs)\n",
        "\n",
        "        for i, item in enumerate(temp_data):\n",
        "            item['desc'] = torch.tensor(all_descs_scaled[i], dtype=torch.float32)\n",
        "            self.data.append(item)\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Pad Sequence for Transformer\n",
        "    pad_id = 0\n",
        "    seqs = pad_sequence([x['seq'] for x in batch], batch_first=True, padding_value=pad_id)\n",
        "\n",
        "    descs = torch.stack([x['desc'] for x in batch])\n",
        "    fps = torch.stack([x['fp'] for x in batch])\n",
        "    labels = torch.cat([x['y'] for x in batch])\n",
        "\n",
        "    return seqs, descs, fps, labels\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Model Architecture (Transformer Encoder + Fusion)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 3.1 Transformer Branch ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, pad_idx=0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            batch_first=True, dropout=dropout\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output_dim = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Seq]\n",
        "        padding_mask = (x == self.pad_idx)\n",
        "\n",
        "        x = self.embedding(x) * math.sqrt(self.output_dim)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        # Mean Pooling (Masked)\n",
        "        mask_expanded = (~padding_mask).unsqueeze(-1).float()\n",
        "        sum_embeddings = torch.sum(x * mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "        feature = sum_embeddings / sum_mask\n",
        "        return feature\n",
        "\n",
        "# --- 3.2 Fusion Architecture ---\n",
        "class TransFusionModel(nn.Module):\n",
        "    def __init__(self, mode, vocab_size, aux_dim=0, pad_idx=0):\n",
        "        \"\"\"\n",
        "        mode: 'only' (SMILES Only), 'desc' (SMILES + Desc), 'fp' (SMILES + FP)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        # 1. Transformer Branch (Fixed Parameters)\n",
        "        self.trans = TransformerEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=128, nhead=4, num_layers=3,\n",
        "            dim_feedforward=256, pad_idx=pad_idx, dropout=0.1\n",
        "        )\n",
        "\n",
        "        trans_out_dim = 128\n",
        "\n",
        "        if mode == 'only':\n",
        "            # Single Modality\n",
        "            self.aux_mlp = None\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(trans_out_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(64, 1)\n",
        "            )\n",
        "        else:\n",
        "            # Dual Modality\n",
        "            # Map auxiliary features to 128 dim\n",
        "            self.aux_mlp = nn.Sequential(\n",
        "                nn.Linear(aux_dim, 128),\n",
        "                nn.BatchNorm1d(128),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2)\n",
        "            )\n",
        "\n",
        "            # Fusion Head (Concat: 128 + 128 -> 256)\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(256, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(64, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, seq, aux_data=None):\n",
        "        # Path A: Transformer\n",
        "        trans_feat = self.trans(seq)\n",
        "\n",
        "        if self.mode == 'only':\n",
        "            return self.head(trans_feat).squeeze()\n",
        "\n",
        "        # Path B: Auxiliary\n",
        "        aux_feat = self.aux_mlp(aux_data)\n",
        "\n",
        "        # Fusion\n",
        "        combined = torch.cat([trans_feat, aux_feat], dim=1)\n",
        "        return self.head(combined).squeeze()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Training Engine\n",
        "# ==============================================================================\n",
        "def train_step(model, loader, optimizer, criterion, mode):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for seqs, descs, fps, labels in loader:\n",
        "        seqs, labels = seqs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        if mode == 'desc': aux = descs.to(DEVICE)\n",
        "        elif mode == 'fp': aux = fps.to(DEVICE)\n",
        "        else: aux = None # 'only' mode\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(seqs, aux)\n",
        "        loss = criterion(preds, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_step(model, loader, mode):\n",
        "    model.eval()\n",
        "    preds_all, labels_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for seqs, descs, fps, labels in loader:\n",
        "            seqs = seqs.to(DEVICE)\n",
        "\n",
        "            if mode == 'desc': aux = descs.to(DEVICE)\n",
        "            elif mode == 'fp': aux = fps.to(DEVICE)\n",
        "            else: aux = None\n",
        "\n",
        "            p = model(seqs, aux)\n",
        "            preds_all.extend(p.cpu().numpy())\n",
        "            labels_all.extend(labels.numpy())\n",
        "\n",
        "    r2 = r2_score(labels_all, preds_all)\n",
        "    mse = mean_squared_error(labels_all, preds_all)\n",
        "    return mse, r2, np.array(labels_all), np.array(preds_all)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Main Execution\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. Data Prep\n",
        "    csv_path = '/content/SweetpredDB.csv'\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(\"Generating dummy data for demo...\")\n",
        "        df = pd.DataFrame({'Smiles': ['C']*20 + ['CCO']*20, 'logSw': np.random.rand(40)})\n",
        "    else:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.rename(columns=str.lower, inplace=True)\n",
        "        df.rename(columns={'smiles': 'Smiles', 'logsw': 'logSw'}, inplace=True)\n",
        "\n",
        "    tokenizer = SMILESTokenizer()\n",
        "    tokenizer.build_vocab(df['Smiles'].tolist())\n",
        "\n",
        "    dataset = TransFusionDataset(df, tokenizer)\n",
        "\n",
        "    # Split\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_ds, test_ds = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_ds, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "    # Dimensions\n",
        "    sample = dataset[0]\n",
        "    desc_dim = sample['desc'].shape[0] # 9\n",
        "    fp_dim = sample['fp'].shape[0]     # 1024\n",
        "    vocab_size = len(tokenizer.vocab)\n",
        "    pad_id = tokenizer.vocab[\"<pad>\"]\n",
        "\n",
        "    print(f\"\\nModel Config: Vocab={vocab_size}, Desc_Dim={desc_dim}, FP_Dim={fp_dim}\")\n",
        "\n",
        "    # 2. Initialize Models\n",
        "    # Model A: SMILES Only\n",
        "    model_only = TransFusionModel('only', vocab_size, 0, pad_id).to(DEVICE)\n",
        "    # Model B: Transformer + Descriptors\n",
        "    model_desc = TransFusionModel('desc', vocab_size, desc_dim, pad_id).to(DEVICE)\n",
        "    # Model C: Transformer + Fingerprints\n",
        "    model_fp = TransFusionModel('fp', vocab_size, fp_dim, pad_id).to(DEVICE)\n",
        "\n",
        "    # 3. Optimizers\n",
        "    opt_only = optim.Adam(model_only.parameters(), lr=0.0005)\n",
        "    opt_desc = optim.Adam(model_desc.parameters(), lr=0.0005)\n",
        "    opt_fp = optim.Adam(model_fp.parameters(), lr=0.0005)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # 4. Training Loop\n",
        "    epochs = 50\n",
        "    history = {'only_loss': [], 'desc_loss': [], 'fp_loss': [],\n",
        "               'only_r2': [], 'desc_r2': [], 'fp_r2': []}\n",
        "\n",
        "    print(\"\\nüöÄ Starting Comparative Training (Transformer)...\")\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        l_only = train_step(model_only, train_loader, opt_only, criterion, 'only')\n",
        "        l_desc = train_step(model_desc, train_loader, opt_desc, criterion, 'desc')\n",
        "        l_fp = train_step(model_fp, train_loader, opt_fp, criterion, 'fp')\n",
        "\n",
        "        # Eval\n",
        "        _, r2_only, _, _ = eval_step(model_only, test_loader, 'only')\n",
        "        _, r2_desc, _, _ = eval_step(model_desc, test_loader, 'desc')\n",
        "        _, r2_fp, _, _ = eval_step(model_fp, test_loader, 'fp')\n",
        "\n",
        "        history['only_loss'].append(l_only)\n",
        "        history['desc_loss'].append(l_desc)\n",
        "        history['fp_loss'].append(l_fp)\n",
        "\n",
        "        history['only_r2'].append(r2_only)\n",
        "        history['desc_r2'].append(r2_desc)\n",
        "        history['fp_r2'].append(r2_fp)\n",
        "\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"Ep {epoch+1:02d} | [Only] R2:{r2_only:.4f} | [Desc] R2:{r2_desc:.4f} | [FP] R2:{r2_fp:.4f}\")\n",
        "\n",
        "    # 5. Visualization\n",
        "    print(\"\\nüìä Generating Comparison Charts...\")\n",
        "    _, final_r2_only, y_true, y_pred_only = eval_step(model_only, test_loader, 'only')\n",
        "    _, final_r2_desc, _, y_pred_desc = eval_step(model_desc, test_loader, 'desc')\n",
        "    _, final_r2_fp, _, y_pred_fp = eval_step(model_fp, test_loader, 'fp')\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Plot 1: Loss\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    ax1.plot(history['only_loss'], label='SMILES Only', color='gray', linestyle='--')\n",
        "    ax1.plot(history['desc_loss'], label='SMILES + Desc', color='tab:blue')\n",
        "    ax1.plot(history['fp_loss'], label='SMILES + FP', color='tab:orange')\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: R2 Score\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    ax2.plot(history['only_r2'], label='SMILES Only', color='gray', linestyle='--')\n",
        "    ax2.plot(history['desc_r2'], label='SMILES + Desc', color='tab:blue')\n",
        "    ax2.plot(history['fp_r2'], label='SMILES + FP', color='tab:orange')\n",
        "    ax2.set_title('Test R2 Score')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Scatter Plot\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    ax3.scatter(y_true, y_pred_only, alpha=0.3, s=15, label=f'Only (R2={final_r2_only:.3f})', color='gray')\n",
        "    ax3.scatter(y_true, y_pred_desc, alpha=0.5, s=20, label=f'+Desc (R2={final_r2_desc:.3f})', color='tab:blue')\n",
        "    ax3.scatter(y_true, y_pred_fp, alpha=0.5, s=20, label=f'+FP (R2={final_r2_fp:.3f})', color='tab:orange', marker='x')\n",
        "\n",
        "    min_v, max_v = y_true.min(), y_true.max()\n",
        "    ax3.plot([min_v, max_v], [min_v, max_v], 'k--', lw=1)\n",
        "    ax3.set_title('Final Prediction Correlation')\n",
        "    ax3.set_xlabel('True Sweetness')\n",
        "    ax3.set_ylabel('Predicted')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nüèÜ Final Comparison:\")\n",
        "    print(f\"   Model A (SMILES Only):        R2 = {final_r2_only:.4f}\")\n",
        "    print(f\"   Model B (SMILES + Desc):      R2 = {final_r2_desc:.4f}\")\n",
        "    print(f\"   Model C (SMILES + Fingerprints): R2 = {final_r2_fp:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ddxZTzYz3d0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SMIELS+Graph**"
      ],
      "metadata": {
        "id": "KIFctOM-3rYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy.sparse import coo_matrix\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# RDKit & Sklearn\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdchem, rdBase\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# PyG imports\n",
        "try:\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    from torch_geometric.nn import GATConv, global_mean_pool\n",
        "except ImportError:\n",
        "    print(\"Installing PyG dependencies...\")\n",
        "    os.system(\"pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\")\n",
        "    os.system(\"pip install -q torch-geometric\")\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    from torch_geometric.nn import GATConv, global_mean_pool\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. Global Configuration\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "rdBase.DisableLog('rdApp.warning')\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using Device: {DEVICE}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Data Processing: Tokenizer & Featurizer\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1.1 SMILES Tokenizer (for Transformer) ---\n",
        "class SMILESTokenizer:\n",
        "    def __init__(self, max_len=128):\n",
        "        self.max_len = max_len\n",
        "        self.pattern =  r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "        self.regex = re.compile(self.pattern)\n",
        "        self.special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "        self.vocab = {t: i for i, t in enumerate(self.special_tokens)}\n",
        "        self.unk_token = \"<unk>\"\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "        self.pad_token = \"<pad>\"\n",
        "\n",
        "    def tokenize(self, smiles):\n",
        "        return [token for token in self.regex.findall(smiles)]\n",
        "\n",
        "    def build_vocab(self, smiles_list):\n",
        "        counter = Counter()\n",
        "        for s in smiles_list:\n",
        "            counter.update(self.tokenize(s))\n",
        "        for token, _ in counter.most_common():\n",
        "            if token not in self.vocab:\n",
        "                self.vocab[token] = len(self.vocab)\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, smiles):\n",
        "        tokens = self.tokenize(smiles)[:self.max_len - 2]\n",
        "        ids = [self.vocab.get(t, self.vocab[self.unk_token]) for t in tokens]\n",
        "        return [self.vocab[self.sos_token]] + ids + [self.vocab[self.eos_token]]\n",
        "\n",
        "# --- 1.2 Graph Featurizer (for GNN) ---\n",
        "def one_hot_encoding(value, choices):\n",
        "    encoding = [0] * (len(choices) + 1)\n",
        "    index = choices.index(value) if value in choices else -1\n",
        "    encoding[index] = 1\n",
        "    return encoding\n",
        "\n",
        "class MoleculeFeaturizer(object):\n",
        "    def _atom_featurizer(self, atom):\n",
        "        # 37 atom types + 1 other\n",
        "        atomic_numer = [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 46, 47, 48, 49, 50, 51, 52, 53]\n",
        "        return (one_hot_encoding(atom.GetAtomicNum(), atomic_numer) +\n",
        "                one_hot_encoding(atom.GetTotalDegree(), list(range(5))) +\n",
        "                one_hot_encoding(int(atom.GetHybridization()), list(range(len(Chem.HybridizationType.names)-1))) +\n",
        "                one_hot_encoding(atom.GetChiralTag(), list(range(len(Chem.ChiralType.names)-1))) +\n",
        "                one_hot_encoding(atom.GetTotalNumHs(), list(range(5))) +\n",
        "                [1 if atom.GetIsAromatic() else 0])\n",
        "\n",
        "    def _bond_featurizer(self, bond):\n",
        "        bt = [\n",
        "            int(bond.GetBondType() == rdchem.BondType.SINGLE),\n",
        "            int(bond.GetBondType() == rdchem.BondType.DOUBLE),\n",
        "            int(bond.GetBondType() == rdchem.BondType.TRIPLE),\n",
        "            int(bond.GetBondType() == rdchem.BondType.AROMATIC)\n",
        "        ]\n",
        "        return bt + [int(bond.GetIsConjugated()), int(bond.IsInRing())]\n",
        "\n",
        "    def __call__(self, mol):\n",
        "        atom_features = [self._atom_featurizer(atom) for atom in mol.GetAtoms()]\n",
        "        x = torch.tensor(atom_features, dtype=torch.float32)\n",
        "\n",
        "        adj = Chem.GetAdjacencyMatrix(mol)\n",
        "        coo_adj = coo_matrix(adj)\n",
        "        row, col = coo_adj.row, coo_adj.col\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "\n",
        "        bond_features = []\n",
        "        for i, j in zip(row, col):\n",
        "            bond = mol.GetBondBetweenAtoms(int(i), int(j))\n",
        "            bond_features.append(self._bond_featurizer(bond))\n",
        "\n",
        "        edge_attr = torch.tensor(bond_features, dtype=torch.float32) if len(bond_features) > 0 else torch.empty((0, 6), dtype=torch.float32)\n",
        "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Dataset Construction\n",
        "# ==============================================================================\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, featurizer):\n",
        "        self.data = []\n",
        "        print(\"Processing data (Graph + Sequence)...\")\n",
        "\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            mol = Chem.MolFromSmiles(row['Smiles'])\n",
        "            if mol is None: continue\n",
        "\n",
        "            # 1. Graph Data\n",
        "            graph_data = featurizer(mol)\n",
        "\n",
        "            # 2. Sequence Data\n",
        "            seq_ids = tokenizer.encode(row['Smiles'])\n",
        "\n",
        "            self.data.append({\n",
        "                'graph': graph_data,\n",
        "                'seq': torch.tensor(seq_ids, dtype=torch.long),\n",
        "                'y': torch.tensor([row['logSw']], dtype=torch.float32)\n",
        "            })\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    graphs = Batch.from_data_list([item['graph'] for item in batch])\n",
        "\n",
        "    # Pad Sequence\n",
        "    pad_id = 0 # Assuming <pad> is at index 0\n",
        "    seqs = pad_sequence([item['seq'] for item in batch], batch_first=True, padding_value=pad_id)\n",
        "\n",
        "    labels = torch.cat([item['y'] for item in batch])\n",
        "    return graphs, seqs, labels\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Model Architectures\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 3.1 GNN Encoder (Strictly matches Strategy B parameters) ---\n",
        "class GNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=69, hidden_dim=16, heads=2, edge_dim=6, dropout=0.4):\n",
        "        super(GNNEncoder, self).__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout, edge_dim=edge_dim)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout, edge_dim=edge_dim)\n",
        "        self.pool = global_mean_pool\n",
        "        self.output_dim = hidden_dim # 16\n",
        "\n",
        "    def forward(self, x, edge_index, batch, edge_attr):\n",
        "        x = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
        "        x = F.elu(x)\n",
        "        x = self.gat2(x, edge_index, edge_attr=edge_attr)\n",
        "        x = F.elu(x)\n",
        "        x = self.pool(x, batch)\n",
        "        return x\n",
        "\n",
        "# --- 3.2 Transformer Encoder (Strictly matches Encoder-only code) ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerEncoderBranch(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, pad_idx=0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            batch_first=True, dropout=dropout\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output_dim = d_model # 128\n",
        "\n",
        "    def forward(self, x):\n",
        "        padding_mask = (x == self.pad_idx)\n",
        "        x = self.embedding(x) * math.sqrt(self.output_dim)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        # Masked Mean Pooling\n",
        "        mask_expanded = (~padding_mask).unsqueeze(-1).float()\n",
        "        sum_embeddings = torch.sum(x * mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "        feature = sum_embeddings / sum_mask\n",
        "        return feature\n",
        "\n",
        "# --- 3.3 Universal Model Wrapper (Single / Dual) ---\n",
        "class SweetNetModel(nn.Module):\n",
        "    def __init__(self, mode, gnn_cfg, trans_cfg):\n",
        "        \"\"\"\n",
        "        mode: 'graph', 'seq', 'fusion'\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        # Initialize Branches\n",
        "        self.gnn = GNNEncoder(**gnn_cfg) if mode in ['graph', 'fusion'] else None\n",
        "        self.trans = TransformerEncoderBranch(**trans_cfg) if mode in ['seq', 'fusion'] else None\n",
        "\n",
        "        # Determine Fusion Dimension\n",
        "        fusion_dim = 0\n",
        "        if self.gnn: fusion_dim += self.gnn.output_dim # 16\n",
        "        if self.trans: fusion_dim += self.trans.output_dim # 128\n",
        "\n",
        "        # Prediction Head (MLP) - Consistent with previous code\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3), # Combined dropout\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Init weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, graph_batch, seq_batch):\n",
        "        feats = []\n",
        "\n",
        "        if self.gnn:\n",
        "            g_emb = self.gnn(graph_batch.x, graph_batch.edge_index, graph_batch.batch, graph_batch.edge_attr)\n",
        "            feats.append(g_emb)\n",
        "\n",
        "        if self.trans:\n",
        "            s_emb = self.trans(seq_batch)\n",
        "            feats.append(s_emb)\n",
        "\n",
        "        # Concatenate Features\n",
        "        combined = torch.cat(feats, dim=1)\n",
        "        return self.mlp(combined).squeeze()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Training Engine (Standardized)\n",
        "# ==============================================================================\n",
        "def train_step(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for graphs, seqs, labels in loader:\n",
        "        graphs, seqs, labels = graphs.to(DEVICE), seqs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(graphs, seqs)\n",
        "        loss = criterion(preds, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_step(model, loader):\n",
        "    model.eval()\n",
        "    preds_all, labels_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for graphs, seqs, labels in loader:\n",
        "            graphs, seqs, labels = graphs.to(DEVICE), seqs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            p = model(graphs, seqs)\n",
        "            preds_all.extend(p.cpu().numpy())\n",
        "            labels_all.extend(labels.cpu().numpy())\n",
        "\n",
        "    mse = mean_squared_error(labels_all, preds_all)\n",
        "    r2 = r2_score(labels_all, preds_all)\n",
        "    return mse, r2, np.array(labels_all), np.array(preds_all)\n",
        "\n",
        "def run_experiment(model_name, mode, gnn_cfg, trans_cfg, train_loader, val_loader, test_loader, epochs=200):\n",
        "    print(f\"\\nüöÄ Training Model: [{model_name}] (Mode: {mode})\")\n",
        "\n",
        "    # Init Model\n",
        "    model = SweetNetModel(mode, gnn_cfg, trans_cfg).to(DEVICE)\n",
        "\n",
        "    # Optimizer (Use weight_decay=2e-3 for GNN consistency, adjusted slightly for Transformer stability)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4) # Balanced decay\n",
        "    criterion = nn.MSELoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
        "\n",
        "    # Training Loop\n",
        "    best_val_r2 = -float('inf')\n",
        "    best_wts = copy.deepcopy(model.state_dict())\n",
        "    history = {'loss': [], 'val_r2': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_step(model, train_loader, optimizer, criterion)\n",
        "        val_mse, val_r2, _, _ = eval_step(model, val_loader)\n",
        "\n",
        "        scheduler.step(val_mse)\n",
        "        history['loss'].append(loss)\n",
        "        history['val_r2'].append(val_r2)\n",
        "\n",
        "        if val_r2 > best_val_r2:\n",
        "            best_val_r2 = val_r2\n",
        "            best_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch+1) % 20 == 0:\n",
        "            print(f\"   Ep {epoch+1:03d} | Loss: {loss:.4f} | Val R2: {val_r2:.4f}\")\n",
        "\n",
        "    # Final Test Evaluation\n",
        "    model.load_state_dict(best_wts)\n",
        "    _, test_r2, y_true, y_pred = eval_step(model, test_loader)\n",
        "    print(f\"   üèÜ Final Test R2: {test_r2:.4f}\")\n",
        "\n",
        "    return test_r2, history, y_true, y_pred\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Main Execution\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. Data Prep\n",
        "    csv_path = '/content/SweetpredDB.csv'\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(\"Generating dummy data for demo...\")\n",
        "        df = pd.DataFrame({'Smiles': ['C']*50 + ['CC']*50, 'logSw': np.random.rand(100)})\n",
        "    else:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.rename(columns=str.lower, inplace=True)\n",
        "        df.rename(columns={'smiles': 'Smiles', 'logsw': 'logSw'}, inplace=True)\n",
        "\n",
        "    tokenizer = SMILESTokenizer()\n",
        "    tokenizer.build_vocab(df['Smiles'].tolist())\n",
        "    featurizer = MoleculeFeaturizer()\n",
        "\n",
        "    dataset = MultimodalDataset(df, tokenizer, featurizer)\n",
        "\n",
        "    # 8:1:1 Split\n",
        "    train_len = int(0.8 * len(dataset))\n",
        "    val_len = int(0.1 * len(dataset))\n",
        "    test_len = len(dataset) - train_len - val_len\n",
        "    train_ds, val_ds, test_ds = random_split(dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Model Configs\n",
        "    gnn_config = {\n",
        "        'input_dim': 69, 'hidden_dim': 16, 'heads': 2, 'edge_dim': 6, 'dropout': 0.4\n",
        "    }\n",
        "    trans_config = {\n",
        "        'vocab_size': len(tokenizer.vocab), 'd_model': 128, 'nhead': 4,\n",
        "        'num_layers': 3, 'dim_feedforward': 256, 'pad_idx': tokenizer.vocab[\"<pad>\"], 'dropout': 0.1\n",
        "    }\n",
        "\n",
        "    # 2. Run Comparisons\n",
        "    results = {}\n",
        "\n",
        "    # Experiment A: Graph Only\n",
        "    r2_g, hist_g, y_g, pred_g = run_experiment(\"Graph Only\", 'graph', gnn_config, trans_config, train_loader, val_loader, test_loader)\n",
        "\n",
        "    # Experiment B: SMILES Only\n",
        "    r2_s, hist_s, y_s, pred_s = run_experiment(\"SMILES Only\", 'seq', gnn_config, trans_config, train_loader, val_loader, test_loader)\n",
        "\n",
        "    # Experiment C: Graph + SMILES Fusion\n",
        "    r2_f, hist_f, y_f, pred_f = run_experiment(\"Graph + SMILES\", 'fusion', gnn_config, trans_config, train_loader, val_loader, test_loader)\n",
        "\n",
        "    # 3. Visualization\n",
        "    fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Plot 1: Validation R2 Curves\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    ax1.plot(hist_g['val_r2'], label='Graph Only', color='tab:blue')\n",
        "    ax1.plot(hist_s['val_r2'], label='SMILES Only', color='tab:orange')\n",
        "    ax1.plot(hist_f['val_r2'], label='Fusion (Graph+SMILES)', color='tab:green', linewidth=2)\n",
        "    ax1.set_title('Validation R2 History')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('R2 Score')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Bar Chart\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    labels = ['Graph Only', 'SMILES Only', 'Fusion']\n",
        "    values = [r2_g, r2_s, r2_f]\n",
        "    colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "    bars = ax2.bar(labels, values, color=colors)\n",
        "    ax2.set_title('Final Test R2 Score Comparison')\n",
        "    ax2.set_ylim(0, 1.0)\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "    for bar in bars:\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f\"{bar.get_height():.3f}\", ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 3: Scatter Plot (Fusion)\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    ax3.scatter(y_f, pred_f, alpha=0.5, color='tab:green', s=20, label=f'Fusion (R2={r2_f:.3f})')\n",
        "    # Add light references for others\n",
        "    # ax3.scatter(y_g, pred_g, alpha=0.1, color='tab:blue', s=10)\n",
        "\n",
        "    min_v, max_v = y_f.min(), y_f.max()\n",
        "    ax3.plot([min_v, max_v], [min_v, max_v], 'k--', lw=1)\n",
        "    ax3.set_title('Fusion Model Predictions')\n",
        "    ax3.set_xlabel('True Sweetness')\n",
        "    ax3.set_ylabel('Predicted')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üèÜ SUMMARY REPORT\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Graph Only R2:      {r2_g:.4f}\")\n",
        "    print(f\"SMILES Only R2:     {r2_s:.4f}\")\n",
        "    print(f\"Fusion Model R2:    {r2_f:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "UIVSxJdssUaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Á¨¨2ÈÉ®ÂàÜÔºöÂÜçËØïËØïÂè¶‰∏ÄÁßç‰∫åÊ®°ÊÄÅÔºöGraph/SMILES+È¢ÑËÆ≠ÁªÉÁâπÂæÅ**"
      ],
      "metadata": {
        "id": "sBjRmCfO7jNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# RDKit & Sklearn\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdchem, rdBase, AllChem, Descriptors\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# PyG imports\n",
        "try:\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    from torch_geometric.nn import GATConv, global_mean_pool\n",
        "except ImportError:\n",
        "    print(\"Installing PyG dependencies...\")\n",
        "    os.system(\"pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\")\n",
        "    os.system(\"pip install -q torch-geometric\")\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    from torch_geometric.nn import GATConv, global_mean_pool\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "rdBase.DisableLog('rdApp.warning')\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚úÖ Using Device: {DEVICE}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Data Processing Components\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1.1 Tokenizer ---\n",
        "class SMILESTokenizer:\n",
        "    def __init__(self, max_len=128):\n",
        "        self.max_len = max_len\n",
        "        self.pattern =  r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "        self.regex = re.compile(self.pattern)\n",
        "        self.special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "        self.vocab = {t: i for i, t in enumerate(self.special_tokens)}\n",
        "\n",
        "    def tokenize(self, smiles):\n",
        "        return [token for token in self.regex.findall(smiles)]\n",
        "\n",
        "    def build_vocab(self, smiles_list):\n",
        "        counter = Counter()\n",
        "        for s in smiles_list:\n",
        "            counter.update(self.tokenize(s))\n",
        "        for token, _ in counter.most_common():\n",
        "            if token not in self.vocab:\n",
        "                self.vocab[token] = len(self.vocab)\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, smiles):\n",
        "        tokens = self.tokenize(smiles)[:self.max_len - 2]\n",
        "        ids = [self.vocab.get(t, self.vocab[\"<unk>\"]) for t in tokens]\n",
        "        return [self.vocab[\"<sos>\"]] + ids + [self.vocab[\"<eos>\"]]\n",
        "\n",
        "# --- 1.2 Graph Featurizer ---\n",
        "def one_hot_encoding(value, choices):\n",
        "    encoding = [0] * (len(choices) + 1)\n",
        "    index = choices.index(value) if value in choices else -1\n",
        "    encoding[index] = 1\n",
        "    return encoding\n",
        "\n",
        "class MoleculeFeaturizer:\n",
        "    def _atom_featurizer(self, atom):\n",
        "        atomic_numer = [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 46, 47, 48, 49, 50, 51, 52, 53]\n",
        "        return (one_hot_encoding(atom.GetAtomicNum(), atomic_numer) +\n",
        "                one_hot_encoding(atom.GetTotalDegree(), list(range(5))) +\n",
        "                one_hot_encoding(int(atom.GetHybridization()), list(range(len(Chem.HybridizationType.names)-1))) +\n",
        "                one_hot_encoding(atom.GetChiralTag(), list(range(len(Chem.ChiralType.names)-1))) +\n",
        "                one_hot_encoding(atom.GetTotalNumHs(), list(range(5))) +\n",
        "                [1 if atom.GetIsAromatic() else 0])\n",
        "\n",
        "    def _bond_featurizer(self, bond):\n",
        "        bt = [int(bond.GetBondType() == t) for t in [rdchem.BondType.SINGLE, rdchem.BondType.DOUBLE, rdchem.BondType.TRIPLE, rdchem.BondType.AROMATIC]]\n",
        "        return bt + [int(bond.GetIsConjugated()), int(bond.IsInRing())]\n",
        "\n",
        "    def __call__(self, mol):\n",
        "        atom_features = [self._atom_featurizer(atom) for atom in mol.GetAtoms()]\n",
        "        x = torch.tensor(atom_features, dtype=torch.float32)\n",
        "\n",
        "        adj = Chem.GetAdjacencyMatrix(mol)\n",
        "        coo_adj = coo_matrix(adj)\n",
        "        row, col = coo_adj.row, coo_adj.col\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "\n",
        "        bond_features = []\n",
        "        for i, j in zip(row, col):\n",
        "            bond = mol.GetBondBetweenAtoms(int(i), int(j))\n",
        "            bond_features.append(self._bond_featurizer(bond))\n",
        "\n",
        "        edge_attr = torch.tensor(bond_features, dtype=torch.float32) if bond_features else torch.empty((0, 6), dtype=torch.float32)\n",
        "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "# --- 1.3 Pretrained Featurizer ---\n",
        "class PretrainFeaturizer:\n",
        "    def __init__(self):\n",
        "        model_name = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
        "        print(f\"ü§ñ Loading ChemBERTa ({model_name})...\")\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModel.from_pretrained(model_name).to(DEVICE).eval()\n",
        "            self.active = True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load ChemBERTa: {e}\")\n",
        "            self.active = False\n",
        "\n",
        "    def __call__(self, smiles):\n",
        "        if not self.active: return torch.randn(768)\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "            outputs = self.model(**inputs)\n",
        "            return outputs.last_hidden_state[0, 0, :].cpu() # CLS token\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Dataset & Dataloader\n",
        "# ==============================================================================\n",
        "class HybridDataset(Dataset):\n",
        "    def __init__(self, df, smiles_tokenizer, save_path=\"hybrid_data.pt\"):\n",
        "        self.data = []\n",
        "\n",
        "        if os.path.exists(save_path):\n",
        "            print(f\"üîÑ Loading cached data from {save_path}...\")\n",
        "            self.data = torch.load(save_path)\n",
        "        else:\n",
        "            print(\"‚ö° Processing data (Graph + Sequence + Pretrained)...\")\n",
        "            graph_featurizer = MoleculeFeaturizer()\n",
        "            pretrain_featurizer = PretrainFeaturizer()\n",
        "\n",
        "            for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "                mol = Chem.MolFromSmiles(row['Smiles'])\n",
        "                if mol is None: continue\n",
        "\n",
        "                self.data.append({\n",
        "                    'graph': graph_featurizer(mol),\n",
        "                    'seq': torch.tensor(smiles_tokenizer.encode(row['Smiles']), dtype=torch.long),\n",
        "                    'pretrain': pretrain_featurizer(row['Smiles']),\n",
        "                    'y': torch.tensor([row['logSw']], dtype=torch.float32)\n",
        "                })\n",
        "            torch.save(self.data, save_path)\n",
        "            print(\"‚úÖ Data processed and saved.\")\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    graphs = Batch.from_data_list([x['graph'] for x in batch])\n",
        "    seqs = pad_sequence([x['seq'] for x in batch], batch_first=True, padding_value=0)\n",
        "    pretrains = torch.stack([x['pretrain'] for x in batch])\n",
        "    labels = torch.cat([x['y'] for x in batch])\n",
        "    return graphs, seqs, pretrains, labels\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Model Architectures\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Tower 1: Graph (GAT) ---\n",
        "class GraphTower(nn.Module):\n",
        "    def __init__(self, input_dim=69, hidden_dim=16, heads=2, edge_dim=6, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout, edge_dim=edge_dim)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout, edge_dim=edge_dim)\n",
        "        self.pool = global_mean_pool\n",
        "        self.out_dim = hidden_dim\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "        x = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
        "        x = F.elu(x)\n",
        "        x = self.gat2(x, edge_index, edge_attr=edge_attr)\n",
        "        x = F.elu(x)\n",
        "        return self.pool(x, batch)\n",
        "\n",
        "# --- Tower 2: Sequence (Transformer) ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class SequenceTower(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, 256, batch_first=True, dropout=0.1)\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\n",
        "        self.out_dim = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = (x == self.pad_idx)\n",
        "        x = self.embedding(x) * math.sqrt(self.out_dim)\n",
        "        x = self.pos(x)\n",
        "        x = self.transformer(x, src_key_padding_mask=mask)\n",
        "        # Mean pooling ignoring padding\n",
        "        mask_expanded = (~mask).unsqueeze(-1).float()\n",
        "        return (x * mask_expanded).sum(1) / mask_expanded.sum(1).clamp(min=1e-9)\n",
        "\n",
        "# --- Tower 3: Pretrained (MLP Projection) ---\n",
        "class PretrainTower(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.out_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --- Fusion Model ---\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, mode, vocab_size, pad_idx):\n",
        "        \"\"\"\n",
        "        mode: 'graph', 'seq', 'graph+pre', 'seq+pre'\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        # Initialize Towers based on mode\n",
        "        self.graph = GraphTower() if 'graph' in mode else None\n",
        "        self.seq = SequenceTower(vocab_size, pad_idx=pad_idx) if 'seq' in mode else None\n",
        "        self.pre = PretrainTower() if 'pre' in mode else None\n",
        "\n",
        "        # Calculate fusion dimension\n",
        "        fusion_dim = 0\n",
        "        if self.graph: fusion_dim += self.graph.out_dim # 16\n",
        "        if self.seq: fusion_dim += self.seq.out_dim # 128\n",
        "        if self.pre: fusion_dim += self.pre.out_dim # 128\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, graph_data, seq_data, pre_data):\n",
        "        feats = []\n",
        "        if self.graph: feats.append(self.graph(graph_data))\n",
        "        if self.seq: feats.append(self.seq(seq_data))\n",
        "        if self.pre: feats.append(self.pre(pre_data))\n",
        "\n",
        "        combined = torch.cat(feats, dim=1)\n",
        "        return self.head(combined).squeeze()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Training Engine\n",
        "# ==============================================================================\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for graphs, seqs, pres, labels in loader:\n",
        "        graphs = graphs.to(DEVICE)\n",
        "        seqs = seqs.to(DEVICE)\n",
        "        pres = pres.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(graphs, seqs, pres)\n",
        "        loss = criterion(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for graphs, seqs, pres, labels in loader:\n",
        "            graphs = graphs.to(DEVICE)\n",
        "            seqs = seqs.to(DEVICE)\n",
        "            pres = pres.to(DEVICE)\n",
        "\n",
        "            p = model(graphs, seqs, pres)\n",
        "            preds.extend(p.cpu().numpy())\n",
        "            targets.extend(labels.numpy())\n",
        "    return r2_score(targets, preds), np.array(targets), np.array(preds)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Main Comparison Loop\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. Load Data\n",
        "    csv_path = 'SweetpredDB.csv'\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(\"Creating dummy data...\")\n",
        "        df = pd.DataFrame({'Smiles': ['C']*20 + ['CC']*20, 'logSw': np.random.rand(40)})\n",
        "    else:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.rename(columns=str.lower, inplace=True)\n",
        "        df.rename(columns={'smiles': 'Smiles', 'logsw': 'logSw'}, inplace=True)\n",
        "\n",
        "    tokenizer = SMILESTokenizer()\n",
        "    tokenizer.build_vocab(df['Smiles'].tolist())\n",
        "\n",
        "    # 2. Prepare Dataset\n",
        "    dataset = HybridDataset(df, tokenizer)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_ds, test_ds = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_ds, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "    vocab_size = len(tokenizer.vocab)\n",
        "    pad_id = tokenizer.vocab[\"<pad>\"]\n",
        "\n",
        "    # 3. Define Configurations\n",
        "    configs = {\n",
        "        'Graph Only': 'graph',\n",
        "        'SMILES Only': 'seq',\n",
        "        'Graph + Pretrain': 'graph+pre',\n",
        "        'SMILES + Pretrain': 'seq+pre'\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    histories = {}\n",
        "\n",
        "    # 4. Train All Models\n",
        "    print(f\"\\nüöÄ Starting Comparison of {len(configs)} Models...\")\n",
        "\n",
        "    for name, mode in configs.items():\n",
        "        print(f\"\\nTraining [{name}]...\")\n",
        "        model = FusionModel(mode, vocab_size, pad_id).to(DEVICE)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        loss_hist = []\n",
        "        for epoch in range(50):\n",
        "            loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "            loss_hist.append(loss)\n",
        "            if (epoch+1) % 10 == 0:\n",
        "                r2, _, _ = evaluate(model, test_loader)\n",
        "                print(f\"  Ep {epoch+1:02d} | Loss: {loss:.4f} | Test R2: {r2:.4f}\")\n",
        "\n",
        "        final_r2, y_true, y_pred = evaluate(model, test_loader)\n",
        "        results[name] = final_r2\n",
        "        histories[name] = loss_hist\n",
        "\n",
        "    # 5. Visualization\n",
        "    print(\"\\nüèÜ Final Results:\")\n",
        "    for k, v in results.items():\n",
        "        print(f\"{k:<20}: R2 = {v:.4f}\")\n",
        "\n",
        "    fig = plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Loss Curves\n",
        "    ax1 = plt.subplot(1, 2, 1)\n",
        "    for name, hist in histories.items():\n",
        "        ax1.plot(hist, label=name)\n",
        "    ax1.set_title(\"Training Loss Curves\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"MSE Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Performance Bar Chart\n",
        "    ax2 = plt.subplot(1, 2, 2)\n",
        "    colors = ['gray', 'gray', '#1f77b4', '#ff7f0e'] # Highlight multimodal\n",
        "    bars = ax2.bar(results.keys(), results.values(), color=colors)\n",
        "    ax2.set_title(\"Test Set R2 Score Comparison\")\n",
        "    ax2.set_ylim(0, 1.0)\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, yval, f\"{yval:.3f}\", va='bottom', ha='center', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "AA9Qfm-Z7qTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ÁúãÁúãÈ¢ÑËÆ≠ÁªÉÁâπÂæÅÈïøÂï•Ê†∑**"
      ],
      "metadata": {
        "id": "25eABE-cFEuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "\n",
        "# ==========================================\n",
        "# 1. ÂáÜÂ§áÊï∞ÊçÆ\n",
        "# ==========================================\n",
        "# Èöè‰æøÊâæ 3 ‰∏™ÂàÜÂ≠êÔºöÁî≤ÁÉ∑„ÄÅ‰πôÈÖ∏„ÄÅËãØ\n",
        "smiles_samples = [\n",
        "    \"C\",              # ÂæàÁü≠\n",
        "    \"CC(=O)O\",        # ‰∏≠Á≠â\n",
        "    \"c1ccccc1\"        # ËãØÁéØ\n",
        "]\n",
        "\n",
        "print(f\"üß™ ÂéüÂßãËæìÂÖ•Êï∞ÊçÆ (List of SMILES): {smiles_samples}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# 2. Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã (ChemBERTa)\n",
        "# ==========================================\n",
        "# ËøôÊòØ‰∏Ä‰∏™Âú®Êï∞Áôæ‰∏áÂåñÂ≠¶ÂàÜÂ≠ê‰∏äÈ¢ÑËÆ≠ÁªÉËøáÁöÑÊ®°Âûã\n",
        "model_name = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
        "print(f\"ü§ñ Ê≠£Âú®Âä†ËΩΩÊ®°Âûã: {model_name} ...\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    print(\"‚úÖ Âä†ËΩΩÊàêÂäüÔºÅ\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Âä†ËΩΩÂ§±Ë¥• (ÂèØËÉΩÁΩëÁªúÈóÆÈ¢ò): {e}\")\n",
        "    # ‰∏∫‰∫ÜÊºîÁ§∫ÔºåÂ¶ÇÊûú‰∏ãËΩΩÂ§±Ë¥•ÔºåÊàë‰ª¨Â∞±‰∏çÁªßÁª≠ËøêË°åÊ®°ÂûãÈÉ®ÂàÜ‰∫Ü\n",
        "    model = None\n",
        "\n",
        "if model:\n",
        "    # ==========================================\n",
        "    # 3. Tokenizer Â§ÑÁêÜ (ÊñáÊú¨ -> Êï∞Â≠óID)\n",
        "    # ==========================================\n",
        "    # padding=True: ÊääÁü≠ÁöÑÂàÜÂ≠êË°•ÈΩêÔºå‰øùËØÅ‰∏Ä‰∏™batchÈáåÈïøÂ∫¶‰∏ÄÊ†∑\n",
        "    # return_tensors=\"pt\": ËøîÂõû PyTorch ÁöÑ Tensor Ê†ºÂºè\n",
        "    inputs = tokenizer(smiles_samples, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    print(\"\\nüîç [Step 1] Tokenizer Â§ÑÁêÜÂêéÁöÑÊï∞ÊçÆ:\")\n",
        "    print(f\"Input IDs (ÂΩ¢Áä∂ {inputs['input_ids'].shape}):\\n{inputs['input_ids']}\")\n",
        "    print(f\"\\nAttention Mask (ÂΩ¢Áä∂ {inputs['attention_mask'].shape}):\\n{inputs['attention_mask']}\")\n",
        "    print(\"(Ê≥®ÊÑèÁúãÔºö1 ‰ª£Ë°®ÊúâÊïàtokenÔºå0 ‰ª£Ë°®Ë°•ÈΩêÁöÑpadding)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. Ê®°ÂûãÂâçÂêë‰º†Êí≠ (Êï∞Â≠óID -> È´òÁª¥ÂêëÈáè)\n",
        "    # ==========================================\n",
        "    print(\"\\n‚ö° [Step 2] ËæìÂÖ•Ê®°ÂûãËøõË°åËÆ°ÁÆó...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # ÊèêÂèñÊ†∏ÂøÉËæìÂá∫\n",
        "    # last_hidden_state: ÊØè‰∏Ä‰∏™ Token ÂØπÂ∫îÁöÑÂêëÈáè\n",
        "    # pooler_output (ÊàñËÄÖÂèñÁ¨¨‰∏Ä‰∏™token CLS): Êï¥‰∏™ÂàÜÂ≠êÁöÑ‰ª£Ë°®ÂêëÈáè\n",
        "\n",
        "    token_embeddings = outputs.last_hidden_state\n",
        "    cls_embedding = token_embeddings[:, 0, :] # ÂèñÊØè‰∏™Âè•Â≠êÁöÑÁ¨¨‰∏Ä‰∏™token (CLS) ‰Ωú‰∏∫Êï¥‰ΩìÁâπÂæÅ\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(\"üìä [Step 3] ÊúÄÁªàËæìÂá∫ÁªìÊûúËß£Êûê:\")\n",
        "\n",
        "    print(f\"\\n1. ÂÆåÊï¥Â∫èÂàóÁâπÂæÅ (Last Hidden State):\")\n",
        "    print(f\"   ÂΩ¢Áä∂: {token_embeddings.shape}\")\n",
        "    print(f\"   Ëß£ËØª: [Batch Size (3), Â∫èÂàóÈïøÂ∫¶ ({token_embeddings.shape[1]}), ÁâπÂæÅÁª¥Â∫¶ (768)]\")\n",
        "    print(\"   Âê´‰πâ: ÊØè‰∏™ÂéüÂ≠ê/Â≠óÁ¨¶ÈÉΩË¢´ËΩ¨Êç¢Êàê‰∫Ü‰∏Ä‰∏™ 768 Áª¥ÁöÑÂêëÈáè„ÄÇ\")\n",
        "\n",
        "    print(f\"\\n2. ÂàÜÂ≠êÊï¥‰ΩìÁâπÂæÅ (CLS Token Embedding):\")\n",
        "    print(f\"   ÂΩ¢Áä∂: {cls_embedding.shape}\")\n",
        "    print(f\"   Ëß£ËØª: [Batch Size (3), ÁâπÂæÅÁª¥Â∫¶ (768)]\")\n",
        "    print(\"   Âê´‰πâ: ËøôÂ∞±ÊòØÊàë‰ª¨ÈÄöÂ∏∏Áî®Êù•ÂÅö‚Äò‰∏ãÊ∏∏‰ªªÂä°‚ÄôÔºàÂ¶ÇÁîúÂ∫¶È¢ÑÊµãÔºâÁöÑÈÇ£‰∏™ËûçÂêàÁâπÂæÅÂêëÈáè„ÄÇ\")\n",
        "\n",
        "    print(f\"\\n3. ÁúãÁúãÁ¨¨‰∏Ä‰∏™ÂàÜÂ≠ê ('C') ÁöÑÂâç10‰ΩçÁâπÂæÅÊï∞ÂÄº:\")\n",
        "    print(f\"   {cls_embedding[0][:10].numpy()}\")"
      ],
      "metadata": {
        "id": "_8dCFBjTE0in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Á¨¨2ÈÉ®ÂàÜÔºöËØïËØïÊõ¥Â§öÊ®°ÊÄÅÔºàGraph+SMILES+ÊåáÁ∫π&ÊèèËø∞Á¨¶+È¢ÑËÆ≠ÁªÉÁâπÂæÅÔºâ**"
      ],
      "metadata": {
        "id": "Cdl0a15A9rV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import pickle  # Áî®‰∫é‰øùÂ≠òÂ≠óÂÖ∏Êï∞ÊçÆ\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy.sparse import coo_matrix\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "# RDKit & Sklearn\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, AllChem, rdBase, rdchem\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split # ‰ΩøÁî® sklearn ËøõË°åÊõ¥ÂèØÊéßÁöÑÂàíÂàÜ\n",
        "\n",
        "# PyG & Transformers\n",
        "try:\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    from torch_geometric.nn import GATConv, global_mean_pool\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "except ImportError:\n",
        "    print(\"Ê≠£Âú®ÂÆâË£Ö‰æùËµñÂ∫ì...\")\n",
        "    os.system(\"pip install -q torch-geometric transformers\")\n",
        "    from torch_geometric.data import Data, Batch\n",
        "    from torch_geometric.nn import GATConv, global_mean_pool\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. ÂÖ®Â±ÄÈÖçÁΩÆ‰∏éÁõÆÂΩïËÆæÁΩÆ\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "import logging\n",
        "warnings.filterwarnings('ignore')\n",
        "rdBase.DisableLog('rdApp.warning')\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚úÖ Using Device: {DEVICE}\")\n",
        "\n",
        "# ÂàõÂª∫‰øùÂ≠òÁõÆÂΩï\n",
        "SAVE_DIR = \"experiment_results\"\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "    print(f\"üìÇ Created directory: {SAVE_DIR}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ÁâπÂæÅÂ∑•ÂéÇ (Feature Factory)\n",
        "# ==============================================================================\n",
        "\n",
        "class SMILESTokenizer:\n",
        "    def __init__(self, max_len=128):\n",
        "        self.max_len = max_len\n",
        "        self.pattern =  r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "        self.regex = re.compile(self.pattern)\n",
        "        self.special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "        self.vocab = {t: i for i, t in enumerate(self.special_tokens)}\n",
        "\n",
        "    def tokenize(self, smiles):\n",
        "        return [token for token in self.regex.findall(smiles)]\n",
        "\n",
        "    def build_vocab(self, smiles_list):\n",
        "        counter = Counter()\n",
        "        for s in smiles_list:\n",
        "            counter.update(self.tokenize(s))\n",
        "        for token, _ in counter.most_common():\n",
        "            if token not in self.vocab:\n",
        "                self.vocab[token] = len(self.vocab)\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, smiles):\n",
        "        tokens = self.tokenize(smiles)[:self.max_len - 2]\n",
        "        ids = [self.vocab.get(t, self.vocab[\"<unk>\"]) for t in tokens]\n",
        "        return [self.vocab[\"<sos>\"]] + ids + [self.vocab[\"<eos>\"]]\n",
        "\n",
        "def one_hot_encoding(value, choices):\n",
        "    encoding = [0] * (len(choices) + 1)\n",
        "    index = choices.index(value) if value in choices else -1\n",
        "    encoding[index] = 1\n",
        "    return encoding\n",
        "\n",
        "class MoleculeFeaturizer:\n",
        "    def _atom_featurizer(self, atom):\n",
        "        atomic_numer = [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 46, 47, 48, 49, 50, 51, 52, 53]\n",
        "        return (one_hot_encoding(atom.GetAtomicNum(), atomic_numer) +\n",
        "                one_hot_encoding(atom.GetTotalDegree(), list(range(5))) +\n",
        "                one_hot_encoding(int(atom.GetHybridization()), list(range(len(Chem.HybridizationType.names)-1))) +\n",
        "                one_hot_encoding(atom.GetChiralTag(), list(range(len(Chem.ChiralType.names)-1))) +\n",
        "                one_hot_encoding(atom.GetTotalNumHs(), list(range(5))) +\n",
        "                [1 if atom.GetIsAromatic() else 0])\n",
        "\n",
        "    def _bond_featurizer(self, bond):\n",
        "        bt = [int(bond.GetBondType() == t) for t in [rdchem.BondType.SINGLE, rdchem.BondType.DOUBLE, rdchem.BondType.TRIPLE, rdchem.BondType.AROMATIC]]\n",
        "        return bt + [int(bond.GetIsConjugated()), int(bond.IsInRing())]\n",
        "\n",
        "    def __call__(self, mol):\n",
        "        atom_features = [self._atom_featurizer(atom) for atom in mol.GetAtoms()]\n",
        "        x = torch.tensor(atom_features, dtype=torch.float32)\n",
        "\n",
        "        adj = Chem.GetAdjacencyMatrix(mol)\n",
        "        coo_adj = coo_matrix(adj)\n",
        "        row, col = coo_adj.row, coo_adj.col\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "\n",
        "        bond_features = []\n",
        "        for i in range(len(row)):\n",
        "            u, v = int(row[i]), int(col[i])\n",
        "            bond = mol.GetBondBetweenAtoms(u, v)\n",
        "            if bond is not None:\n",
        "                bond_features.append(self._bond_featurizer(bond))\n",
        "            else:\n",
        "                bond_features.append([0]*6)\n",
        "\n",
        "        if len(bond_features) > 0:\n",
        "            edge_attr = torch.tensor(bond_features, dtype=torch.float32)\n",
        "        else:\n",
        "            edge_attr = torch.empty((0, 6), dtype=torch.float32)\n",
        "\n",
        "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "class AdvancedFeaturizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModel.from_pretrained(self.model_name).to(DEVICE).eval()\n",
        "            self.has_bert = True\n",
        "            print(\"‚úÖ ChemBERTa Loaded.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è ChemBERTa load failed: {e}\")\n",
        "            self.has_bert = False\n",
        "\n",
        "    def get_bert_feature(self, smiles):\n",
        "        if not self.has_bert: return torch.randn(768)\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "            outputs = self.model(**inputs)\n",
        "            return outputs.last_hidden_state[0, 0, :].cpu()\n",
        "\n",
        "    def get_expert_features(self, mol):\n",
        "        desc = [Descriptors.MolWt(mol), Descriptors.MolLogP(mol), Descriptors.NumHDonors(mol),\n",
        "                Descriptors.NumHAcceptors(mol), Descriptors.TPSA(mol), Descriptors.NumRotatableBonds(mol),\n",
        "                Descriptors.FractionCSP3(mol), Descriptors.HallKierAlpha(mol), Descriptors.RingCount(mol)]\n",
        "        fp = list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024))\n",
        "        return torch.tensor(desc + fp, dtype=torch.float32)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Universal Dataset & Data Splitting\n",
        "# ==============================================================================\n",
        "class UniversalDataset(Dataset):\n",
        "    def __init__(self, df, smiles_tokenizer, save_path=None):\n",
        "        self.data = []\n",
        "        # ‰øùÂ≠òÂéüÂßã SMILES ‰ª•‰æøÂêéÁª≠ÂàÜÊûê\n",
        "        self.raw_smiles = df['Smiles'].tolist()\n",
        "        self.raw_labels = df['logSw'].tolist()\n",
        "\n",
        "        if save_path and os.path.exists(save_path):\n",
        "            print(f\"üîÑ Loading cached data from {save_path}...\")\n",
        "            self.data = torch.load(save_path)\n",
        "        else:\n",
        "            print(\"‚ö° Generating ALL features (Graph, Seq, Desc, FP, BERT)...\")\n",
        "            graph_featurizer = MoleculeFeaturizer()\n",
        "            adv_featurizer = AdvancedFeaturizer()\n",
        "\n",
        "            expert_feats_list = []\n",
        "\n",
        "            for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "                mol = Chem.MolFromSmiles(row['Smiles'])\n",
        "                if mol is None:\n",
        "                    # ‰øùÊåÅÁ¥¢Âºï‰∏ÄËá¥ÔºåÊó†ÊïàÂàÜÂ≠êÂ°´ NoneÔºåÂêéÁª≠Â§ÑÁêÜ\n",
        "                    self.data.append(None)\n",
        "                    continue\n",
        "\n",
        "                expert_f = adv_featurizer.get_expert_features(mol)\n",
        "                expert_feats_list.append(expert_f)\n",
        "\n",
        "                self.data.append({\n",
        "                    'graph': graph_featurizer(mol),\n",
        "                    'seq': torch.tensor(smiles_tokenizer.encode(row['Smiles']), dtype=torch.long),\n",
        "                    'expert': expert_f,\n",
        "                    'bert': adv_featurizer.get_bert_feature(row['Smiles']),\n",
        "                    'y': torch.tensor([row['logSw']], dtype=torch.float32),\n",
        "                    'index': i # ‰øùÂ≠òÂéüÂßãÁ¥¢Âºï\n",
        "                })\n",
        "\n",
        "            # Filter None data but keep track of indices mapping if needed\n",
        "            # ËøôÈáåÁÆÄÂçïËµ∑ËßÅÔºåÊàë‰ª¨ÁßªÈô§ NoneÔºåÊ≥®ÊÑè self.raw_smiles ÂØπÂ∫îÂÖ≥Á≥ª‰ºöÂèò\n",
        "            # Êõ¥Â•ΩÁöÑÂÅöÊ≥ïÊòØÈáçÊñ∞ÂØπÈΩê raw_smiles\n",
        "            valid_indices = [i for i, x in enumerate(self.data) if x is not None]\n",
        "            self.data = [self.data[i] for i in valid_indices]\n",
        "            self.raw_smiles = [self.raw_smiles[i] for i in valid_indices]\n",
        "            self.raw_labels = [self.raw_labels[i] for i in valid_indices]\n",
        "\n",
        "            # Normalize Expert Features\n",
        "            if expert_feats_list:\n",
        "                all_experts = torch.stack(expert_feats_list)\n",
        "                mean = all_experts.mean(dim=0)\n",
        "                std = all_experts.std(dim=0) + 1e-6\n",
        "                for d in self.data:\n",
        "                    d['expert'] = (d['expert'] - mean) / std\n",
        "\n",
        "            if save_path:\n",
        "                torch.save(self.data, save_path)\n",
        "                print(\"‚úÖ Data saved.\")\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def universal_collate(batch):\n",
        "    graphs = Batch.from_data_list([x['graph'] for x in batch])\n",
        "    seqs = pad_sequence([x['seq'] for x in batch], batch_first=True, padding_value=0)\n",
        "    experts = torch.stack([x['expert'] for x in batch])\n",
        "    berts = torch.stack([x['bert'] for x in batch])\n",
        "    labels = torch.cat([x['y'] for x in batch])\n",
        "    return graphs, seqs, experts, berts, labels\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Model Architecture\n",
        "# ==============================================================================\n",
        "# (Same model classes as before)\n",
        "class GNNBranch(nn.Module):\n",
        "    def __init__(self, input_dim=69, hidden_dim=16, heads=2, edge_dim=6, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout, edge_dim=edge_dim)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout, edge_dim=edge_dim)\n",
        "        self.pool = global_mean_pool\n",
        "        self.out_dim = hidden_dim\n",
        "    def forward(self, x, edge_index, batch, edge_attr):\n",
        "        x = self.gat1(x, edge_index, edge_attr=edge_attr)\n",
        "        x = F.elu(x)\n",
        "        x = self.gat2(x, edge_index, edge_attr=edge_attr)\n",
        "        x = F.elu(x)\n",
        "        return self.pool(x, batch)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div)\n",
        "        pe[:, 1::2] = torch.cos(position * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerBranch(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        enc = nn.TransformerEncoderLayer(d_model, nhead, 256, batch_first=True, dropout=0.1)\n",
        "        self.trans = nn.TransformerEncoder(enc, num_layers)\n",
        "        self.out_dim = d_model\n",
        "    def forward(self, x):\n",
        "        mask = (x == self.pad_idx)\n",
        "        x = self.pos(self.emb(x) * math.sqrt(self.out_dim))\n",
        "        x = self.trans(x, src_key_padding_mask=mask)\n",
        "        mask_exp = (~mask).unsqueeze(-1).float()\n",
        "        return (x * mask_exp).sum(1) / mask_exp.sum(1).clamp(min=1e-9)\n",
        "\n",
        "class Projector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.BatchNorm1d(output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.out_dim = output_dim\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class FlexibleSweetNet(nn.Module):\n",
        "    def __init__(self, config, vocab_size, pad_idx):\n",
        "        super().__init__()\n",
        "        self.cfg = config\n",
        "        self.gnn = GNNBranch() if config['use_graph'] else None\n",
        "        self.trans = TransformerBranch(vocab_size, pad_idx=pad_idx) if config['use_seq'] else None\n",
        "        self.expert = Projector(1033) if config['use_expert'] else None\n",
        "        self.bert = Projector(768) if config['use_bert'] else None\n",
        "\n",
        "        fusion_dim = 0\n",
        "        if self.gnn: fusion_dim += self.gnn.out_dim\n",
        "        if self.trans: fusion_dim += self.trans.out_dim\n",
        "        if self.expert: fusion_dim += self.expert.out_dim\n",
        "        if self.bert: fusion_dim += self.bert.out_dim\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, graphs, seqs, experts, berts):\n",
        "        feats = []\n",
        "        if self.gnn: feats.append(self.gnn(graphs.x, graphs.edge_index, graphs.batch, graphs.edge_attr))\n",
        "        if self.trans: feats.append(self.trans(seqs))\n",
        "        if self.expert: feats.append(self.expert(experts))\n",
        "        if self.bert: feats.append(self.bert(berts))\n",
        "        combined = torch.cat(feats, dim=1)\n",
        "        return self.head(combined).squeeze()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Training Engine\n",
        "# ==============================================================================\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for g, s, e, b, y in loader:\n",
        "        g, s, e, b, y = g.to(DEVICE), s.to(DEVICE), e.to(DEVICE), b.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(g, s, e, b)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for g, s, e, b, y in loader:\n",
        "            g, s, e, b, y = g.to(DEVICE), s.to(DEVICE), e.to(DEVICE), b.to(DEVICE), y.to(DEVICE)\n",
        "            p = model(g, s, e, b)\n",
        "            preds.extend(p.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "    return r2_score(targets, preds), np.array(targets), np.array(preds)\n",
        "\n",
        "def run_experiment(name, config, vocab_size, pad_id, loaders, epochs=100):\n",
        "    print(f\"\\nüöÄ Training [{name}]...\")\n",
        "    train_loader, val_loader, test_loader = loaders\n",
        "\n",
        "    model = FlexibleSweetNet(config, vocab_size, pad_id).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
        "\n",
        "    best_val_r2 = -float('inf')\n",
        "    best_wts = copy.deepcopy(model.state_dict())\n",
        "    loss_hist, r2_hist = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "        val_r2, _, _ = evaluate(model, val_loader)\n",
        "        scheduler.step(loss)\n",
        "        loss_hist.append(loss)\n",
        "        r2_hist.append(val_r2)\n",
        "\n",
        "        if val_r2 > best_val_r2:\n",
        "            best_val_r2 = val_r2\n",
        "            best_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch+1) % 20 == 0:\n",
        "            print(f\"   Ep {epoch+1:03d} | Loss: {loss:.4f} | Val R2: {val_r2:.4f}\")\n",
        "\n",
        "    # Save Best Checkpoint\n",
        "    safe_name = name.replace(\" \", \"_\").replace(\"+\", \"_\")\n",
        "    ckpt_path = os.path.join(SAVE_DIR, f\"model_{safe_name}.pt\")\n",
        "    torch.save(best_wts, ckpt_path)\n",
        "    print(f\"   üíæ Checkpoint saved: {ckpt_path}\")\n",
        "\n",
        "    # Final Test\n",
        "    model.load_state_dict(best_wts)\n",
        "    test_r2, y_true, y_pred = evaluate(model, test_loader)\n",
        "    print(f\"   üèÜ Final Test R2: {test_r2:.4f}\")\n",
        "\n",
        "    return {'name': name, 'r2': test_r2, 'loss': loss_hist, 'val_r2': r2_hist, 'pred': (y_true, y_pred)}\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Main Execution\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. Load Data\n",
        "    csv_path = 'SweetpredDB.csv'\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(\"Creating dummy data...\")\n",
        "        df = pd.DataFrame({'Smiles': ['C']*50 + ['CC']*50, 'logSw': np.random.rand(100)})\n",
        "    else:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.rename(columns=str.lower, inplace=True)\n",
        "        df.rename(columns={'smiles': 'Smiles', 'logsw': 'logSw'}, inplace=True)\n",
        "\n",
        "    tokenizer = SMILESTokenizer()\n",
        "    tokenizer.build_vocab(df['Smiles'].tolist())\n",
        "    pad_id = tokenizer.vocab[\"<pad>\"]\n",
        "\n",
        "    # ‰øùÂ≠ò Tokenizer (Âõ†‰∏∫È¢ÑÊµãÊó∂ÈúÄË¶Å)\n",
        "    with open(os.path.join(SAVE_DIR, 'vocab.json'), 'w') as f:\n",
        "        json.dump(tokenizer.vocab, f)\n",
        "\n",
        "    # Âº∫Âà∂Âà†Èô§ÊóßÁºìÂ≠òÔºåÁ°Æ‰øùÁâπÂæÅÊòØÊúÄÊñ∞ÁöÑ\n",
        "    dataset_cache = os.path.join(SAVE_DIR, \"universal_data_v3.pt\")\n",
        "    if os.path.exists(dataset_cache): os.remove(dataset_cache)\n",
        "\n",
        "    dataset = UniversalDataset(df, tokenizer, save_path=dataset_cache)\n",
        "\n",
        "    # 2. Split with Indices (Critical for Analysis!)\n",
        "    indices = np.arange(len(dataset))\n",
        "    train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Save Indices\n",
        "    split_info = {'train': train_idx.tolist(), 'val': val_idx.tolist(), 'test': test_idx.tolist()}\n",
        "    with open(os.path.join(SAVE_DIR, 'split_indices.json'), 'w') as f:\n",
        "        json.dump(split_info, f)\n",
        "\n",
        "    # Save Test Metadata (SMILES and True Labels) for Deep Analysis\n",
        "    test_meta = []\n",
        "    for idx in test_idx:\n",
        "        test_meta.append({\n",
        "            'index': int(idx),\n",
        "            'SMILES': dataset.raw_smiles[idx],\n",
        "            'True_LogSw': dataset.raw_labels[idx]\n",
        "        })\n",
        "    pd.DataFrame(test_meta).to_csv(os.path.join(SAVE_DIR, \"test_metadata.csv\"), index=False)\n",
        "    print(\"üìù Test metadata saved.\")\n",
        "\n",
        "    # Create Subsets and Loaders\n",
        "    train_ds = Subset(dataset, train_idx)\n",
        "    val_ds = Subset(dataset, val_idx)\n",
        "    test_ds = Subset(dataset, test_idx)\n",
        "\n",
        "    loaders = (\n",
        "        DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=universal_collate),\n",
        "        DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=universal_collate),\n",
        "        DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=universal_collate)\n",
        "    )\n",
        "\n",
        "    # 3. Experiments\n",
        "    experiments = [\n",
        "        (\"Graph Only\", {'use_graph': True, 'use_seq': False, 'use_expert': False, 'use_bert': False}),\n",
        "        (\"SMILES Only\", {'use_graph': False, 'use_seq': True, 'use_expert': False, 'use_bert': False}),\n",
        "        (\"G+S+Exp\", {'use_graph': True, 'use_seq': True, 'use_expert': True, 'use_bert': False}),\n",
        "        (\"G+S+BERT\", {'use_graph': True, 'use_seq': True, 'use_expert': False, 'use_bert': True}),\n",
        "        (\"All-In-One\", {'use_graph': True, 'use_seq': True, 'use_expert': True, 'use_bert': True})\n",
        "    ]\n",
        "\n",
        "    # 4. Run & Save\n",
        "    all_results = []\n",
        "    for name, cfg in experiments:\n",
        "        res = run_experiment(name, cfg, len(tokenizer.vocab), pad_id, loaders, epochs=100)\n",
        "        all_results.append(res)\n",
        "\n",
        "    # Save Experiment Results Pickle\n",
        "    # Convert numpy arrays to lists for JSON serialization if needed, or use pickle\n",
        "    with open(os.path.join(SAVE_DIR, \"all_results.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(all_results, f)\n",
        "    print(f\"üíæ All results saved to {SAVE_DIR}/all_results.pkl\")\n",
        "\n",
        "    # 5. Visualization (Basic)\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üìä COMPARISON SUMMARY\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Bar Chart\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    names = [r['name'] for r in all_results]\n",
        "    scores = [r['r2'] for r in all_results]\n",
        "    bars = ax1.bar(names, scores, color=sns.color_palette(\"viridis\", len(names)))\n",
        "    ax1.set_title(\"Test R2 Score Comparison\")\n",
        "    ax1.set_ylim(0, 1.0)\n",
        "    ax1.set_xticks(range(len(names)))\n",
        "    ax1.set_xticklabels(names, rotation=45, ha=\"right\")\n",
        "    for bar in bars:\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f\"{bar.get_height():.3f}\", ha='center', va='bottom')\n",
        "\n",
        "    # Val R2 Curves\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    for res in all_results:\n",
        "        ax2.plot(res['val_r2'], label=res['name'])\n",
        "    ax2.set_title(\"Validation R2 History\")\n",
        "    ax2.legend()\n",
        "\n",
        "    # Scatter (Best Model)\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    best_res = max(all_results, key=lambda x: x['r2'])\n",
        "    y_true, y_pred = best_res['pred']\n",
        "    ax3.scatter(y_true, y_pred, alpha=0.5, color='#2ca02c', label=f\"Best: {best_res['name']}\")\n",
        "    ax3.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')\n",
        "    ax3.set_title(f\"Prediction: {best_res['name']}\")\n",
        "    ax3.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(SAVE_DIR, \"summary_plot.png\"))\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "hDcpM_JjCSDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ÁªìÊûúÂàÜÊûê**"
      ],
      "metadata": {
        "id": "kNu2d0MbLlnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, AllChem\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. ÈÖçÁΩÆ‰∏éÂä†ËΩΩ\n",
        "# ==============================================================================\n",
        "SAVE_DIR = \"experiment_results\"\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# È¢ÑÂÆö‰πâÁöÑÈ¢úËâ≤Êò†Â∞ÑÔºàÁ°Æ‰øùÂêçÂ≠ó‰∏éËÆ≠ÁªÉÊó∂ÁöÑ Name ‰∏ÄËá¥Ôºâ\n",
        "# Â¶ÇÊûú‰Ω†ÁöÑÊ®°ÂûãÂêçÂ≠ó‰∏ç‰∏ÄÊ†∑ÔºåËØ∑Âú®ËøôÈáå‰øÆÊîπ Key\n",
        "MODEL_COLORS = {\n",
        "    'Graph Only': '#999999',      # ÁÅ∞\n",
        "    'SMILES Only': '#e6550d',     # Ê©ô\n",
        "    'G+S+Exp': '#3182bd',         # Ëìù\n",
        "    'G+S+BERT': '#756bb1',        # Á¥´\n",
        "    'All-In-One': '#2ca02c'       # Áªø\n",
        "}\n",
        "\n",
        "def load_and_prep():\n",
        "    print(\"üì• Ê≠£Âú®Âä†ËΩΩÊï∞ÊçÆ...\")\n",
        "    try:\n",
        "        with open(os.path.join(SAVE_DIR, \"all_results.pkl\"), \"rb\") as f:\n",
        "            all_results = pickle.load(f)\n",
        "        df = pd.read_csv(os.path.join(SAVE_DIR, \"test_metadata.csv\"))\n",
        "    except:\n",
        "        print(\"‚ùå Êï∞ÊçÆÁº∫Â§±ÔºåËØ∑ÂÖàËøêË°åËÆ≠ÁªÉ‰ª£Á†ÅÔºÅ\")\n",
        "        return None, None, None\n",
        "\n",
        "    # ËÆ°ÁÆóÊÄßË¥®\n",
        "    print(\"‚öóÔ∏è ËÆ°ÁÆóÂàÜÂ≠êÊÄßË¥®...\")\n",
        "    mols = [Chem.MolFromSmiles(s) for s in df['SMILES']]\n",
        "    df['MolWt'] = [Descriptors.MolWt(m) if m else 0 for m in mols]\n",
        "    df['LogP'] = [Descriptors.MolLogP(m) if m else 0 for m in mols]\n",
        "    df['TPSA'] = [Descriptors.TPSA(m) if m else 0 for m in mols]\n",
        "\n",
        "    model_names = []\n",
        "    for res in all_results:\n",
        "        name = res['name']\n",
        "        model_names.append(name)\n",
        "        preds = res['pred'][1]\n",
        "        if len(preds) == len(df):\n",
        "            df[f'AbsErr_{name}'] = np.abs(df['True_LogSw'] - preds)\n",
        "\n",
        "    # Ê£ÄÊü•È¢úËâ≤Â≠óÂÖ∏ÊòØÂê¶Ë¶ÜÁõñ‰∫ÜÊâÄÊúâÊ®°Âûã\n",
        "    # Â¶ÇÊûúÊ≤°ÊúâÔºåËá™Âä®Ë°•ÂÖÖÈªòËÆ§È¢úËâ≤ÔºåÈò≤Ê≠¢Êä•Èîô\n",
        "    unique_models = model_names\n",
        "    palette = []\n",
        "    for name in unique_models:\n",
        "        if name in MODEL_COLORS:\n",
        "            palette.append(MODEL_COLORS[name])\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Ë≠¶Âëä: Ê®°Âûã '{name}' Ê≤°ÊúâÈ¢ÑÂÆö‰πâÈ¢úËâ≤Ôºå‰ΩøÁî®ÈªòËÆ§ÈªëËâ≤„ÄÇ\")\n",
        "            palette.append('#000000')\n",
        "\n",
        "    return df, model_names, mols, palette\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ‰øÆÂ§çÂêéÁöÑÂ±ûÊÄßÂàáÁâáÂàÜÊûê (Error Trend)\n",
        "# ==============================================================================\n",
        "def plot_property_slicing_fixed(df, model_names, palette):\n",
        "    \"\"\"\n",
        "    ÁªòÂà∂ËØØÂ∑ÆÈöè MW, LogP, TPSA ÂèòÂåñÁöÑË∂ãÂäøÂõæ„ÄÇ\n",
        "    \"\"\"\n",
        "    print(\"üìä Ê≠£Âú®ÁªòÂà∂Â±ûÊÄßË∂ãÂäøÂõæ...\")\n",
        "\n",
        "    props = [\n",
        "        ('MolWt', 'Molecular Weight'),\n",
        "        ('LogP', 'LogP (Polarity)'),\n",
        "        ('TPSA', 'TPSA (Complexity)')\n",
        "    ]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for i, (col, title) in enumerate(props):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # 1. Êï∞ÊçÆÂáÜÂ§áÔºöÊâÅÂπ≥Âåñ (Melt)\n",
        "        # Êàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™ÈïøÊ†ºÂºè DataFrame: [Value, Error, Model]\n",
        "        plot_data = []\n",
        "\n",
        "        # ÂÖàÂØπÂ±ûÊÄßÂàÜÊ°∂ (5Á≠âÂàÜ)\n",
        "        df[f'{col}_Bin'] = pd.qcut(df[col], q=5, duplicates='drop')\n",
        "\n",
        "        for name in model_names:\n",
        "            # ËÆ°ÁÆóÊØè‰∏™Ê°∂ÁöÑÂπ≥ÂùáËØØÂ∑Æ\n",
        "            # group: (Bin_Interval, Mean_Error)\n",
        "            group = df.groupby(f'{col}_Bin')[f'AbsErr_{name}'].mean().reset_index()\n",
        "            group.columns = ['Bin', 'MAE']\n",
        "            group['Model'] = name\n",
        "            # ÂèñÂå∫Èó¥‰∏≠ÁÇπ‰Ωú‰∏∫ X ËΩ¥Êï∞ÂÄº (Êñπ‰æøÁªòÂõæ)\n",
        "            group['X_Value'] = group['Bin'].apply(lambda x: x.mid)\n",
        "            plot_data.append(group)\n",
        "\n",
        "        final_df = pd.concat(plot_data)\n",
        "\n",
        "        # 2. ÁªòÂõæ\n",
        "        # Ê≥®ÊÑèÔºöËøôÈáåÁõ¥Êé•‰º†ÂÖ• palette listÔºåÈ°∫Â∫èÂøÖÈ°ªÂíå hue_order ‰∏ÄËá¥\n",
        "        sns.lineplot(\n",
        "            data=final_df,\n",
        "            x='X_Value',\n",
        "            y='MAE',\n",
        "            hue='Model',\n",
        "            hue_order=model_names,\n",
        "            palette=palette,\n",
        "            marker='o',\n",
        "            linewidth=2.5,\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        ax.set_title(f\"Error Trend by {title}\", fontweight='bold')\n",
        "        ax.set_ylabel(\"Mean Absolute Error (MAE)\")\n",
        "        ax.set_xlabel(title)\n",
        "        if i == 0:\n",
        "            ax.legend(title='Model')\n",
        "        else:\n",
        "            ax.legend().remove() # Âè™Âú®Á¨¨‰∏ÄÂº†ÂõæÊòæÁ§∫Âõæ‰æãÔºåÈÅøÂÖçÈÅÆÊå°\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. ÂàÜÊ®°ÂûãÂåñÂ≠¶Á©∫Èó¥ËØØÂ∑ÆÂõæ (Separate Maps)\n",
        "# ==============================================================================\n",
        "def plot_separate_error_maps(df, model_names, mols):\n",
        "    print(\"üó∫Ô∏è Ê≠£Âú®ÁªòÂà∂ 5 Âº†Áã¨Á´ãÁöÑÂåñÂ≠¶Á©∫Èó¥ËØØÂ∑ÆÂõæ...\")\n",
        "\n",
        "    # 1. ËÆ°ÁÆó t-SNE (ÂÖ±Áî®ÂùêÊ†á)\n",
        "    valid_indices = [i for i, m in enumerate(mols) if m]\n",
        "    if not valid_indices: return\n",
        "\n",
        "    fps = [AllChem.GetMorganFingerprintAsBitVect(mols[i], 2, nBits=1024) for i in valid_indices]\n",
        "    tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
        "    X_emb = tsne.fit_transform(np.array(fps))\n",
        "\n",
        "    plot_df = df.iloc[valid_indices].copy()\n",
        "    plot_df['x'] = X_emb[:, 0]\n",
        "    plot_df['y'] = X_emb[:, 1]\n",
        "\n",
        "    # 2. Áªü‰∏ÄËâ≤Èò∂ (ÂÖ≥ÈîÆÔºÅ)\n",
        "    # ÊâæÂá∫ÊâÄÊúâÊ®°Âûã‰∏≠ÁöÑÊúÄÂ§ßËØØÂ∑ÆÂÄº (95% ÂàÜ‰Ωç)Ôºå‰ª•Ê≠§‰∏∫‰∏äÈôêÔºå‰øùËØÅÈ¢úËâ≤ÂèØÊØîÊÄß\n",
        "    all_err_vals = []\n",
        "    for name in model_names:\n",
        "        all_err_vals.extend(plot_df[f'AbsErr_{name}'].values)\n",
        "    vmax = np.percentile(all_err_vals, 95)\n",
        "\n",
        "    # 3. Âæ™ÁéØÁªòÂõæ (ÊØè‰∏™Ê®°Âûã‰∏ÄÂº†Âõæ)\n",
        "    # Â∏ÉÂ±ÄÔºö2Ë°å3Âàó\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, name in enumerate(model_names):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Ëé∑ÂèñËØØÂ∑Æ\n",
        "        errors = plot_df[f'AbsErr_{name}']\n",
        "\n",
        "        # Êï£ÁÇπÂõæ\n",
        "        # viridis_r: Á¥´Ëâ≤=‰ΩéËØØÂ∑Æ(Â•Ω)ÔºåÈªÑËâ≤=È´òËØØÂ∑Æ(Â∑Æ)\n",
        "        sc = ax.scatter(\n",
        "            plot_df['x'], plot_df['y'],\n",
        "            c=errors,\n",
        "            cmap='viridis_r',\n",
        "            vmin=0, vmax=vmax,\n",
        "            s=30, alpha=0.8, edgecolors='k', linewidth=0.1\n",
        "        )\n",
        "\n",
        "        ax.set_title(f\"{name}\", fontsize=14, fontweight='bold', color=MODEL_COLORS.get(name, 'black'))\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.grid(False) # ÂéªÊéâÁΩëÊ†ºÊõ¥Âπ≤ÂáÄ\n",
        "\n",
        "    # ÈöêËóèÂ§ö‰ΩôÂ≠êÂõæ (Â¶ÇÊûúÊúâ)\n",
        "    for j in range(len(model_names), len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    # Ê∑ªÂä†Áªü‰∏Ä Colorbar\n",
        "    cbar_ax = fig.add_axes([0.92, 0.2, 0.02, 0.6])\n",
        "    cbar = fig.colorbar(sc, cax=cbar_ax)\n",
        "    cbar.set_label('Absolute Prediction Error (MAE)', fontsize=12)\n",
        "\n",
        "    plt.suptitle(\"Chemical Space Error Maps (Separate Models)\\n(Darker/Purple = Better, Brighter/Yellow = Worse)\", fontsize=18, y=0.96)\n",
        "    plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. ‰∏ªÊâßË°åÈÄªËæë\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. Âä†ËΩΩ\n",
        "    df, model_names, mols, palette = load_and_prep()\n",
        "    if df is None: return\n",
        "\n",
        "    print(f\"üìä ÂæÖÂàÜÊûêÊ®°Âûã: {model_names}\")\n",
        "\n",
        "    # 2. ÁªòÂà∂Â±ûÊÄßË∂ãÂäøÂõæ (‰øÆÂ§çÁâà)\n",
        "    plot_property_slicing_fixed(df, model_names, palette)\n",
        "\n",
        "    # 3. ÁªòÂà∂ÂàÜÊ®°ÂûãÂåñÂ≠¶Á©∫Èó¥Âõæ\n",
        "    plot_separate_error_maps(df, model_names, mols)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "E0qPgkcAFvBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}